{% load static %}

<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />
<link rel="shortcut icon" href="{% static 'favicon.ico' %}"/>




<title>RO-MAN 2020</title>

<script src="{% static 'roman2020_libs/jquery-1.11.3/jquery.min.js' %}"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="{% static 'roman2020_libs/bootstrap-3.3.5/css/bootstrap.min.css' %}" rel="stylesheet" />
<script src="{% static 'roman2020_libs/bootstrap-3.3.5/js/bootstrap.min.js' %}"></script>
<script src="{% static 'roman2020_libs/bootstrap-3.3.5/shim/html5shiv.min.js' %}"></script>
<script src="{% static 'roman2020_libs/bootstrap-3.3.5/shim/respond.min.js' %}"></script>
<script src="{% static 'roman2020_libs/jqueryui-1.11.4/jquery-ui.min.js' %}"></script>
<link href="{% static 'roman2020_libs/tocify-1.9.1/jquery.tocify.css' %}" rel="stylesheet" />
<script src="{% static 'roman2020_libs/tocify-1.9.1/jquery.tocify.js' %}"></script>
<script src="{% static 'roman2020_libs/navigation-1.1/tabsets.js' %}"></script>
<script src="{% static 'roman2020_libs/navigation-1.1/codefolding.js' %}"></script>
<link href="{% static 'roman2020_libs/highlightjs-9.12.0/default.css' %}" rel="stylesheet" />
<script src="{% static 'roman2020_libs/highlightjs-9.12.0/highlight.js' %}"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>



<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->
<style type="text/css">
.code-folding-btn { margin-bottom: 4px; }
</style>



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="fluid-row" id="header">

<div class="btn-group pull-right">
<button type="button" class="btn btn-default btn-xs dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"><span>Code</span> <span class="caret"></span></button>
<ul class="dropdown-menu" style="min-width: 50px;">
<li><a id="rmd-show-all-code" href="#">Show All Code</a></li>
<li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li>
</ul>
</div>



<h1 class="title toc-ignore">RO-MAN 2020</h1>

</div>


<pre class="r"><code>library(car)
library(stats)
library(grid)
library(gridExtra)
library(fitdistrplus)
# library(dplyr)
# library(forcats)
# library(tibble)
# library(tidyr)
# library(readr)
# library(ggplot2)
# library(stringr)
library(tidyverse)
library(ggsignif)
library(ggthemes)
library(jtools)
library(broom)
library(modelr)
library(loo)
# library(rstanarm)
library(brms)
library(bayesplot)
library(GGally)
library(tidybayes)
library(bayestestR)
library(sjstats)
library(parameters)
library(see)

# Setup for multiprocessing
options(mc.cores = 4)
# library(future)
# plan(multiprocess)

# Make the default coding of contrasts sum-coding; just in case
options(contrasts = c(&quot;contr.sum&quot;, &quot;contr.poly&quot;))</code></pre>
<p>Global options:</p>
<pre class="r"><code># Script execution globals
train_models = F     # Retrain the MCMC models. Also infer unknown demographics
plot_posteriors = F  # Plot the posterior distributions (debug)
plot_paper_posteriors = T  # Plot the posterior distributions that are in the paper
plot_diagnostics = T # Plot the diagnostics
report_bayesfactors = F  # Report the Bayes Factor scores of model parameters
default_seed = 0x1331 # For repeatable models and diagnostics
data_folder = &quot;~/Documents/GT/Research/Data/arbitration/2019-12-09/results&quot;</code></pre>
<p>Helper functions:</p>
<pre class="r"><code># Create a simple coding contrasts matrix
contr.deviation = function(nlevels) {
  x1 = contr.treatment(nlevels)
  x2 = matrix(rep(1/nlevels, (nlevels-1) * nlevels), ncol = (nlevels-1))
  return(x1-x2)
}

# NOTE: For now we&#39;re ignoring this advice
# # Use a simple coding contrasts matrix to add our own contrasts columns
# # Explained in: http://talklab.psy.gla.ac.uk/tvw/catpred/
# create_simple_contrasts = function(d, column_name) {
#  factor_column = d %&gt;% pull(column_name)
#  contrast_matrix = contr.deviation(length(levels(factor_column)))
# }

# Save a model
saveModel = function(model, folder = data_folder) {
  saveRDS(model, file = file.path(folder, paste(substitute(model), &quot;.rds&quot;, sep = &#39;&#39;)))
}

# Load a model
loadModel = function(model_name, folder = data_folder) {
  return(readRDS(file.path(folder, paste(model_name, &#39;.rds&#39;, sep = &#39;&#39;))))
}

# Test hypotheses. The NULL arguments need to be provided in this case
test_hypotheses = function(h_df, rope_values = NULL, log_odds_effects = T) {
  # Set the default ROPE values
  if (is.null(rope_values)) {
    rope_values = c(-0.1, 0.1)
  }

  pd_test = h_df %&gt;% pd() %&gt;% as_tibble()

  eq_test = h_df %&gt;%
    equivalence_test(range = rope_values, ci = 1.0, verbose = T) %&gt;%
    as_tibble() %&gt;%
    select(Parameter, ROPE_Percentage, ROPE_Equivalence)

  hdi_test = h_df %&gt;% hdi(ci = .89, verbose = T) %&gt;% as_tibble() %&gt;% select(Parameter, CI_low, CI_high)

  point_test = h_df %&gt;%
    point_interval(.width = 0.89, .point = median, .interval = hdi, .simple_names = T) %&gt;%
    select(colnames(h_df)) %&gt;%
    gather(key = &quot;Parameter&quot;, value = &quot;Estimate&quot;)

  if (!log_odds_effects) {
    sd_estimate = h_df %&gt;%
      point_interval(.point = mad, .simple_names = T) %&gt;%
      select(colnames(h_df)) %&gt;%
      gather(key = &quot;Parameter&quot;, value = &quot;Std.Error&quot;)

    effect_test = point_test %&gt;%
      inner_join(hdi_test, by = &quot;Parameter&quot;) %&gt;%
      inner_join(sd_estimate, by = &quot;Parameter&quot;) %&gt;%
      mutate(Effect_Size = abs(Estimate) / 100 * Std.Error) %&gt;% # Rescale by 100 because we assume 1-100 data
      select(-Std.Error)
  } else {
    effect_test = point_test %&gt;%
      inner_join(hdi_test, by = &quot;Parameter&quot;) %&gt;%
      mutate(Effect_Size = abs(Estimate) * sqrt(3) / pi)
             # There&#39;s quite a few nuances that I&#39;m missing with this calculation
             # Effect_CI_low = pmin(abs(CI_low), abs(CI_high)) * sqrt(3) / pi,
             # Effect_CI_high = pmax(abs(CI_low), abs(CI_high)) * sqrt(3) / pi)
  }

  hyp_results = effect_test %&gt;%
    inner_join(pd_test, by = &quot;Parameter&quot;) %&gt;%
    inner_join(eq_test, by = &quot;Parameter&quot;)

  return(hyp_results)
}

# Fit distributions to vectors and print the results. Also return the fit, if necessary
fit_and_print_dist = function(vec, distr, string, use_mass = F) {
  if (!use_mass) {
    f = fitdist(vec, distr)
  } else {
    f = MASS::fitdistr(vec, distr)
  }
  print(paste(string, f$loglik, f$aic))
  return(f)
}</code></pre>
<p>Data loading:</p>
<pre class="r"><code># Load the CSV files. If the age_group fill model is run again and we get a different output,
# then remember to update the value here
loadCSV = function(filename, age_group_fill, contrast_func) {
  dat = read_csv(
    file.path(data_folder, filename),
    col_types = cols(
      study_condition = col_factor(),
      suggestion_type = col_factor(levels = c(&quot;NONE&quot;, &quot;AX&quot;, &quot;DX&quot;, &quot;DXAX&quot;)),
      noise_level = col_factor(ordered = T),
      # gender = col_factor(levels = c(&quot;F&quot;, &quot;M&quot;, &quot;U&quot;)),
      # age_group = col_factor(levels = seq(from = 0, to = 8)),  # We don&#39;t load this as a factor yet
      robot_experience = col_factor(levels = seq(from = 0, to = 4))
    )
  )

  # Relabel the factors
  dat = dat %&gt;%
    mutate(study_condition = fct_recode(study_condition,
                                        BASELINE=&quot;1&quot;,
                                        DX_100=&quot;2&quot;, AX_100=&quot;3&quot;, DXAX_100=&quot;4&quot;,
                                        DX_90=&quot;5&quot;, AX_90=&quot;6&quot;, DXAX_90=&quot;7&quot;,
                                        DX_80=&quot;8&quot;, AX_80=&quot;9&quot;, DXAX_80=&quot;10&quot;)) %&gt;%
    mutate(study_condition = fct_relevel(study_condition, c(&quot;DX_100&quot;, &quot;AX_100&quot;, &quot;DXAX_100&quot;,
                                                            &quot;DX_90&quot;, &quot;AX_90&quot;, &quot;DXAX_90&quot;,
                                                            &quot;DX_80&quot;, &quot;AX_80&quot;, &quot;DXAX_80&quot;)))

  # Relevel the non-binary gender and make it a factor
  dat = dat %&gt;%
    mutate(gender = ifelse(gender == &quot;U&quot;, &quot;M&quot;, gender)) %&gt;%
    mutate(gender = factor(gender, levels = c(&quot;F&quot;, &quot;M&quot;)))

  # Change binary responses to integers
  dat = dat %&gt;% mutate(scenario_completed = as.integer(scenario_completed))

  # Relevel age_group and make it a factor
  dat = dat %&gt;%
    mutate(age_group = ifelse(age_group == 0, age_group_fill, age_group)) %&gt;%
    mutate(age_group = factor(age_group, levels = seq(from = 0, to = 8)))

  # Create an unordered noise_level. Also drop unused levels
  dat$age_group = droplevels(dat$age_group)
  dat$gender = droplevels(dat$gender)
  dat$noise_level_f = factor(dat$noise_level, ordered = F)

  # Set the contrasts for everything
  contrasts(dat$age_group) = contrast_func(length(levels(dat$age_group)))
  contrasts(dat$robot_experience) = contrast_func(length(levels(dat$robot_experience)))
  contrasts(dat$gender) = contrast_func(length(levels(dat$gender)))

  contrasts(dat$has_ax) = contrast_func(2)
  contrasts(dat$has_dx) = contrast_func(2)
  contrasts(dat$has_noise) = contrast_func(2)
  contrasts(dat$has_dxax) = contrast_func(2)
  contrasts(dat$has_ax_only) = contrast_func(2)
  contrasts(dat$has_dx_only) = contrast_func(2)
  contrasts(dat$has_suggestions) = contrast_func(2)
  contrasts(dat$suggestion_type) = contr.helmert(4)  # This is a hardcode

  contrasts(dat$noise_level_f) = contrast_func(length(levels(dat$noise_level_f)))

  # Return the data frame
  return(dat)
}

# Get the users df and the actions df
users = loadCSV(&quot;users.csv&quot;, 4, contr.sum)
actions = loadCSV(&quot;actions.csv&quot;, 4, contr.sum)

# Change more binary responses to integers
# actions %&gt;%
#   mutate(optimal_ax)
actions = actions %&gt;%
  mutate(optimal_ax = as.integer(optimal_ax),
         chose_ax = as.integer(chose_ax),
         optimal_dx = as.integer(optimal_dx),
         chose_dx = as.integer(chose_dx))

# Relabel user ids to be in the range 1-200. Otherwise, we&#39;re using DB ids
# The user ID column in the users table that we can now join on is X1
actions$user_id = actions %&gt;% group_indices(user_id)

# Rescale state ids
actions$state_idx_rescaled = scales::rescale(actions$state_idx)

# Code to relevel the age group factor to remove unused levels. Also, predict the
# value of the 1 unknown age from the other indicators of demographics
# NOTE: If we retrain the model and the output doesn&#39;t match the hardcoded
# values, then update those values
if (train_models) {
  plot_df = subset(users, users$age_group != 0)
  plot_df$age_group = factor(plot_df$age_group, levels = seq(from = 0, to = 8), ordered = T)
  plot_df$age_group = droplevels(plot_df$age_group)
  age_group_model = brm(age_group ~ gender + robot_experience, data = plot_df, family = &quot;cumulative&quot;)
  data_to_predict = users %&gt;% filter(age_group == 0) %&gt;% select(c(&quot;robot_experience&quot;, &quot;gender&quot;))
  as_tibble(predict(age_group_model, data_to_predict))
# # A tibble: 1 x 7
#   `P(Y = 2)` `P(Y = 3)` `P(Y = 4)` `P(Y = 5)` `P(Y = 6)` `P(Y = 7)` `P(Y = 8)`
#        &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;
# 1     0.0838      0.202      0.215      0.188      0.110      0.089      0.111
  rm(age_group_model)
}</code></pre>
<p>Note that in this notebook, the accuracy variable from the paper is coded as a noise level variable:</p>
<table>
<thead>
<tr class="header">
<th>Accuracy</th>
<th>Noise Level</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>100%</td>
<td>0</td>
</tr>
<tr class="even">
<td>90%</td>
<td>1</td>
</tr>
<tr class="odd">
<td>80%</td>
<td>2</td>
</tr>
</tbody>
</table>
<p>Since noise level increases as accuracy decreases, the “sign” of any linear trends observed in the following analyses should be reversed. Everything else stays pretty much the same.</p>
<p>For each of our dependent variables, there are 2 models:</p>
<ol style="list-style-type: decimal">
<li>With the noise variable as an ordered factor, so that we can make inferences on the trends in the variable</li>
<li>With the noise variable as an unordered factor, so that we can make inferences on the values of the variable</li>
</ol>
<p>We use Deviation / Sum coding in the coding of non-ordered factors so that we can test the influence of individual factor levels over and above the grand mean. We code the suggestion_type as (reverse) Helmert coding so that collinearity between the suggestion condition values is reduced (collinearity invalidates Bayesian hypothesis testing). (The reverse Helmert coding is the default form of helmert coding that’s followed in R).</p>
<p>In all our models:</p>
<ul>
<li><span class="math inline">\(\beta_0\)</span> is assumed to be the intercept. In a mixed effects model, this is additionally indexed by <span class="math inline">\(i\)</span>, the user; i.e. <span class="math inline">\(\beta_{0i}\)</span>.</li>
<li><span class="math inline">\(\text{suggestion_type}_i\)</span> which denotes the type of suggestion that the participant received. This factor is coded, using Helmert coding, into three sub-variables (<code>baseline</code> is a reference level for this factor):
<ul>
<li><span class="math inline">\(\text{ax_only}_i\)</span> denotes if sample <span class="math inline">\(i\)</span> received AX suggestions (only) or not</li>
<li><span class="math inline">\(\text{dx_only}_i\)</span> denotes if sample <span class="math inline">\(i\)</span> received DX suggestions (only) or not</li>
<li><span class="math inline">\(\text{dxax}_i\)</span> denotes if sample <span class="math inline">\(i\)</span> received DXAX suggestions or not</li>
</ul></li>
<li><span class="math inline">\(\text{noise}_i\)</span> denotes the level of noise in the suggestions that sample <span class="math inline">\(i\)</span> received. 0 (100% accuracy) if none was present.</li>
<li><span class="math inline">\(\mathbf{X_{demo,i}}\)</span> is a vector of demographic information. For one participant, this information is imputed from a simple linear model of the other participants.</li>
<li><span class="math inline">\(\text{no}_i\)</span> denotes the number of optimal actions for the scenario present in sample <span class="math inline">\(i\)</span>. This is a proxy for a difficulty rating of the error scenario given to the participant</li>
<li><span class="math inline">\(\text{state}_{ij}\)</span> denotes the state the user <span class="math inline">\(i\)</span> visited on action number <span class="math inline">\(j\)</span>. The sample, in this case, is indexed by <span class="math inline">\(j\)</span>. The states are indexed according to the frequency of users visits (0 = most visited state), and then all the indices are rescaled into the range 0-1.</li>
</ul>
<p>We test the following hypotheses (the explanations are a statement of the null hypotheses; the coefficients are from the expected regression parameters, given sum/Helmert coding):</p>
<ul>
<li><span class="math inline">\(2\beta_{ax} = 0\)</span>: The effect of action suggestions is no better than the baseline condition with no suggestions.</li>
<li><span class="math inline">\(3\beta_{dx} + \beta_{ax} = 0\)</span>: The effect of diagnosis suggestions is no better than the baseline condition with no suggestions.</li>
<li><span class="math inline">\(4\beta_{dxax} + \beta_{ax} + \beta_{dx} = 0\)</span>: The effect of both suggestions is no better than the baseline condition with no suggestions.</li>
<li><span class="math inline">\(\beta_{noise_L} = 0; \beta_{noise_Q} = 0\)</span>: Noise does not have a linear (L) / quadratic (Q) effect on the outcome.</li>
</ul>
<p>We do not include or test interaction effects in our model because interaction effects add more parameters to our model than can be estimated given the experimental design. Therefore, we assume that noise and the suggestion type have independent effects on each of the outcomes, and comparisons are made against the grand means of all the data.</p>
<p>The method of reporting and testing is based on the following papers:</p>
<ol style="list-style-type: decimal">
<li><a href="https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.12577">A protocol for conducting and presenting results of regression‐type analyses</a></li>
<li><a href="https://www.frontiersin.org/articles/10.3389/fpsyg.2019.02767/full">Indices of Effect Existence and Significance in the Bayesian Framework</a>. The paper is associated with <a href="https://easystats.github.io/bayestestR/articles/guidelines.html">this post</a> on how to present results, and <a href="https://easystats.github.io/bayestestR/articles/region_of_practical_equivalence.html">this post</a> giving a quick overview of terms (the posts are part of a package I’m using heavily in these analyses)</li>
</ol>
<p>Note taht we are NOT going to perform model-selection here. Based on what I’ve read, we’re doing confirmatory hypothesis testing, which is not where one should use model selection paradigms.</p>
<div id="frr-fault-resolution-rate" class="section level1">
<h1>FRR: Fault Resolution Rate</h1>
<p><strong>Did the person complete the scenario or not?</strong></p>
<p>In the code, this variable is called <code>scenario_completed</code>.</p>
<pre class="r"><code>plot_df = users %&gt;%
  select(X1, id, study_condition, start_condition, num_optimal,
         age_group, gender, robot_experience,
         noise_level, noise_level_f, has_noise, suggestion_type,
         has_dx, has_ax, has_ax_only, has_dx_only, has_dxax,
         scenario_completed)

print(summary(plot_df %&gt;%
                mutate(scenario_completed = factor(scenario_completed)) %&gt;%
                select(-X1, -id)
))</code></pre>
<pre><code>##  study_condition start_condition     num_optimal   age_group gender
##  DX_100  :20     Length:200         Min.   :3.00   2:21      F: 73
##  AX_100  :20     Class :character   1st Qu.:3.75   3:47      M:127
##  DXAX_100:20     Mode  :character   Median :4.00   4:44
##  DX_90   :20                        Mean   :4.50   5:33
##  AX_90   :20                        3rd Qu.:4.75   6:19
##  DXAX_90 :20                        Max.   :7.00   7:16
##  (Other) :80                                       8:20
##  robot_experience noise_level noise_level_f has_noise   suggestion_type
##  0:133            0.0:80      0.0:80        FALSE: 80   NONE:20
##  1: 33            1.0:60      1.0:60        TRUE :120   AX  :60
##  2: 12            2.0:60      2.0:60                    DX  :60
##  3: 12                                                  DXAX:60
##  4: 10
##
##
##    has_dx      has_ax    has_ax_only has_dx_only  has_dxax   scenario_completed
##  FALSE: 80   FALSE: 80   FALSE:140   FALSE:140   FALSE:140   0: 38
##  TRUE :120   TRUE :120   TRUE : 60   TRUE : 60   TRUE : 60   1:162
##
##
##
##
## </code></pre>
<pre class="r"><code>print(as.matrix(
  plot_df %&gt;%
    filter(scenario_completed == 1) %&gt;%
    group_by(study_condition) %&gt;%
    count() %&gt;%
    mutate(.value = n/20) %&gt;%
    select(-n)
))</code></pre>
<pre><code>##       study_condition .value
##  [1,] &quot;DX_100&quot;        &quot;0.70&quot;
##  [2,] &quot;AX_100&quot;        &quot;1.00&quot;
##  [3,] &quot;DXAX_100&quot;      &quot;0.90&quot;
##  [4,] &quot;DX_90&quot;         &quot;0.60&quot;
##  [5,] &quot;AX_90&quot;         &quot;0.80&quot;
##  [6,] &quot;DXAX_90&quot;       &quot;0.90&quot;
##  [7,] &quot;DX_80&quot;         &quot;0.65&quot;
##  [8,] &quot;AX_80&quot;         &quot;0.95&quot;
##  [9,] &quot;DXAX_80&quot;       &quot;0.85&quot;
## [10,] &quot;BASELINE&quot;      &quot;0.75&quot;</code></pre>
<div id="data" class="section level2">
<h2>Data</h2>
<pre class="r"><code># Plot by the study condition
plot_df %&gt;%
  mutate(scenario_completed = factor(scenario_completed)) %&gt;%
  mutate(scenario_completed = fct_recode(scenario_completed, incomplete=&quot;0&quot;, complete = &quot;1&quot;)) %&gt;%
  count(study_condition, scenario_completed) %&gt;%
  ggplot(aes(study_condition, n / 20, fill=scenario_completed)) +
    geom_bar(stat=&quot;identity&quot;) +
    labs(y = &quot;Fraction completed&quot;) +
    scale_fill_economist()</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-6-1.png' %}" width="1440" /></p>
<pre class="r"><code># Plot by suggestion type
gg_df = plot_df %&gt;%
  mutate(scenario_completed = factor(scenario_completed)) %&gt;%
  mutate(scenario_completed = fct_recode(scenario_completed, incomplete=&quot;0&quot;, complete = &quot;1&quot;)) %&gt;%
  mutate(noise_level = fct_rev(noise_level)) %&gt;%
  count(suggestion_type, noise_level, scenario_completed)

gg_df %&gt;%
  ggplot(aes(suggestion_type, n/20, fill=scenario_completed)) +
    geom_bar(stat = &quot;identity&quot;) +
    facet_grid(cols = vars(noise_level), labeller = label_both) +
    labs(y = &quot;Fraction completed&quot;) +
    scale_fill_economist() +
    theme(legend.position = &quot;bottom&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-6-2.png' %}" width="1440" /></p>
<pre class="r"><code>gg_df %&gt;%
  ggplot(aes(noise_level, n/20, fill=scenario_completed)) +
    geom_bar(stat = &quot;identity&quot;) +
    facet_grid(cols = vars(suggestion_type)) +
    labs(y = &quot;Fraction completed&quot;) +
    scale_fill_economist() +
    theme(legend.position = &quot;bottom&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-6-3.png' %}" width="1440" /></p>
<pre class="r"><code>p1 = gg_df %&gt;%
  filter(scenario_completed == &#39;complete&#39;) %&gt;%
  ggplot(aes(suggestion_type, n/20)) +
    geom_violin() +
    geom_point(aes(color = paste(suggestion_type, noise_level, sep=&quot;:&quot;), size = 2),
               position = position_jitter(width = .2, height = 0)) +
    stat_summary(fun = median, geom=&quot;point&quot;, shape=23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    legend_none()</code></pre>
<pre><code>## Warning: Ignoring unknown parameters: fun</code></pre>
<pre class="r"><code>p2 = gg_df %&gt;%
  filter(scenario_completed == &#39;complete&#39;) %&gt;%
  ggplot(aes(noise_level, n/20)) +
    geom_violin() +
    geom_point(aes(color = paste(suggestion_type, noise_level, sep=&quot;:&quot;), size = 2),
               position = position_jitter(width = .2, height = 0)) +
    stat_summary(fun = median, geom=&quot;point&quot;, shape=23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(color = &quot;Type:Noise&quot;) +
    theme(legend.position = &quot;right&quot;) +
    guides(size = F)</code></pre>
<pre><code>## Warning: Ignoring unknown parameters: fun</code></pre>
<pre class="r"><code>grid.arrange(p1, p2, ncol = 2)</code></pre>
<pre><code>## No summary function supplied, defaulting to `mean_se()
## No summary function supplied, defaulting to `mean_se()</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-6-4.png' %}" width="1440" /></p>
</div>
<div id="model" class="section level2">
<h2>Model</h2>
<p>Based on the data, we assume the following structural model:</p>
<p><span class="math display">\[FRR_i = Bernoulli(p_i)\]</span> <span class="math display">\[\begin{aligned}
logit(p_i) &amp;= \beta_0 + \beta_{ax}\text{ax_only}_i + \beta_{dx}\text{dx_only}_i + \beta_{dxax}\text{dxax}_i +\\ &amp;\beta_{noise_L}\text{noise}_i + \beta_{noise_Q}\text{noise}_i +\\
&amp;\beta_{no}\text{no}_i + \mathbf{\beta_{demo}X_{demo,i}}
\end{aligned}\]</span> <span class="math display">\[\beta_{.} \sim Normal(0, 10)\]</span></p>
<p>Model fitting:</p>
<pre class="r"><code># A null model to compare against
scenario_completed.model.null = brm(
  scenario_completed ~ 0 + Intercept,
  family = &quot;bernoulli&quot;,
  prior = set_prior(&quot;normal(0, 10)&quot;, class = &quot;b&quot;),
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
scenario_completed.model.null = add_criterion(scenario_completed.model.null, &quot;waic&quot;)
scenario_completed.model.null = add_criterion(scenario_completed.model.null, &quot;loo&quot;, reloo = T)
saveModel(scenario_completed.model.null)

# The trend model to see if there is a trend in the noise level variable
scenario_completed.model.t = brm(
  scenario_completed ~ 0 + Intercept + (suggestion_type + noise_level) + (gender + age_group + robot_experience) + num_optimal,
  family = &quot;bernoulli&quot;,
  prior = set_prior(&quot;normal(0, 10)&quot;, class = &quot;b&quot;),
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
scenario_completed.model.t = add_criterion(scenario_completed.model.t, &quot;waic&quot;)
scenario_completed.model.t = add_criterion(scenario_completed.model.t, &quot;loo&quot;, reloo = T)
saveModel(scenario_completed.model.t)

# The values model, to see if specific values of the noise level variable are significant
scenario_completed.model.v = brm(
  scenario_completed ~ 0 + Intercept + (suggestion_type + noise_level_f) + (gender + age_group + robot_experience) + num_optimal,
  family = &quot;bernoulli&quot;,
  prior = set_prior(&quot;normal(0, 10)&quot;, class = &quot;b&quot;),
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
scenario_completed.model.v = add_criterion(scenario_completed.model.v, &quot;waic&quot;)
scenario_completed.model.v = add_criterion(scenario_completed.model.v, &quot;loo&quot;, reloo = T)
saveModel(scenario_completed.model.v)</code></pre>
<pre class="r"><code># Load the models
scenario_completed.model.null = loadModel(&quot;scenario_completed.model.null&quot;)
scenario_completed.model.t = loadModel(&quot;scenario_completed.model.t&quot;)
scenario_completed.model.v = loadModel(&quot;scenario_completed.model.v&quot;)</code></pre>
<p>Model fitting results:</p>
<pre class="r"><code>print(performance::r2(scenario_completed.model.null))</code></pre>
<pre><code>## # Bayesian R2 with Standard Error
##
##   Conditional R2: 0.000 [0.000]</code></pre>
<pre class="r"><code># Print the parameters, and some initial diagnostics
print(model_parameters(scenario_completed.model.t,
                       centrality = &quot;median&quot;,
                       ci = 0.89,
                       ci_method = &quot;hdi&quot;,
                       test = c(&quot;pd&quot;, &quot;rope&quot;),
                       rope_range = c(-0.055, 0.055),
                       rope_ci = 1,
                       effects = &quot;fixed&quot;))</code></pre>
<pre><code>## # Fixed effects
##
## Parameter         | Median |         89% CI |     pd | % in ROPE |  ESS |  Rhat
## -------------------------------------------------------------------------------
## (Intercept)       |   2.19 | [ 0.86,  3.44] | 99.80% |     0.12% | 2589 | 1.001
## suggestion_type1  |   0.94 | [ 0.14,  1.69] | 97.20% |     1.38% | 3721 | 0.999
## suggestion_type2  |  -0.42 | [-0.74, -0.10] | 98.47% |     2.43% | 3470 | 1.000
## suggestion_type3  |   0.26 | [ 0.03,  0.49] | 97.32% |     5.83% | 4359 | 0.999
## noise_level.L     |  -0.30 | [-0.99,  0.45] | 73.95% |     7.78% | 3480 | 1.000
## noise_level.Q     |   0.76 | [ 0.12,  1.40] | 97.00% |     1.75% | 3692 | 1.000
## gender1           |  -0.10 | [-0.46,  0.28] | 65.40% |    16.95% | 4315 | 1.001
## age_group1        |   0.43 | [-0.76,  1.57] | 72.82% |     4.80% | 3410 | 1.001
## age_group2        |  -0.51 | [-1.22,  0.23] | 85.70% |     5.22% | 3326 | 1.000
## age_group3        |  -0.27 | [-0.98,  0.45] | 71.88% |     8.88% | 3967 | 1.000
## age_group4        |   1.28 | [ 0.29,  2.48] | 98.02% |     0.90% | 3720 | 1.000
## age_group5        |  -1.28 | [-2.20, -0.29] | 98.35% |     0.75% | 3703 | 1.001
## age_group6        |   1.13 | [-0.20,  2.41] | 94.00% |     1.70% | 3049 | 1.001
## robot_experience1 |   1.21 | [ 0.54,  1.85] | 99.92% |     0.18% | 3197 | 1.000
## robot_experience2 |   0.87 | [ 0.01,  1.80] | 94.53% |     2.12% | 3758 | 1.000
## robot_experience3 |  -1.09 | [-2.14,  0.05] | 94.38% |     2.30% | 3327 | 1.000
## robot_experience4 |  -0.91 | [-1.93,  0.16] | 91.53% |     2.62% | 3700 | 1.000
## num_optimal       |  -0.24 | [-0.49, -0.02] | 94.77% |     8.28% | 2868 | 1.001</code></pre>
<pre class="r"><code>print(performance::r2(scenario_completed.model.t))</code></pre>
<pre><code>## # Bayesian R2 with Standard Error
##
##   Conditional R2: 0.250 [0.041]</code></pre>
<pre class="r"><code>print(model_parameters(scenario_completed.model.v,
                       centrality = &quot;median&quot;,
                       ci = 0.89,
                       ci_method = &quot;hdi&quot;,
                       test = c(&quot;pd&quot;, &quot;rope&quot;),
                       rope_range = c(-0.055, 0.055),
                       rope_ci = 1,
                       effects = &quot;fixed&quot;))</code></pre>
<pre><code>## # Fixed effects
##
## Parameter         | Median |         89% CI |     pd | % in ROPE |  ESS |  Rhat
## -------------------------------------------------------------------------------
## (Intercept)       |   2.22 | [ 1.01,  3.57] | 99.78% |     0.07% | 2376 | 1.001
## suggestion_type1  |   0.93 | [ 0.16,  1.72] | 97.45% |     1.52% | 3073 | 1.000
## suggestion_type2  |  -0.43 | [-0.74, -0.13] | 98.92% |     2.02% | 3553 | 1.000
## suggestion_type3  |   0.26 | [ 0.04,  0.49] | 97.05% |     5.58% | 3833 | 0.999
## noise_level_f1    |   0.51 | [-0.06,  1.13] | 91.47% |     5.00% | 2707 | 1.000
## noise_level_f2    |  -0.61 | [-1.14, -0.04] | 96.80% |     2.57% | 3127 | 1.000
## gender1           |  -0.10 | [-0.46,  0.29] | 66.25% |    16.62% | 4870 | 1.000
## age_group1        |   0.43 | [-0.73,  1.63] | 72.22% |     5.47% | 3007 | 1.000
## age_group2        |  -0.52 | [-1.29,  0.17] | 86.75% |     5.30% | 3793 | 1.000
## age_group3        |  -0.28 | [-0.95,  0.52] | 73.25% |     8.03% | 3856 | 1.000
## age_group4        |   1.30 | [ 0.23,  2.43] | 98.12% |     0.88% | 3619 | 1.000
## age_group5        |  -1.29 | [-2.20, -0.33] | 98.35% |     0.83% | 3189 | 1.000
## age_group6        |   1.14 | [-0.11,  2.48] | 94.27% |     1.93% | 3039 | 1.000
## robot_experience1 |   1.22 | [ 0.60,  1.91] |   100% |     0.10% | 3267 | 1.001
## robot_experience2 |   0.85 | [ 0.01,  1.82] | 94.23% |     2.33% | 4036 | 1.000
## robot_experience3 |  -1.08 | [-2.15,  0.03] | 94.38% |     1.60% | 3397 | 1.001
## robot_experience4 |  -0.91 | [-2.04,  0.13] | 91.00% |     2.38% | 3864 | 1.000
## num_optimal       |  -0.24 | [-0.48, -0.02] | 95.10% |     8.05% | 2579 | 1.000</code></pre>
<pre class="r"><code>print(performance::r2(scenario_completed.model.v))</code></pre>
<pre><code>## # Bayesian R2 with Standard Error
##
##   Conditional R2: 0.249 [0.040]</code></pre>
</div>
<div id="diagnostic" class="section level2">
<h2>Diagnostic</h2>
<pre class="r"><code># Alternative diagnostics: diagnostic_posterior(scenario_completed.model.v)

# LOO cross-validation results
print(scenario_completed.model.null$criteria$loo$estimates)</code></pre>
<pre><code>##           Estimate         SE
## elpd_loo -98.28041  8.1859369
## p_loo      1.03581  0.1164151
## looic    196.56083 16.3718739</code></pre>
<pre class="r"><code>print(scenario_completed.model.t$criteria$loo$estimates)</code></pre>
<pre><code>##            Estimate        SE
## elpd_loo -100.81823 10.476017
## p_loo      23.74845  2.957806
## looic     201.63647 20.952034</code></pre>
<pre class="r"><code>print(scenario_completed.model.v$criteria$loo$estimates)</code></pre>
<pre><code>##            Estimate        SE
## elpd_loo -100.48415 10.443425
## p_loo      23.41446  2.908508
## looic     200.96830 20.886851</code></pre>
<pre class="r"><code>print(loo_compare(scenario_completed.model.null,
                  scenario_completed.model.t,
                  scenario_completed.model.v))</code></pre>
<pre><code>##                               elpd_diff se_diff
## scenario_completed.model.null  0.0       0.0
## scenario_completed.model.v    -2.2       6.8
## scenario_completed.model.t    -2.5       6.9</code></pre>
<pre class="r"><code># Diagnostics of the sampling process
par(mfrow=c(1,2))
plot(scenario_completed.model.t$criteria$loo, main = &quot;Trend Model&quot;)
plot(scenario_completed.model.v$criteria$loo, main = &quot;Values Model&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-10-1.png' %}" width="1440" /></p>
<pre class="r"><code># Include posterior predictive checks of the sampling (are sampling according
# to the data that we actually have?)
# mcmc_intervals(as.matrix(scenario_completed.model.null), regex_pars = &quot;b_&quot;, prob = 0.90, prob_outer = 0.95) +
#   ggtitle(&quot;Null Model&quot;)

p0 = pp_check(scenario_completed.model.null, type = &quot;bars&quot;, stat = &quot;median&quot;) + ggtitle(&quot;null model&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars&#39; by default.</code></pre>
<pre><code>## Warning: The following arguments were unrecognized and ignored: stat</code></pre>
<pre class="r"><code>p1 = pp_check(scenario_completed.model.t, type = &quot;bars_grouped&quot;, group = &quot;suggestion_type&quot;, stat = &quot;median&quot;) + ggtitle(&quot;t:suggestion_type&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars_grouped&#39; by default.</code></pre>
<pre><code>## Warning: The following arguments were unrecognized and ignored: stat</code></pre>
<pre class="r"><code>p2 = pp_check(scenario_completed.model.t, type = &quot;bars_grouped&quot;, group = &quot;noise_level&quot;, stat = &quot;median&quot;) + ggtitle(&quot;t:noise_level&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars_grouped&#39; by default.</code></pre>
<pre><code>## Warning: The following arguments were unrecognized and ignored: stat</code></pre>
<pre class="r"><code>grid.arrange(p1, p2, p0, nrow = 2, ncol = 2)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-10-2.png' %}" width="1440" /></p>
<pre class="r"><code>mcmc_intervals(as.matrix(scenario_completed.model.t), regex_pars = &quot;b_&quot;, prob = 0.90, prob_outer = 0.95) +
  ggtitle(&quot;Trends Model&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-10-3.png' %}" width="1440" /></p>
<pre class="r"><code>p1 = pp_check(scenario_completed.model.v, type = &quot;bars_grouped&quot;, group = &quot;suggestion_type&quot;, stat = &quot;median&quot;) + ggtitle(&quot;v:suggestion_type&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars_grouped&#39; by default.</code></pre>
<pre><code>## Warning: The following arguments were unrecognized and ignored: stat</code></pre>
<pre class="r"><code>p2 = pp_check(scenario_completed.model.v, type = &quot;bars_grouped&quot;, group = &quot;noise_level_f&quot;, stat = &quot;median&quot;) + ggtitle(&quot;v:noise_level&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars_grouped&#39; by default.</code></pre>
<pre><code>## Warning: The following arguments were unrecognized and ignored: stat</code></pre>
<pre class="r"><code>grid.arrange(p1, p2, p0, nrow = 2, ncol = 2)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-10-4.png' %}" width="1440" /></p>
<pre class="r"><code>mcmc_intervals(as.matrix(scenario_completed.model.v), regex_pars = &quot;b_&quot;, prob = 0.90, prob_outer = 0.95) +
  ggtitle(&quot;Values Model&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-10-5.png' %}" width="1440" /></p>
<pre class="r"><code># Plot predictions vs original
preds_p_df = scenario_completed.model.null %&gt;% predict() %&gt;% as_tibble()
p1 =
  preds_p_df %&gt;%
    add_column(scenario_completed = scenario_completed.model.null$data$scenario_completed) %&gt;%
    ggplot(aes(x = Estimate, y = scenario_completed, color = scenario_completed)) +
      geom_point() +
      labs(x = &quot;Predicted values&quot;) +
      ylim(0, 1) +
      xlim(0, 1) +
      ggtitle(&quot;Null Model&quot;)

preds_p_df = plot_df %&gt;%
  add_predicted_draws(scenario_completed.model.null) %&gt;%
  group_by(scenario_completed, .prediction) %&gt;%
  count()
p2 =
  preds_p_df %&gt;%
    ggplot(aes(x = scenario_completed, y = .prediction, fill = n / sum(preds_p_df$n))) +
      geom_tile() +
      geom_text(aes(label = sprintf(&quot;%1.2f&quot;, n / sum(preds_p_df$n))), size = 10, vjust = 1) +
      scale_fill_gradient(limits = c(0, 1))

preds_p_df = scenario_completed.model.t %&gt;% predict() %&gt;% as_tibble()
p3 =
  preds_p_df %&gt;%
    add_column(scenario_completed = scenario_completed.model.t$data$scenario_completed) %&gt;%
    ggplot(aes(x = Estimate, y = scenario_completed, color = scenario_completed)) +
      geom_point() +
      labs(x = &quot;Predicted values&quot;) +
      ylim(0, 1) +
      xlim(0, 1) +
      ggtitle(&quot;Trends Model&quot;)

preds_p_df = plot_df %&gt;%
  add_predicted_draws(scenario_completed.model.t) %&gt;%
  group_by(scenario_completed, .prediction) %&gt;%
  count()
p4 =
  preds_p_df %&gt;%
    ggplot(aes(x = scenario_completed, y = .prediction, fill = n / sum(preds_p_df$n))) +
      geom_tile() +
      geom_text(aes(label = sprintf(&quot;%1.2f&quot;, n / sum(preds_p_df$n))), size = 10, vjust = 1) +
      scale_fill_gradient(limits = c(0, 1))

# Plot predictions vs original
preds_p_df = scenario_completed.model.v %&gt;% predict() %&gt;% as_tibble()
p5 =
  preds_p_df %&gt;%
    add_column(scenario_completed = scenario_completed.model.v$data$scenario_completed) %&gt;%
    ggplot(aes(x = Estimate, y = scenario_completed, color = scenario_completed)) +
      geom_point() +
      labs(x = &quot;Predicted values&quot;) +
      ylim(0, 1) +
      xlim(0, 1) +
      ggtitle(&quot;Values Model&quot;)

preds_p_df = plot_df %&gt;%
  add_predicted_draws(scenario_completed.model.v) %&gt;%
  group_by(scenario_completed, .prediction) %&gt;%
  count()
p6 =
  preds_p_df %&gt;%
    ggplot(aes(x = scenario_completed, y = .prediction, fill = n / sum(preds_p_df$n))) +
      geom_tile() +
      geom_text(aes(label = sprintf(&quot;%1.2f&quot;, n / sum(preds_p_df$n))), size = 10, vjust = 1) +
      scale_fill_gradient(limits = c(0, 1))

grid.arrange(p1, p3, p5, p2, p4, p6, nrow = 2, ncol = 3)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-10-6.png' %}" width="1440" /></p>
<pre class="r"><code>rm(p0, p1, p2, p3, p4, p5, p6)</code></pre>
<p>We are able to recreate the data, and there is a mild improvement in the recreation / prediction as a result of our model.</p>
</div>
<div id="inference" class="section level2">
<h2>Inference</h2>
<pre class="r"><code># Compute the bayes factor for each parameter value. Note: we compare
# here against the ROPE, and not the degenerate null hypothesis of 0
# Bayes-Factor values would be more significant in the latter case
# According to the Raftery, 1995 rules
# bf = 1 - 3: Weak
# bf = 3 - 20: Positive
# bf = 20 - 150: Strong
# bf &gt; 150: Very strong
print(bayesfactor_rope(scenario_completed.model.null, null = c(-0.055, 0.055)), n = 30)
print(bayesfactor_rope(scenario_completed.model.t, null = c(-0.055, 0.055)), n = 30)
print(bayesfactor_rope(scenario_completed.model.v, null = c(-0.055, 0.055)), n = 30)</code></pre>
<pre class="r"><code>options(digits = 3)
# Check the significance of each hypothesis
h_df = as_tibble(insight::get_parameters(scenario_completed.model.t)) %&gt;%
  transmute(
    Intercept = b_Intercept,
    has_ax_test = 2 * b_suggestion_type1,
    has_dx_test = (3 * b_suggestion_type2) + b_suggestion_type1,
    has_dxax_test = (4 * b_suggestion_type3) + b_suggestion_type1 + b_suggestion_type2,
    noise_levelL_test = b_noise_level.L,
    noise_levelQ_test = b_noise_level.Q
  )

hyp_results = test_hypotheses(h_df, rope_values = c(-0.055, 0.055))
print(hyp_results %&gt;% as.matrix())</code></pre>
<pre><code>##      Parameter           Estimate CI_low   CI_high Effect_Size pd
## [1,] &quot;Intercept&quot;         &quot; 2.190&quot; &quot; 0.862&quot; &quot;3.439&quot; &quot;1.207&quot;     &quot;0.998&quot;
## [2,] &quot;has_ax_test&quot;       &quot; 1.889&quot; &quot; 0.287&quot; &quot;3.373&quot; &quot;1.041&quot;     &quot;0.972&quot;
## [3,] &quot;has_dx_test&quot;       &quot;-0.337&quot; &quot;-1.760&quot; &quot;0.959&quot; &quot;0.186&quot;     &quot;0.649&quot;
## [4,] &quot;has_dxax_test&quot;     &quot; 1.571&quot; &quot; 0.158&quot; &quot;2.974&quot; &quot;0.866&quot;     &quot;0.962&quot;
## [5,] &quot;noise_levelL_test&quot; &quot;-0.297&quot; &quot;-0.991&quot; &quot;0.451&quot; &quot;0.164&quot;     &quot;0.740&quot;
## [6,] &quot;noise_levelQ_test&quot; &quot; 0.765&quot; &quot; 0.116&quot; &quot;1.401&quot; &quot;0.422&quot;     &quot;0.970&quot;
##      ROPE_Percentage ROPE_Equivalence
## [1,] &quot;0.00125&quot;       &quot;Rejected&quot;
## [2,] &quot;0.00725&quot;       &quot;Rejected&quot;
## [3,] &quot;0.04250&quot;       &quot;Undecided&quot;
## [4,] &quot;0.01150&quot;       &quot;Rejected&quot;
## [5,] &quot;0.07775&quot;       &quot;Undecided&quot;
## [6,] &quot;0.01750&quot;       &quot;Rejected&quot;</code></pre>
<pre class="r"><code>h_df = as_tibble(insight::get_parameters(scenario_completed.model.v)) %&gt;%
  transmute(
    Intercept = b_Intercept,
    has_ax_test = 2 * b_suggestion_type1,
    has_dx_test = (3 * b_suggestion_type2) + b_suggestion_type1,
    has_dxax_test = (4 * b_suggestion_type3) + b_suggestion_type1 + b_suggestion_type2,
    noise_level1_test = b_noise_level_f1,
    noise_level2_test = b_noise_level_f2,
    noise_level3_test = - b_noise_level_f1 - b_noise_level_f2
  )

hyp_results = test_hypotheses(h_df, rope_values = c(-0.055, 0.055))
print(hyp_results %&gt;% as.matrix())</code></pre>
<pre><code>##      Parameter           Estimate CI_low    CI_high   Effect_Size pd
## [1,] &quot;Intercept&quot;         &quot; 2.222&quot; &quot; 1.0126&quot; &quot; 3.5750&quot; &quot;1.2252&quot;    &quot;0.998&quot;
## [2,] &quot;has_ax_test&quot;       &quot; 1.853&quot; &quot; 0.3191&quot; &quot; 3.4356&quot; &quot;1.0218&quot;    &quot;0.975&quot;
## [3,] &quot;has_dx_test&quot;       &quot;-0.346&quot; &quot;-1.7984&quot; &quot; 0.9296&quot; &quot;0.1908&quot;    &quot;0.656&quot;
## [4,] &quot;has_dxax_test&quot;     &quot; 1.549&quot; &quot; 0.1708&quot; &quot; 3.0447&quot; &quot;0.8539&quot;    &quot;0.955&quot;
## [5,] &quot;noise_level1_test&quot; &quot; 0.508&quot; &quot;-0.0623&quot; &quot; 1.1288&quot; &quot;0.2801&quot;    &quot;0.915&quot;
## [6,] &quot;noise_level2_test&quot; &quot;-0.608&quot; &quot;-1.1355&quot; &quot;-0.0449&quot; &quot;0.3350&quot;    &quot;0.968&quot;
## [7,] &quot;noise_level3_test&quot; &quot; 0.096&quot; &quot;-0.4420&quot; &quot; 0.6510&quot; &quot;0.0529&quot;    &quot;0.609&quot;
##      ROPE_Percentage ROPE_Equivalence
## [1,] &quot;0.00075&quot;       &quot;Rejected&quot;
## [2,] &quot;0.00775&quot;       &quot;Rejected&quot;
## [3,] &quot;0.04550&quot;       &quot;Undecided&quot;
## [4,] &quot;0.01075&quot;       &quot;Rejected&quot;
## [5,] &quot;0.05000&quot;       &quot;Undecided&quot;
## [6,] &quot;0.02575&quot;       &quot;Undecided&quot;
## [7,] &quot;0.11275&quot;       &quot;Undecided&quot;</code></pre>
<pre class="r"><code>options(digits = 7)</code></pre>
<p><strong>Results</strong>:</p>
<ol style="list-style-type: decimal">
<li>AX suggestions
<ul>
<li>Positive effect pd = 97.2%</li>
<li>Median = 1.89, 89% CI [0.29, 3.37]</li>
<li>Large, Effect Size = 1.04</li>
<li>0.73% in ROPE</li>
</ul></li>
<li>DXAX suggestions
<ul>
<li>Positive effect pd = 96.2%</li>
<li>Median = 1.57, 89% CI [0.16, 2.97]</li>
<li>Large, Effect Size = 0.87</li>
<li>1.15% in ROPE</li>
</ul></li>
<li>Noise Level Quadratic
<ul>
<li>Positive (convex-shape) pd = 97.0%</li>
<li>Median = 0.77, 89% CI [0.12, 1.40]</li>
<li>Small, Effect Size = 0.42</li>
<li>1.75% in ROPE</li>
</ul></li>
</ol>
<p>(The ROPE is defined as [-0.055, 0.055]. It is the range is suggested in the literature for logistic models and it corresponds to a probability range of 0.11. Therefore, a probability change of less than 0.055 is considered no different from a probability change of 0)</p>
</div>
<div id="posterior-plots" class="section level2">
<h2>Posterior Plots</h2>
<pre class="r"><code># A visualization data frame
gg_df = plot_df %&gt;%
  mutate(scenario_completed = factor(scenario_completed)) %&gt;%
  mutate(scenario_completed = fct_recode(scenario_completed, incomplete=&quot;0&quot;, complete = &quot;1&quot;)) %&gt;%
  mutate(noise_level = fct_rev(noise_level)) %&gt;%
  count(has_ax_only, has_dx_only, has_dxax, noise_level, scenario_completed)

# Simulate predictions on new data
p_df =
  plot_df %&gt;%
  data_grid(scenario_completed,
            suggestion_type,
            noise_level_f,
            gender,
            age_group,
            robot_experience,
            num_optimal,
            .model = scenario_completed.model.v)

fits_p_df = p_df %&gt;%
  add_fitted_draws(scenario_completed.model.v) %&gt;%  # Linear predicted values
  mutate(has_ax_only = (suggestion_type == &quot;AX&quot;),
         has_dx_only = (suggestion_type == &quot;DX&quot;),
         has_dxax = (suggestion_type == &quot;DXAX&quot;)) %&gt;%
  mutate(has_ax_only = factor(has_ax_only, levels = c(F, T)),
         has_dx_only = factor(has_dx_only, levels = c(F, T)),
         has_dxax = factor(has_dxax, levels = c(F, T)))
pars_p_df = scenario_completed.model.t %&gt;% extract_draws(newdata = p_df %&gt;% rename(noise_level = noise_level_f))

# Plot the posterior distributions. In each of the following, the reference ROPE band
# can be marked based on 3 methods. Make sure to update that ROPE value baased on the
# method that we want to display.

# If centering ROPE on the Intercept (NULL or Fitted?)
# rope_value = inv_logit_scaled(fixef(scenario_completed.model.null)[&quot;Intercept&quot;,&quot;Estimate&quot;])
# rope_value = inv_logit_scaled(fixef(scenario_completed.model.t)[&quot;Intercept&quot;,&quot;Estimate&quot;])

# If centering ROPE on the actual data
# rope_value = gg_df %&gt;%
#   filter(scenario_completed == &#39;complete&#39;, has_ax_only == F) %&gt;%
#   summarise(.value = median(n) / 20) %&gt;%
#   pull(.value)

# If centering ROPE on the posterior distributions. Here we center the ROPE around the posterior
# predicted value for the BASELINE condition
preds_p_df = p_df %&gt;%
  add_predicted_draws(scenario_completed.model.v)  %&gt;% # 0, 1 predictions for each predicted value
  ungroup() %&gt;%
  count(suggestion_type, noise_level_f, .prediction) %&gt;%
  mutate(has_ax_only = (suggestion_type == &quot;AX&quot;),
         has_dx_only = (suggestion_type == &quot;DX&quot;),
         has_dxax = (suggestion_type == &quot;DXAX&quot;)) %&gt;%
  mutate(has_ax_only = factor(has_ax_only, levels = c(F, T)),
         has_dx_only = factor(has_dx_only, levels = c(F, T)),
         has_dxax = factor(has_dxax, levels = c(F, T)))
rope_value = median(
  preds_p_df %&gt;%
    filter(suggestion_type == &quot;NONE&quot;) %&gt;%
    spread(.prediction, n) %&gt;%
    mutate(.value = `1` / (`0` + `1`)) %&gt;%
    pull(.value)
)

gg_df %&gt;%
  filter(scenario_completed == &#39;complete&#39;) %&gt;%
  ggplot(aes(x = has_ax_only, y = n / 20)) +
    geom_rect(ymin = rope_value-0.055, ymax = rope_value+0.055,
              xmin = -Inf, xmax = Inf,
              color = &quot;transparent&quot;, fill = &quot;white&quot;) +
    annotate(&quot;text&quot;, x = 0.5, y = rope_value, label = &quot;ROPE&quot;, angle = 90) +
    stat_halfeye(aes(y = .value), data = fits_p_df, position = position_nudge(x = 0.1)) +
    geom_point(aes(color = paste(has_ax_only, has_dx_only, has_dxax, noise_level, sep = &quot;:&quot;)),
               size = 2, position = position_jitter(width = .1, height = 0)) +
    stat_summary(fun = median, geom = &quot;point&quot;, shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(y = &quot;FRR&quot;, color = &quot;AX:DX:DXAX:Noise&quot;)

gg_df %&gt;%
  filter(scenario_completed == &#39;complete&#39;) %&gt;%
  ggplot(aes(x = has_dx_only, y = n / 20)) +
    geom_rect(ymin = rope_value-0.055, ymax = rope_value+0.055,
              xmin = -Inf, xmax = Inf,
              color = &quot;transparent&quot;, fill = &quot;white&quot;) +
    annotate(&quot;text&quot;, x = 0.5, y = rope_value, label = &quot;ROPE&quot;, angle = 90) +
    stat_halfeye(aes(y = .value), data = fits_p_df, position = position_nudge(x = 0.1)) +
    geom_point(aes(color = paste(has_ax_only, has_dx_only, has_dxax, noise_level, sep = &quot;:&quot;)),
               size = 2, position = position_jitter(width = .1, height = 0)) +
    stat_summary(fun = median, geom = &quot;point&quot;, shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(y = &quot;FRR&quot;, color = &quot;AX:DX:DXAX:Noise&quot;)

gg_df %&gt;%
  filter(scenario_completed == &#39;complete&#39;) %&gt;%
  ggplot(aes(x = has_dxax, y = n / 20)) +
    geom_rect(ymin = rope_value-0.055, ymax = rope_value+0.055,
              xmin = -Inf, xmax = Inf,
              color = &quot;transparent&quot;, fill = &quot;white&quot;) +
    annotate(&quot;text&quot;, x = 0.5, y = rope_value, label = &quot;ROPE&quot;, angle = 90) +
    stat_halfeye(aes(y = .value), data = fits_p_df, position = position_nudge(x = 0.1)) +
    geom_point(aes(color = paste(has_ax_only, has_dx_only, has_dxax, noise_level, sep = &quot;:&quot;)),
               size = 2, position = position_jitter(width = .1, height = 0)) +
    stat_summary(fun = median, geom = &quot;point&quot;, shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(y = &quot;FRR&quot;, color = &quot;AX:DX:DXAX:Noise&quot;)

gg_df %&gt;%
  filter(scenario_completed == &#39;complete&#39;) %&gt;%
  mutate(noise_level = fct_rev(noise_level)) %&gt;%
  ggplot(aes(x = noise_level, y = n / 20)) +
    geom_rect(ymin = rope_value-0.055, ymax = rope_value+0.055,
              xmin = -Inf, xmax = Inf,
              color = &quot;transparent&quot;, fill = &quot;white&quot;) +
    annotate(&quot;text&quot;, x = 0.5, y = rope_value, label = &quot;ROPE&quot;, angle = 90) +
    stat_halfeye(aes(y = .value, x = noise_level_f), data = fits_p_df, position = position_nudge(x = 0.1)) +
    geom_point(aes(color = paste(has_ax_only, has_dx_only, has_dxax, noise_level, sep = &quot;:&quot;)),
               size = 2, position = position_jitter(width = .1, height = 0)) +
    stat_summary(fun = median, geom = &quot;point&quot;, shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(y = &quot;FRR&quot;, color = &quot;AX:DX:DXAX:Noise&quot;)

# Create a plot to show the trend
pars_p_df_X = pars_p_df$dpars$mu$fe$X[,c(&quot;Intercept&quot;, &quot;noise_level.Q&quot;)]
pars_p_df_b = pars_p_df$dpars$mu$fe$b[,c(&quot;b_Intercept&quot;, &quot;b_noise_level.Q&quot;)]
pars_p_df_y = pars_p_df_X %*% t(pars_p_df_b)[,as.integer(seq(from = 1, to = 4000, length.out = 100))]
pars_p_df_y = inv_logit_scaled(pars_p_df_y)
p_df = p_df %&gt;%
  bind_cols(as_tibble(pars_p_df_y)) %&gt;%
  gather(key = &quot;.sample&quot;, value = &quot;.value&quot;, V1:V100) %&gt;%
  rename(noise_level = noise_level_f)
gg_df %&gt;%
  filter(scenario_completed == &#39;complete&#39;) %&gt;%
  mutate(noise_level = fct_rev(noise_level)) %&gt;%
  ggplot(aes(x = noise_level, y = n / 20)) +
    geom_rect(ymin = rope_value-0.055, ymax = rope_value+0.055,
              xmin = -Inf, xmax = Inf,
              color = &quot;transparent&quot;, fill = &quot;white&quot;) +
    annotate(&quot;text&quot;, x = 0.5, y = rope_value, label = &quot;ROPE&quot;, angle = 90) +
    stat_halfeye(aes(y = .value, x = noise_level_f), data = fits_p_df, position = position_nudge(x = 0.1)) +
    geom_line(aes(y = .value, group = .sample, alpha = 0.3), data = p_df) +
    geom_point(aes(color = paste(has_ax_only, has_dx_only, has_dxax, noise_level, sep = &quot;:&quot;)),
               size = 2, position = position_jitter(width = .1, height = 0)) +
    stat_summary(fun = median, geom = &quot;point&quot;, shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(y = &quot;FRR&quot;, color = &quot;AX:DX:DXAX:Noise&quot;) +
    guides(alpha = F)

# Remove the giant fits data frames
rm(preds_p_df) # If using the posterior-predictions-based ROPE
rm(fits_p_df, pars_p_df, pars_p_df_b, pars_p_df_X, pars_p_df_y, p_df, gg_df)</code></pre>
<pre class="r"><code>eff = fixef(scenario_completed.model.t)
base_eff = inv_logit_scaled(
  eff[&quot;Intercept&quot;, &quot;Estimate&quot;]
  - eff[&quot;suggestion_type1&quot;, &quot;Estimate&quot;]
  - eff[&quot;suggestion_type2&quot;, &quot;Estimate&quot;]
  - eff[&quot;suggestion_type3&quot;, &quot;Estimate&quot;]
)
ax_eff = inv_logit_scaled(
  eff[&quot;Intercept&quot;, &quot;Estimate&quot;]
  + eff[&quot;suggestion_type1&quot;, &quot;Estimate&quot;]
  - eff[&quot;suggestion_type2&quot;, &quot;Estimate&quot;]
  - eff[&quot;suggestion_type3&quot;, &quot;Estimate&quot;]
)
dx_eff = inv_logit_scaled(
  eff[&quot;Intercept&quot;, &quot;Estimate&quot;]
  + (2 * eff[&quot;suggestion_type2&quot;, &quot;Estimate&quot;])
  - eff[&quot;suggestion_type3&quot;, &quot;Estimate&quot;]
)
dxax_eff = inv_logit_scaled(
  eff[&quot;Intercept&quot;, &quot;Estimate&quot;]
  + (3 * eff[&quot;suggestion_type3&quot;, &quot;Estimate&quot;])
)

gg_df = plot_df %&gt;%
  mutate(scenario_completed = factor(scenario_completed)) %&gt;%
  mutate(scenario_completed = fct_recode(scenario_completed, Unresolved=&quot;0&quot;, Resolved = &quot;1&quot;)) %&gt;%
  count(suggestion_type, has_ax_only, has_dx_only, has_dxax, noise_level, scenario_completed) %&gt;%
  mutate(has_ax_only = fct_recode(has_ax_only, &quot;No AX&quot;=&quot;FALSE&quot;, &quot;AX&quot;=&quot;TRUE&quot;),
         has_dx_only = fct_recode(has_dx_only, &quot;No DX&quot;=&quot;FALSE&quot;, &quot;DX&quot;=&quot;TRUE&quot;),
         has_dxax = fct_recode(has_dxax, &quot;No DXAX&quot;=&quot;FALSE&quot;, &quot;DXAX&quot;=&quot;TRUE&quot;),
         noise_level = fct_recode(noise_level, &quot;Acc: 100%&quot;=&quot;0.0&quot;, &quot;Acc: 90%&quot;=&quot;1.0&quot;, &quot;Acc: 80%&quot;=&quot;2.0&quot;)) %&gt;%
  mutate(estimate = ifelse(suggestion_type == &quot;NONE&quot;, base_eff, -1)) %&gt;%
  mutate(estimate = ifelse(suggestion_type == &quot;AX&quot;, ax_eff, estimate)) %&gt;%
  mutate(estimate = ifelse(suggestion_type == &quot;DX&quot;, dx_eff, estimate)) %&gt;%
  mutate(estimate = ifelse(suggestion_type == &quot;DXAX&quot;, dxax_eff, estimate))

p1 = gg_df %&gt;%
  filter(scenario_completed == &#39;Resolved&#39;) %&gt;%
  ggplot(aes(suggestion_type, n/20, group = suggestion_type, colour = suggestion_type)) +
    geom_count(aes(colour = NULL)) +
    geom_boxplot(aes(y = estimate)) +
    geom_signif(y_position = 1.05, xmin = 1, xmax = 2, annotation = &quot;***&quot;, textsize = 8, color = &quot;black&quot;) +
    geom_signif(y_position = 1.15, xmin = 1, xmax = 4, annotation = &quot;***&quot;, textsize = 8, color = &quot;black&quot;) +
    scale_y_continuous(limits = c(0.5, 1.2), breaks = c(0.5, 0.75, 1.0)) +
    labs(y = &quot;Fraction of participants that resolved fault&quot;, x = NULL) +
    scale_colour_manual(values = c(&quot;#e51400&quot;, &quot;#008a00&quot;, &quot;#f0a30a&quot;, &quot;#a4c400&quot;)) +
    theme_economist_white(base_size = 13) +
    theme(plot.background = element_rect(fill = &quot;white&quot;),
          axis.title.y = element_blank()) +
    legend_none()

eff = fixef(scenario_completed.model.v)
noise_0_eff = inv_logit_scaled(eff[&quot;Intercept&quot;, &quot;Estimate&quot;] + eff[&quot;noise_level_f1&quot;,&quot;Estimate&quot;])
noise_1_eff = inv_logit_scaled(eff[&quot;Intercept&quot;, &quot;Estimate&quot;] + eff[&quot;noise_level_f2&quot;,&quot;Estimate&quot;])
noise_2_eff = inv_logit_scaled(eff[&quot;Intercept&quot;, &quot;Estimate&quot;] - eff[&quot;noise_level_f1&quot;,&quot;Estimate&quot;] - eff[&quot;noise_level_f2&quot;,&quot;Estimate&quot;])

gg_df = gg_df %&gt;%
  mutate(estimate = if_else(noise_level == &quot;Acc: 100%&quot;, noise_0_eff, estimate)) %&gt;%
  mutate(estimate = if_else(noise_level == &quot;Acc: 90%&quot;, noise_1_eff, estimate)) %&gt;%
  mutate(estimate = if_else(noise_level == &quot;Acc: 80%&quot;, noise_2_eff, estimate)) %&gt;%
  mutate(noise_level = fct_rev(noise_level))

p2 = gg_df %&gt;%
  filter(scenario_completed == &#39;Resolved&#39;) %&gt;%
  ggplot(aes(noise_level, n/20, group = noise_level, colour = noise_level)) +
    geom_hline(yintercept = inv_logit_scaled(eff[&quot;Intercept&quot;, &quot;Estimate&quot;]),
               size = .7, linetype = &quot;dashed&quot;, color = &quot;grey&quot;) +
    geom_count(aes(colour = NULL)) +
    geom_boxplot(aes(y = estimate)) +
    annotate(&quot;segment&quot;,
             x = 3, y = noise_0_eff,
             xend = 2, yend = noise_1_eff,
             colour = &quot;black&quot;) +
    annotate(&quot;segment&quot;,
             x = 1, y = noise_2_eff,
             xend = 2, yend = noise_1_eff,
             color = &quot;black&quot;) +
    annotate(&quot;text&quot;,
             x = 3.5,
             y = inv_logit_scaled(eff[&quot;Intercept&quot;, &quot;Estimate&quot;] - 0.85),
             label = &quot;Mean of all levels&quot;,
             angle = 90,
             group = NA,
             colour = &quot;black&quot;) +
    scale_y_continuous(limits = c(0.5, 1.1), breaks = c(0.5, 1.0)) +
    labs(y = &quot;Fraction of participants that resolved fault&quot;, x = NULL) +
    scale_colour_manual(values = c(&quot;#e51400&quot;, &quot;#008a00&quot;, &quot;#f0a30a&quot;, &quot;#a4c400&quot;)) +
    scale_size_area(max_size = 1) +
    theme_economist_white(base_size = 13) +
    theme(plot.background = element_rect(fill = &quot;white&quot;),
          axis.title.y = element_blank(),
          axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
    legend_none()

grid.arrange(p1, p2, nrow = 1, ncol = 2)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-14-1.png' %}" width="672" /></p>
<pre class="r"><code>rm(p1, p2)</code></pre>
</div>
</div>
<div id="rax-rate-of-optimal-action-selection" class="section level1">
<h1>RAX: Rate of Optimal Action Selection</h1>
<p><strong>Did the user take an optimal action given the state that they were in?</strong></p>
<p>This is also a measure of Reliance on Suggestions. In the code, the variable might be referred to as <code>optimal_ax</code>, <code>correct_ax</code>, etc. depending on the version of the codebase</p>
<pre class="r"><code>plot_df = actions %&gt;%
  select(X1, id, user_id, study_condition, start_condition, num_optimal,
         state_idx, state_idx_rescaled,
         age_group, gender, robot_experience,
         noise_level, noise_level_f, has_noise, suggestion_type,
         has_dx, has_ax, has_ax_only, has_dx_only, has_dxax,
         scenario_completed, num_actions, optimal_ax)

print(summary(plot_df %&gt;%
                mutate(optimal_ax = factor(optimal_ax),
                       scenario_completed = factor(scenario_completed)) %&gt;%
                select(-X1, -id)
))</code></pre>
<pre><code>##     user_id      study_condition start_condition     num_optimal
##  Min.   :  1.0   DX_80   :256    Length:2126        Min.   :3.000
##  1st Qu.: 56.0   DX_90   :254    Class :character   1st Qu.:4.000
##  Median :104.0   DX_100  :247    Mode  :character   Median :4.000
##  Mean   :102.6   BASELINE:231                       Mean   :4.778
##  3rd Qu.:149.0   AX_90   :217                       3rd Qu.:7.000
##  Max.   :200.0   AX_80   :204                       Max.   :7.000
##                  (Other) :717
##    state_idx      state_idx_rescaled age_group gender   robot_experience
##  Min.   :  1.00   Min.   :0.0000     2:226     F: 844   0:1318
##  1st Qu.: 35.00   1st Qu.:0.2000     3:493     M:1282   1: 346
##  Median : 79.00   Median :0.4588     4:497              2: 161
##  Mean   : 86.64   Mean   :0.5037     5:283              3: 169
##  3rd Qu.:159.00   3rd Qu.:0.9294     6:222              4: 132
##  Max.   :171.00   Max.   :1.0000     7:170
##                                      8:235
##  noise_level noise_level_f has_noise    suggestion_type   has_dx
##  0.0:823     0.0:823       FALSE: 823   NONE:231        FALSE: 804
##  1.0:654     1.0:654       TRUE :1303   AX  :573        TRUE :1322
##  2.0:649     2.0:649                    DX  :757
##                                         DXAX:565
##
##
##
##    has_ax     has_ax_only  has_dx_only   has_dxax    scenario_completed
##  FALSE: 988   FALSE:1553   FALSE:1369   FALSE:1561   0: 760
##  TRUE :1138   TRUE : 573   TRUE : 757   TRUE : 565   1:1366
##
##
##
##
##
##   num_actions   optimal_ax
##  Min.   : 3.0   0:1301
##  1st Qu.: 8.0   1: 825
##  Median :15.0
##  Mean   :13.9
##  3rd Qu.:20.0
##  Max.   :20.0
## </code></pre>
<pre class="r"><code>print(as.matrix(
  plot_df %&gt;%
    filter(optimal_ax == 1) %&gt;%
    group_by(study_condition, num_actions, user_id) %&gt;%
    count() %&gt;%
    ungroup() %&gt;%
    mutate(.value = n/num_actions) %&gt;%
    group_by(study_condition) %&gt;%
    summarise(.value = median(.value))
))</code></pre>
<pre><code>##       study_condition .value
##  [1,] &quot;DX_100&quot;        &quot;0.5000000&quot;
##  [2,] &quot;AX_100&quot;        &quot;0.7638889&quot;
##  [3,] &quot;DXAX_100&quot;      &quot;0.5634921&quot;
##  [4,] &quot;DX_90&quot;         &quot;0.5714286&quot;
##  [5,] &quot;AX_90&quot;         &quot;0.5714286&quot;
##  [6,] &quot;DXAX_90&quot;       &quot;0.6410256&quot;
##  [7,] &quot;DX_80&quot;         &quot;0.5000000&quot;
##  [8,] &quot;AX_80&quot;         &quot;0.6153846&quot;
##  [9,] &quot;DXAX_80&quot;       &quot;0.6000000&quot;
## [10,] &quot;BASELINE&quot;      &quot;0.5357143&quot;</code></pre>
<div id="data-1" class="section level2">
<h2>Data</h2>
<pre class="r"><code># Plot by the study condition
gg_df =
  plot_df %&gt;%
    mutate(noise_level = fct_rev(noise_level)) %&gt;%
    mutate(optimal_ax = factor(optimal_ax)) %&gt;%
    mutate(user_id = factor(user_id)) %&gt;%
    mutate(optimal_ax = fct_recode(optimal_ax, incorrect=&quot;0&quot;, correct = &quot;1&quot;)) %&gt;%
    mutate(.fill_column = suggestion_type) %&gt;%
    count(user_id, study_condition, .fill_column, optimal_ax, num_actions, has_ax_only, has_dx_only, has_dxax, noise_level,
          name = &quot;num_optimal_ax&quot;)

gg_df %&gt;%
  ggplot(aes(user_id, num_optimal_ax / num_actions, fill=optimal_ax)) +
    geom_bar(stat=&quot;identity&quot;) +
    geom_hline(yintercept = 0.5) +
    facet_wrap(vars(study_condition), nrow = 1, scales = &quot;free_y&quot;) +
    labs(y = &quot;Fraction correct actions&quot;) +
    coord_flip() +
    scale_fill_economist()</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-17-1.png' %}" width="1440" /></p>
<pre class="r"><code>gg_df %&gt;%
  filter(optimal_ax == &quot;correct&quot;) %&gt;%
  ggplot(aes(study_condition, num_optimal_ax / num_actions, fill=.fill_column)) +
    geom_violin() +
    geom_boxplot(width = 0.1) +
    labs(y = &quot;Fraction correct actions&quot;, fill = &quot;Suggestion Type&quot;) +
    scale_fill_economist()</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-17-2.png' %}" width="1440" /></p>
<pre class="r"><code># Plot by suggestion type
gg_df =
  plot_df %&gt;%
    mutate(noise_level = fct_rev(noise_level)) %&gt;%
    mutate(optimal_ax = factor(optimal_ax)) %&gt;%
    mutate(user_id = factor(user_id)) %&gt;%
    mutate(optimal_ax = fct_recode(optimal_ax, incorrect=&quot;0&quot;, correct = &quot;1&quot;)) %&gt;%
    count(user_id, suggestion_type, noise_level, optimal_ax, num_actions,
          name = &quot;num_optimal_ax&quot;)

gg_df %&gt;%
  filter(optimal_ax == &quot;correct&quot;) %&gt;%
  ggplot(aes(suggestion_type, num_optimal_ax / num_actions, fill=suggestion_type)) +
    geom_boxplot() +
    geom_count() +
    facet_grid(cols = vars(noise_level), labeller = label_both) +
    labs(y = &quot;Fraction correct actions&quot;) +
    scale_fill_economist() +
    theme(legend.position = &quot;bottom&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-17-3.png' %}" width="1440" /></p>
<pre class="r"><code>gg_df %&gt;%
  filter(optimal_ax == &quot;correct&quot;) %&gt;%
  ggplot(aes(noise_level, num_optimal_ax / num_actions, fill=suggestion_type)) +
    geom_boxplot() +
    geom_count() +
    facet_grid(cols = vars(suggestion_type)) +
    labs(y = &quot;Fraction correct actions&quot;) +
    scale_fill_economist() +
    theme(legend.position = &quot;bottom&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-17-4.png' %}" width="1440" /></p>
<pre class="r"><code>p1 = gg_df %&gt;%
  filter(optimal_ax == &quot;correct&quot;) %&gt;%
  ggplot(aes(suggestion_type, num_optimal_ax / num_actions)) +
    geom_violin() +
    geom_point(aes(color = paste(suggestion_type, noise_level, sep = &quot;:&quot;)),
               size = 2, position = position_jitter(width = .2, height = 0)) +
    stat_summary(fun = median, geom = &quot;point&quot;, shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    legend_none()</code></pre>
<pre><code>## Warning: Ignoring unknown parameters: fun</code></pre>
<pre class="r"><code>p2 = gg_df %&gt;%
  filter(optimal_ax == &quot;correct&quot;) %&gt;%
  ggplot(aes(noise_level, num_optimal_ax / num_actions)) +
    geom_violin() +
    geom_point(aes(color = paste(suggestion_type, noise_level, sep = &quot;:&quot;)),
               size = 2, position = position_jitter(width = .1, height = 0)) +
    stat_summary(fun = median, geom = &quot;point&quot;, shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(color = &quot;Type:Noise&quot;) +
    theme(legend.position = &quot;right&quot;) +
    guides(size = F)</code></pre>
<pre><code>## Warning: Ignoring unknown parameters: fun</code></pre>
<pre class="r"><code>grid.arrange(p1, p2, ncol = 2)</code></pre>
<pre><code>## No summary function supplied, defaulting to `mean_se()
## No summary function supplied, defaulting to `mean_se()</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-17-5.png' %}" width="1440" /></p>
</div>
<div id="model-1" class="section level2">
<h2>Model</h2>
<p>Based on the data, we assume the following structural model:</p>
<p><span class="math display">\[RAX_{ij} = Bernoulli(p_{ij})\]</span> <span class="math display">\[\begin{aligned}
logit(p_{ij}) &amp;= \beta_0 + \beta_{0i} + \beta_{ax}\text{ax_only}_i + \beta_{dx}\text{dx_only}_i + \beta_{dxax}\text{dxax}_i +\\
&amp;\beta_{noise_L}\text{noise}_i + \beta_{noise_Q}\text{noise}_i +\\
&amp;\beta_{no}\text{no}_i + \mathbf{\beta_{demo}X_{demo,i}} + \beta_{state} \text{state}_{ij}
\end{aligned}\]</span> <span class="math display">\[\begin{aligned}
\beta_{.} &amp;\sim Normal(0, 10) \\
\beta_{0i} &amp;\sim Normal(0, \sigma_i) \\
\sigma_i &amp;\sim HalfStudent(3, 0, 10)
\end{aligned}\]</span></p>
<p>The prior for the <span class="math inline">\(\sigma_i\)</span> parameter is the default used in <code>brms</code>; I see no need to change it.</p>
<p>Model fitting:</p>
<pre class="r"><code># The null model
optimal_ax.model.null = brm(
  optimal_ax ~ 0 + Intercept + (1 | user_id),
  family = &quot;bernoulli&quot;,
  prior = c(set_prior(&quot;normal(0, 10)&quot;, class = &quot;b&quot;),
            set_prior(&quot;student_t(3, 0, 10)&quot;, class = &quot;sd&quot;)),
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
optimal_ax.model.null = add_criterion(optimal_ax.model.null, &quot;waic&quot;)
optimal_ax.model.null = add_criterion(optimal_ax.model.null, &quot;loo&quot;, reloo = T)
saveModel(optimal_ax.model.null)

# The trend model to see if there is a trend in the noise level variable
optimal_ax.model.t = brm(
  optimal_ax ~ 0 + Intercept + (suggestion_type + noise_level) + (age_group + robot_experience + gender) + num_optimal + state_idx_rescaled + (1 | user_id),
  family = bernoulli,
  prior = c(set_prior(&quot;normal(0, 10)&quot;, class = &quot;b&quot;),
            set_prior(&quot;student_t(3, 0, 10)&quot;, class = &quot;sd&quot;)),
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
optimal_ax.model.t = add_criterion(optimal_ax.model.t, &quot;waic&quot;)
optimal_ax.model.t = add_criterion(optimal_ax.model.t, &quot;loo&quot;, reloo = T)
saveModel(optimal_ax.model.t)

# The values model, to see if specific values of the noise level variable are significant
optimal_ax.model.v = brm(
  optimal_ax ~ 0 + Intercept + (suggestion_type + noise_level_f) + (age_group + robot_experience + gender) + num_optimal + state_idx_rescaled + (1 | user_id),
  family = bernoulli,
  prior = c(set_prior(&quot;normal(0, 10)&quot;, class = &quot;b&quot;),
            set_prior(&quot;student_t(3, 0, 10)&quot;, class = &quot;sd&quot;)),
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
optimal_ax.model.v = add_criterion(optimal_ax.model.v, &quot;waic&quot;)
optimal_ax.model.v = add_criterion(optimal_ax.model.v, &quot;loo&quot;, reloo = T)
saveModel(optimal_ax.model.v)</code></pre>
<pre class="r"><code># Load the models
optimal_ax.model.null = loadModel(&quot;optimal_ax.model.null&quot;)
optimal_ax.model.t = loadModel(&quot;optimal_ax.model.t&quot;)
optimal_ax.model.v = loadModel(&quot;optimal_ax.model.v&quot;)</code></pre>
<p>Model fitting results:</p>
<pre class="r"><code>print(performance::r2(optimal_ax.model.null))</code></pre>
<pre><code>## # Bayesian R2 with Standard Error
##
##   Conditional R2: 0.284 [0.013]
##      Marginal R2: 0.000 [0.000]</code></pre>
<pre class="r"><code># Print the parameters, and some initial diagnostics
print(model_parameters(optimal_ax.model.t,
                       centrality = &quot;median&quot;,
                       ci = 0.89,
                       ci_method = &quot;hdi&quot;,
                       test = c(&quot;pd&quot;, &quot;rope&quot;),
                       rope_range = c(-0.055, 0.055),
                       rope_ci = 1,
                       effects = &quot;fixed&quot;))</code></pre>
<pre><code>## # Fixed effects
##
## Parameter          | Median |         89% CI |     pd | % in ROPE |  ESS |  Rhat
## --------------------------------------------------------------------------------
## (Intercept)        |  -0.12 | [-0.85,  0.74] | 59.70% |     8.48% | 1994 | 1.000
## suggestion_type1   |   0.52 | [ 0.08,  0.97] | 96.97% |     3.23% | 2163 | 1.000
## suggestion_type2   |  -0.18 | [-0.39,  0.01] | 92.73% |    12.90% | 1806 | 1.001
## suggestion_type3   |   0.22 | [ 0.10,  0.35] | 99.67% |     1.85% | 2113 | 1.000
## noise_level.L      |  -0.21 | [-0.63,  0.20] | 78.88% |    12.07% | 2114 | 1.001
## noise_level.Q      |   0.26 | [-0.15,  0.65] | 85.55% |    10.45% | 2240 | 1.000
## age_group1         |   0.09 | [-0.59,  0.71] | 59.98% |    11.58% | 1935 | 1.000
## age_group2         |  -0.25 | [-0.68,  0.24] | 80.00% |    10.27% | 1976 | 1.002
## age_group3         |  -0.03 | [-0.50,  0.42] | 53.75% |    15.22% | 2046 | 1.001
## age_group4         |   0.80 | [ 0.26,  1.32] | 98.98% |     0.80% | 2172 | 1.000
## age_group5         |  -0.64 | [-1.26,  0.02] | 94.38% |     3.00% | 2047 | 1.001
## age_group6         |   0.59 | [-0.12,  1.29] | 90.33% |     3.70% | 2251 | 1.000
## robot_experience1  |   1.07 | [ 0.62,  1.47] |   100% |        0% | 2296 | 1.001
## robot_experience2  |   0.74 | [ 0.20,  1.29] | 98.17% |     1.07% | 1885 | 1.000
## robot_experience3  |  -0.25 | [-1.00,  0.56] | 68.27% |     6.83% | 2042 | 1.000
## robot_experience4  |  -0.74 | [-1.50,  0.02] | 93.83% |     2.35% | 2615 | 1.000
## gender1            |  -0.23 | [-0.47,  0.00] | 94.50% |     8.65% | 2119 | 1.002
## num_optimal        |   0.07 | [-0.07,  0.22] | 76.78% |    35.73% | 1948 | 1.000
## state_idx_rescaled |  -2.24 | [-2.57, -1.92] |   100% |        0% | 6100 | 1.000</code></pre>
<pre class="r"><code>print(performance::r2(optimal_ax.model.t))</code></pre>
<pre><code>## # Bayesian R2 with Standard Error
##
##   Conditional R2: 0.362 [0.013]
##      Marginal R2: 0.211 [0.018]</code></pre>
<pre class="r"><code>print(model_parameters(optimal_ax.model.v,
                       centrality = &quot;median&quot;,
                       ci = 0.89,
                       ci_method = &quot;hdi&quot;,
                       test = c(&quot;pd&quot;, &quot;rope&quot;),
                       rope_range = c(-0.055, 0.055),
                       rope_ci = 1,
                       effects = &quot;fixed&quot;))</code></pre>
<pre><code>## # Fixed effects
##
## Parameter          | Median |         89% CI |     pd | % in ROPE |  ESS |  Rhat
## --------------------------------------------------------------------------------
## (Intercept)        |  -0.12 | [-0.95,  0.68] | 59.90% |     9.40% | 1346 | 1.005
## suggestion_type1   |   0.51 | [ 0.03,  0.92] | 96.80% |     2.95% | 1666 | 1.000
## suggestion_type2   |  -0.18 | [-0.37,  0.02] | 92.67% |    12.38% | 1526 | 1.000
## suggestion_type3   |   0.21 | [ 0.08,  0.34] | 99.67% |     2.60% | 1690 | 1.001
## noise_level_f1     |   0.26 | [-0.07,  0.60] | 89.25% |     9.78% | 1623 | 1.000
## noise_level_f2     |  -0.22 | [-0.55,  0.11] | 86.05% |    12.28% | 1754 | 1.001
## age_group1         |   0.10 | [-0.52,  0.71] | 59.65% |    10.47% | 1960 | 1.001
## age_group2         |  -0.26 | [-0.73,  0.22] | 80.58% |     9.62% | 1923 | 1.000
## age_group3         |  -0.03 | [-0.51,  0.42] | 53.73% |    14.47% | 1838 | 1.002
## age_group4         |   0.79 | [ 0.31,  1.35] | 99.05% |     1.07% | 2085 | 1.001
## age_group5         |  -0.66 | [-1.33,  0.00] | 94.00% |     2.88% | 1784 | 1.002
## age_group6         |   0.58 | [-0.12,  1.31] | 90.70% |     4.28% | 2021 | 1.001
## robot_experience1  |   1.07 | [ 0.68,  1.53] |   100% |        0% | 1879 | 1.002
## robot_experience2  |   0.74 | [ 0.18,  1.32] | 98.35% |     1.30% | 1675 | 1.002
## robot_experience3  |  -0.24 | [-1.04,  0.52] | 69.58% |     7.95% | 1928 | 1.001
## robot_experience4  |  -0.76 | [-1.48,  0.05] | 94.00% |     2.88% | 2169 | 1.000
## gender1            |  -0.24 | [-0.47, -0.01] | 94.53% |     7.75% | 1975 | 1.002
## num_optimal        |   0.07 | [-0.07,  0.22] | 77.03% |    36.50% | 1437 | 1.004
## state_idx_rescaled |  -2.24 | [-2.56, -1.91] |   100% |        0% | 5918 | 1.000</code></pre>
<pre class="r"><code>print(performance::r2(optimal_ax.model.v))</code></pre>
<pre><code>## # Bayesian R2 with Standard Error
##
##   Conditional R2: 0.362 [0.013]
##      Marginal R2: 0.211 [0.019]</code></pre>
</div>
<div id="diagnostic-1" class="section level2">
<h2>Diagnostic</h2>
<pre class="r"><code># Alternative diagnostics: diagnostic_posterior(scenario_completed.model.v)

# LOO cross-validation results
print(optimal_ax.model.t$criteria$loo$estimates)</code></pre>
<pre><code>##            Estimate       SE
## elpd_loo -1080.2638 24.09449
## p_loo      153.3255  4.55021
## looic     2160.5276 48.18897</code></pre>
<pre class="r"><code>print(optimal_ax.model.v$criteria$loo$estimates)</code></pre>
<pre><code>##            Estimate        SE
## elpd_loo -1079.0160 24.069953
## p_loo      152.0003  4.514671
## looic     2158.0320 48.139906</code></pre>
<pre class="r"><code>print(loo_compare(optimal_ax.model.null, optimal_ax.model.t, optimal_ax.model.v))</code></pre>
<pre><code>##                       elpd_diff se_diff
## optimal_ax.model.v      0.0       0.0
## optimal_ax.model.t     -1.2       0.4
## optimal_ax.model.null -83.8      12.7</code></pre>
<pre class="r"><code># Diagnostics of the sampling process
par(mfrow=c(1,2))
plot(optimal_ax.model.t$criteria$loo, main = &quot;Trend Model&quot;)
plot(optimal_ax.model.v$criteria$loo, main = &quot;Values Model&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-21-1.png' %}" width="1440" /></p>
<pre class="r"><code># Include posterior predictive checks of the sampling (are sampling according
# to the data that we actually have?)
p0 = pp_check(optimal_ax.model.null, type = &quot;bars&quot;, stat = &quot;median&quot;) + ggtitle(&quot;null model&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars&#39; by default.</code></pre>
<pre><code>## Warning: The following arguments were unrecognized and ignored: stat</code></pre>
<pre class="r"><code>p1 = pp_check(optimal_ax.model.t, type = &quot;bars_grouped&quot;, group = &quot;suggestion_type&quot;, stat = &quot;median&quot;) + ggtitle(&quot;t:suggestion_type&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars_grouped&#39; by default.</code></pre>
<pre><code>## Warning: The following arguments were unrecognized and ignored: stat</code></pre>
<pre class="r"><code>p2 = pp_check(optimal_ax.model.t, type = &quot;bars_grouped&quot;, group = &quot;noise_level&quot;, stat = &quot;median&quot;) + ggtitle(&quot;t:noise_level&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars_grouped&#39; by default.</code></pre>
<pre><code>## Warning: The following arguments were unrecognized and ignored: stat</code></pre>
<pre class="r"><code>grid.arrange(p1, p2, p0, nrow = 2, ncol = 2)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-21-2.png' %}" width="1440" /></p>
<pre class="r"><code>mcmc_intervals(as.matrix(optimal_ax.model.t), regex_pars = &quot;b_|^shape&quot;, prob = 0.90, prob_outer = 0.95) +
  ggtitle(&quot;Trends Model&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-21-3.png' %}" width="1440" /></p>
<pre class="r"><code>p1 = pp_check(optimal_ax.model.v, type = &quot;bars_grouped&quot;, group = &quot;suggestion_type&quot;, stat = &quot;median&quot;) + ggtitle(&quot;v:suggestion_type&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars_grouped&#39; by default.</code></pre>
<pre><code>## Warning: The following arguments were unrecognized and ignored: stat</code></pre>
<pre class="r"><code>p4 = pp_check(optimal_ax.model.v, type = &quot;bars_grouped&quot;, group = &quot;noise_level_f&quot;, stat = &quot;median&quot;) + ggtitle(&quot;v:noise_level&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars_grouped&#39; by default.</code></pre>
<pre><code>## Warning: The following arguments were unrecognized and ignored: stat</code></pre>
<pre class="r"><code>grid.arrange(p1, p2, p0, nrow = 2, ncol = 2)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-21-4.png' %}" width="1440" /></p>
<pre class="r"><code>mcmc_intervals(as.matrix(optimal_ax.model.v), regex_pars = &quot;b_|^shape&quot;, prob = 0.90, prob_outer = 0.95) +
  ggtitle(&quot;Values Model&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-21-5.png' %}" width="1440" /></p>
<pre class="r"><code># Plot predictions vs original
preds_p_df = optimal_ax.model.null %&gt;% predict() %&gt;% as_tibble()
p1 =
  preds_p_df %&gt;%
    add_column(optimal_ax = optimal_ax.model.null$data$optimal_ax) %&gt;%
    ggplot(aes(x = Estimate, y = optimal_ax, color = optimal_ax)) +
      geom_point() +
      labs(x = &quot;Predicted values&quot;) +
      ylim(0, 1) +
      xlim(0, 1) +
      ggtitle(&quot;Null Model&quot;)

preds_p_df = plot_df %&gt;%
  add_predicted_draws(optimal_ax.model.null) %&gt;%
  group_by(optimal_ax, .prediction) %&gt;%
  count()
p2 =
  preds_p_df %&gt;%
    ggplot(aes(x = optimal_ax, y = .prediction, fill = n / sum(preds_p_df$n))) +
      geom_tile() +
      geom_text(aes(label = sprintf(&quot;%1.2f&quot;, n / sum(preds_p_df$n))), size = 10, vjust = 1) +
      scale_fill_gradient(limits = c(0, 1))

preds_p_df = optimal_ax.model.t %&gt;% predict() %&gt;% as_tibble()
p3 =
  preds_p_df %&gt;%
    add_column(optimal_ax = optimal_ax.model.t$data$optimal_ax) %&gt;%
    ggplot(aes(x = Estimate, y = optimal_ax, color = optimal_ax)) +
      geom_point() +
      labs(x = &quot;Predicted values&quot;) +
      ylim(0, 1) +
      xlim(0, 1) +
      ggtitle(&quot;Trends Model&quot;)

preds_p_df = plot_df %&gt;%
  add_predicted_draws(optimal_ax.model.t) %&gt;%
  group_by(optimal_ax, .prediction) %&gt;%
  count()
p4 =
  preds_p_df %&gt;%
    ggplot(aes(x = optimal_ax, y = .prediction, fill = n / sum(preds_p_df$n))) +
      geom_tile() +
      geom_text(aes(label = sprintf(&quot;%1.2f&quot;, n / sum(preds_p_df$n))), size = 10, vjust = 1) +
      scale_fill_gradient(limits = c(0, 1))

# Plot predictions vs original
preds_p_df = optimal_ax.model.v %&gt;% predict() %&gt;% as_tibble()
p5 =
  preds_p_df %&gt;%
    add_column(optimal_ax = optimal_ax.model.v$data$optimal_ax) %&gt;%
    ggplot(aes(x = Estimate, y = optimal_ax, color = optimal_ax)) +
      geom_point() +
      labs(x = &quot;Predicted values&quot;) +
      ylim(0, 1) +
      xlim(0, 1) +
      ggtitle(&quot;Values Model&quot;)

preds_p_df = plot_df %&gt;%
  add_predicted_draws(optimal_ax.model.v) %&gt;%
  group_by(optimal_ax, .prediction) %&gt;%
  count()
p6 =
  preds_p_df %&gt;%
    ggplot(aes(x = optimal_ax, y = .prediction, fill = n / sum(preds_p_df$n))) +
      geom_tile() +
      geom_text(aes(label = sprintf(&quot;%1.2f&quot;, n / sum(preds_p_df$n))), size = 10, vjust = 1) +
      scale_fill_gradient(limits = c(0, 1))

grid.arrange(p1, p3, p5, p2, p4, p6, nrow = 2, ncol = 3)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-21-6.png' %}" width="1440" /></p>
<pre class="r"><code>rm(p0, p1, p2, p3, p4, p5, p6)</code></pre>
<p>We are better able to recreate the data, if only marginally, with the model.</p>
</div>
<div id="inference-1" class="section level2">
<h2>Inference</h2>
<pre class="r"><code># Compute the bayes factor for each parameter value. Note: we compare
# here against the ROPE, and not the degenerate null hypothesis of 0
# Bayes-Factor values would be more significant in the latter case
# According to the Raftery, 1995 rules
# bf = 1 - 3: Weak
# bf = 3 - 20: Positive
# bf = 20 - 150: Strong
# bf &gt; 150: Very strong
print(bayesfactor_rope(optimal_ax.model.null, null = c(-0.055, 0.055)), n = 30)
print(bayesfactor_rope(optimal_ax.model.t, null = c(-0.055, 0.055)), n = 30)
print(bayesfactor_rope(optimal_ax.model.v, null = c(-0.055, 0.055)), n = 30)</code></pre>
<pre class="r"><code>options(digits = 3)
# Check the significance of each hypothesis
h_df = as_tibble(insight::get_parameters(optimal_ax.model.t)) %&gt;%
  transmute(
    Intercept = b_Intercept,
    has_ax_test = 2 * b_suggestion_type1,
    has_dx_test = (3 * b_suggestion_type2) + b_suggestion_type1,
    has_dxax_test = (4 * b_suggestion_type3) + b_suggestion_type1 + b_suggestion_type2,
    noise_levelL_test = b_noise_level.L,
    noise_levelQ_test = b_noise_level.Q
  )

hyp_results = test_hypotheses(h_df, rope_values = c(-0.055, 0.055))
print(hyp_results %&gt;% as.matrix())</code></pre>
<pre><code>##      Parameter           Estimate  CI_low   CI_high Effect_Size pd
## [1,] &quot;Intercept&quot;         &quot;-0.1225&quot; &quot;-0.848&quot; &quot;0.739&quot; &quot;0.0675&quot;    &quot;0.597&quot;
## [2,] &quot;has_ax_test&quot;       &quot; 1.0322&quot; &quot; 0.162&quot; &quot;1.948&quot; &quot;0.5691&quot;    &quot;0.970&quot;
## [3,] &quot;has_dx_test&quot;       &quot;-0.0247&quot; &quot;-0.845&quot; &quot;0.903&quot; &quot;0.0136&quot;    &quot;0.520&quot;
## [4,] &quot;has_dxax_test&quot;     &quot; 1.2042&quot; &quot; 0.385&quot; &quot;2.090&quot; &quot;0.6639&quot;    &quot;0.990&quot;
## [5,] &quot;noise_levelL_test&quot; &quot;-0.2068&quot; &quot;-0.628&quot; &quot;0.201&quot; &quot;0.1140&quot;    &quot;0.789&quot;
## [6,] &quot;noise_levelQ_test&quot; &quot; 0.2648&quot; &quot;-0.153&quot; &quot;0.651&quot; &quot;0.1460&quot;    &quot;0.856&quot;
##      ROPE_Percentage ROPE_Equivalence
## [1,] &quot;0.0848&quot;        &quot;Undecided&quot;
## [2,] &quot;0.0198&quot;        &quot;Rejected&quot;
## [3,] &quot;0.0897&quot;        &quot;Undecided&quot;
## [4,] &quot;0.0050&quot;        &quot;Rejected&quot;
## [5,] &quot;0.1207&quot;        &quot;Undecided&quot;
## [6,] &quot;0.1045&quot;        &quot;Undecided&quot;</code></pre>
<pre class="r"><code>h_df = as_tibble(insight::get_parameters(optimal_ax.model.v)) %&gt;%
  transmute(
    Intercept = b_Intercept,
    has_ax_test = 2 * b_suggestion_type1,
    has_dx_test = (3 * b_suggestion_type2) + b_suggestion_type1,
    has_dxax_test = (4 * b_suggestion_type3) + b_suggestion_type1 + b_suggestion_type2,
    noise_level1_test = b_noise_level_f1,
    noise_level2_test = b_noise_level_f2,
    noise_level3_test = - b_noise_level_f1 - b_noise_level_f2
  )

hyp_results = test_hypotheses(h_df, rope_values = c(-0.055, 0.055))
print(hyp_results %&gt;% as.matrix())</code></pre>
<pre><code>##      Parameter           Estimate  CI_low    CI_high Effect_Size pd
## [1,] &quot;Intercept&quot;         &quot;-0.1230&quot; &quot;-0.9488&quot; &quot;0.675&quot; &quot;0.0678&quot;    &quot;0.599&quot;
## [2,] &quot;has_ax_test&quot;       &quot; 1.0239&quot; &quot; 0.0525&quot; &quot;1.847&quot; &quot;0.5645&quot;    &quot;0.968&quot;
## [3,] &quot;has_dx_test&quot;       &quot;-0.0260&quot; &quot;-0.9612&quot; &quot;0.813&quot; &quot;0.0143&quot;    &quot;0.518&quot;
## [4,] &quot;has_dxax_test&quot;     &quot; 1.1909&quot; &quot; 0.2630&quot; &quot;2.020&quot; &quot;0.6566&quot;    &quot;0.985&quot;
## [5,] &quot;noise_level1_test&quot; &quot; 0.2621&quot; &quot;-0.0672&quot; &quot;0.604&quot; &quot;0.1445&quot;    &quot;0.892&quot;
## [6,] &quot;noise_level2_test&quot; &quot;-0.2205&quot; &quot;-0.5546&quot; &quot;0.110&quot; &quot;0.1216&quot;    &quot;0.861&quot;
## [7,] &quot;noise_level3_test&quot; &quot;-0.0347&quot; &quot;-0.3757&quot; &quot;0.276&quot; &quot;0.0191&quot;    &quot;0.573&quot;
##      ROPE_Percentage ROPE_Equivalence
## [1,] &quot;0.09400&quot;       &quot;Undecided&quot;
## [2,] &quot;0.01275&quot;       &quot;Rejected&quot;
## [3,] &quot;0.07975&quot;       &quot;Undecided&quot;
## [4,] &quot;0.00725&quot;       &quot;Rejected&quot;
## [5,] &quot;0.09775&quot;       &quot;Undecided&quot;
## [6,] &quot;0.12275&quot;       &quot;Undecided&quot;
## [7,] &quot;0.20150&quot;       &quot;Undecided&quot;</code></pre>
<pre class="r"><code>options(digits = 7)</code></pre>
<p><strong>Results</strong>:</p>
<ol style="list-style-type: decimal">
<li>AX suggestions
<ul>
<li>Positive effect pd = 97.0%</li>
<li>Median = 1.03, 89% CI [0.16, 1.95]</li>
<li>Medium, Effect Size = 0.57</li>
<li>1.98% in ROPE</li>
</ul></li>
<li>DXAX suggestions
<ul>
<li>Positive effect pd = 99.0%</li>
<li>Median = 1.20, 89% CI [0.39, 2.09]</li>
<li>Medium, Effect Size = 0.66</li>
<li>0.50% in ROPE</li>
</ul></li>
</ol>
<p>(The ROPE is defined as [-0.055, 0.055]. It is the range is suggested in the literature for logistic models and it corresponds to a probability range of 0.11. Therefore, a probability change of less than 0.055 is considered no different from a probability change of 0)</p>
</div>
<div id="posterior-plots-1" class="section level2">
<h2>Posterior Plots</h2>
<pre class="r"><code># A visualization data frame
gg_df = plot_df %&gt;%
    mutate(noise_level = fct_rev(noise_level)) %&gt;%
    mutate(optimal_ax = factor(optimal_ax)) %&gt;%
    mutate(user_id = factor(user_id)) %&gt;%
    mutate(optimal_ax = fct_recode(optimal_ax, incorrect=&quot;0&quot;, correct = &quot;1&quot;)) %&gt;%
    count(user_id, optimal_ax, num_actions, has_ax_only, has_dx_only, has_dxax, noise_level,
          name = &quot;num_optimal_ax&quot;)

# Simulate predictions on new data
p_df =
  plot_df %&gt;%
  data_grid(optimal_ax,
            suggestion_type,
            noise_level_f,
            gender,
            age_group,
            robot_experience,
            num_optimal,
            state_idx_rescaled = seq_range(state_idx_rescaled, n = 4),
            .model = optimal_ax.model.v)

fits_p_df = p_df %&gt;%
  add_fitted_draws(optimal_ax.model.v, re_formula = NA, n = 20, seed = default_seed) %&gt;%
  mutate(has_ax_only = (suggestion_type == &quot;AX&quot;),
         has_dx_only = (suggestion_type == &quot;DX&quot;),
         has_dxax = (suggestion_type == &quot;DXAX&quot;)) %&gt;%
  mutate(has_ax_only = factor(has_ax_only, levels = c(F, T)),
         has_dx_only = factor(has_dx_only, levels = c(F, T)),
         has_dxax = factor(has_dxax, levels = c(F, T)))

gc()

# Plot the posterior distributions. In each of the following, the reference ROPE band
# can be marked based on 3 methods. Make sure to update that ROPE value baased on the
# method that we want to display.

# If centering the ROPE on the Intercept (NULL or Fitted?)
# rope_value = inv_logit_scaled(fixef(optimal_ax.model.null)[&quot;Intercept&quot;, &quot;Estimate&quot;])
# rope_value = inv_logit_scaled(fixef(optimal_ax.model.t)[&quot;Intercept&quot;, &quot;Estimate&quot;])

# If centering the ROPE on the actual data
# rope_value = gg_df %&gt;%
#   filter(optimal_ax == &#39;correct&#39;, has_ax == F) %&gt;%
#   summarise(.value = median(num_optimal_ax / num_actions)) %&gt;%
#   pull(.value)

# If centering ROPE on the posterior distributions. Here we center the ROPE around the posterior
# predicted value for the BASELINE condition
preds_p_df = p_df %&gt;%
  add_predicted_draws(optimal_ax.model.v, re_formula = NA)  %&gt;% # 0, 1 predictions for each predicted value
  ungroup() %&gt;%
  count(suggestion_type, noise_level_f, .prediction) %&gt;%
  mutate(has_ax_only = (suggestion_type == &quot;AX&quot;),
         has_dx_only = (suggestion_type == &quot;DX&quot;),
         has_dxax = (suggestion_type == &quot;DXAX&quot;)) %&gt;%
  mutate(has_ax_only = factor(has_ax_only, levels = c(F, T)),
         has_dx_only = factor(has_dx_only, levels = c(F, T)),
         has_dxax = factor(has_dxax, levels = c(F, T)))
rope_value = median(
  preds_p_df %&gt;%
    filter(suggestion_type == &quot;NONE&quot;) %&gt;%
    spread(.prediction, n) %&gt;%
    mutate(.value = `1` / (`0` + `1`)) %&gt;%
    pull(.value)
)
gc()

gg_df %&gt;%
  filter(optimal_ax == &#39;correct&#39;) %&gt;%
  ggplot(aes(x = has_ax_only, y = num_optimal_ax / num_actions)) +
    geom_rect(ymin = rope_value-0.055, ymax = rope_value+0.055,
              xmin = -Inf, xmax = Inf,
              color = &quot;transparent&quot;, fill = &quot;white&quot;) +
    annotate(&quot;text&quot;, x = 0.5, y = rope_value, label = &quot;ROPE&quot;, angle = 90) +
    stat_halfeye(aes(y = .value), data = fits_p_df, position = position_nudge(x = 0.1)) +
    geom_point(aes(color = paste(has_ax_only, has_dx_only, has_dxax, noise_level, sep = &quot;:&quot;)),
               size = 2, position = position_jitter(width = .1, height = 0)) +
    stat_summary(fun = median, geom = &quot;point&quot;, shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(y = &quot;RAX&quot;, color = &quot;AX:DX:DXAX:Noise&quot;)

gg_df %&gt;%
  filter(optimal_ax == &#39;correct&#39;) %&gt;%
  ggplot(aes(x = has_dx_only, y = num_optimal_ax / num_actions)) +
    geom_rect(ymin = rope_value-0.055, ymax = rope_value+0.055,
              xmin = -Inf, xmax = Inf,
              color = &quot;transparent&quot;, fill = &quot;white&quot;) +
    annotate(&quot;text&quot;, x = 0.5, y = rope_value, label = &quot;ROPE&quot;, angle = 90) +
    stat_halfeye(aes(y = .value), data = fits_p_df, position = position_nudge(x = 0.1)) +
    geom_point(aes(color = paste(has_ax_only, has_dx_only, has_dxax, noise_level, sep = &quot;:&quot;)),
               size = 2, position = position_jitter(width = .1, height = 0)) +
    stat_summary(fun = median, geom = &quot;point&quot;, shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(y = &quot;RAX&quot;, color = &quot;AX:DX:DXAX:Noise&quot;)

gg_df %&gt;%
  filter(optimal_ax == &#39;correct&#39;) %&gt;%
  ggplot(aes(x = has_dxax, y = num_optimal_ax / num_actions)) +
    geom_rect(ymin = rope_value-0.055, ymax = rope_value+0.055,
              xmin = -Inf, xmax = Inf,
              color = &quot;transparent&quot;, fill = &quot;white&quot;) +
    annotate(&quot;text&quot;, x = 0.5, y = rope_value, label = &quot;ROPE&quot;, angle = 90) +
    stat_halfeye(aes(y = .value), data = fits_p_df, position = position_nudge(x = 0.1)) +
    geom_point(aes(color = paste(has_ax_only, has_dx_only, has_dxax, noise_level, sep = &quot;:&quot;)),
               size = 2, position = position_jitter(width = .1, height = 0)) +
    stat_summary(fun = median, geom = &quot;point&quot;, shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(y = &quot;RAX&quot;, color = &quot;AX:DX:DXAX:Noise&quot;)

gg_df %&gt;%
  filter(optimal_ax == &#39;correct&#39;) %&gt;%
  mutate(noise_level = fct_rev(noise_level)) %&gt;%
  ggplot(aes(x = noise_level, y = num_optimal_ax / num_actions)) +
    geom_rect(ymin = rope_value-0.055, ymax = rope_value+0.055,
              xmin = -Inf, xmax = Inf,
              color = &quot;transparent&quot;, fill = &quot;white&quot;) +
    annotate(&quot;text&quot;, x = 0.5, y = rope_value, label = &quot;ROPE&quot;, angle = 90) +
    stat_halfeye(aes(y = .value, x = noise_level_f), data = fits_p_df, position = position_nudge(x = 0.1)) +
    geom_point(aes(color = paste(has_ax_only, has_dx_only, has_dxax, noise_level, sep = &quot;:&quot;)),
               size = 2, position = position_jitter(width = .1, height = 0)) +
    stat_summary(fun = median, geom = &quot;point&quot;, shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(y = &quot;RAX&quot;, color = &quot;AX:DX:DXAX:Noise&quot;)

# Remove the giant fits data frames
rm(preds_p_df) # If using the posterior-predictions-based ROPE
rm(fits_p_df, p_df, gg_df)</code></pre>
<pre class="r"><code>eff = fixef(optimal_ax.model.t)
base_eff = inv_logit_scaled(
  eff[&quot;Intercept&quot;, &quot;Estimate&quot;]
  - eff[&quot;suggestion_type1&quot;, &quot;Estimate&quot;]
  - eff[&quot;suggestion_type2&quot;, &quot;Estimate&quot;]
  - eff[&quot;suggestion_type3&quot;, &quot;Estimate&quot;]
)
ax_eff = inv_logit_scaled(
  eff[&quot;Intercept&quot;, &quot;Estimate&quot;]
  + eff[&quot;suggestion_type1&quot;, &quot;Estimate&quot;]
  - eff[&quot;suggestion_type2&quot;, &quot;Estimate&quot;]
  - eff[&quot;suggestion_type3&quot;, &quot;Estimate&quot;]
)
dx_eff = inv_logit_scaled(
  eff[&quot;Intercept&quot;, &quot;Estimate&quot;]
  + (2 * eff[&quot;suggestion_type2&quot;, &quot;Estimate&quot;])
  - eff[&quot;suggestion_type3&quot;, &quot;Estimate&quot;]
)
dxax_eff = inv_logit_scaled(
  eff[&quot;Intercept&quot;, &quot;Estimate&quot;]
  + (3 * eff[&quot;suggestion_type3&quot;, &quot;Estimate&quot;])
)

gg_df = plot_df %&gt;%
  mutate(scenario_completed = factor(scenario_completed)) %&gt;%
  mutate(scenario_completed = fct_recode(scenario_completed, Unresolved=&quot;0&quot;, Resolved = &quot;1&quot;),
         has_ax_only = fct_recode(has_ax_only, &quot;No AX&quot;=&quot;FALSE&quot;, &quot;AX&quot;=&quot;TRUE&quot;),
         has_dx_only = fct_recode(has_dx_only, &quot;No DX&quot;=&quot;FALSE&quot;, &quot;DX&quot;=&quot;TRUE&quot;),
         has_dxax = fct_recode(has_dxax, &quot;No DXAX&quot;=&quot;FALSE&quot;, &quot;DXAX&quot;=&quot;TRUE&quot;),
         noise_level = fct_recode(noise_level, &quot;Acc: 100%&quot;=&quot;0.0&quot;, &quot;Acc: 90%&quot;=&quot;1.0&quot;, &quot;Acc: 80%&quot;=&quot;2.0&quot;)) %&gt;%
  count(user_id, suggestion_type, optimal_ax, num_actions, has_ax_only, has_dx_only, has_dxax, noise_level,
        name = &quot;num_optimal_ax&quot;) %&gt;%
  mutate(estimate = ifelse(suggestion_type == &quot;NONE&quot;, base_eff, -1)) %&gt;%
  mutate(estimate = ifelse(suggestion_type == &quot;AX&quot;, ax_eff, estimate)) %&gt;%
  mutate(estimate = ifelse(suggestion_type == &quot;DX&quot;, dx_eff, estimate)) %&gt;%
  mutate(estimate = ifelse(suggestion_type == &quot;DXAX&quot;, dxax_eff, estimate))

gg_df %&gt;%
  filter(optimal_ax == &quot;1&quot;) %&gt;%
  ggplot(aes(suggestion_type, num_optimal_ax / num_actions, group = suggestion_type, colour=suggestion_type)) +
    geom_count(aes(colour = NULL)) +
    geom_boxplot(aes(y = estimate)) +
    geom_signif(y_position = 1.05, xmin = 1, xmax = 2, annotation = &quot;**&quot;, textsize = 8, color = &quot;black&quot;) +
    geom_signif(y_position = 1.15, xmin = 1, xmax = 4, annotation = &quot;**&quot;, textsize = 8, color = &quot;black&quot;) +
    scale_y_continuous(limits = c(0.0, 1.2), breaks = c(0.0, 0.5, 1.0)) +
    labs(y = NULL, x = NULL) +
    scale_colour_manual(values = c(&quot;#e51400&quot;, &quot;#008a00&quot;, &quot;#f0a30a&quot;, &quot;#a4c400&quot;)) +
    theme_economist_white(base_size = 13) +
    theme(plot.background = element_rect(fill = &quot;white&quot;),
          axis.title.y = element_blank()) +
    legend_none()</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-25-1.png' %}" width="672" /></p>
</div>
</div>
<div id="rdx-rate-of-correct-diagnosis-selection" class="section level1">
<h1>RDX: Rate of Correct Diagnosis Selection</h1>
<p><strong>Did the user figure out the correct diagnoses for their situation?</strong></p>
<p>This is also a measure of Reliance on Suggestions. In the code, the variable might be referred to as <code>optimal_dx</code>, <code>correct_dx</code>, etc. depending on the version of the codebase</p>
<pre class="r"><code>plot_df = actions %&gt;%
  select(X1, id, user_id, study_condition, start_condition, num_optimal,
         state_idx, state_idx_rescaled,
         age_group, gender, robot_experience,
         noise_level, noise_level_f, has_noise, suggestion_type,
         has_dx, has_ax, has_ax_only, has_dx_only, has_dxax,
         scenario_completed, num_actions, optimal_dx)

print(summary(plot_df %&gt;%
                mutate(optimal_ax = factor(optimal_dx),
                       scenario_completed = factor(scenario_completed)) %&gt;%
                select(-X1, -id)
))</code></pre>
<pre><code>##     user_id      study_condition start_condition     num_optimal
##  Min.   :  1.0   DX_80   :256    Length:2126        Min.   :3.000
##  1st Qu.: 56.0   DX_90   :254    Class :character   1st Qu.:4.000
##  Median :104.0   DX_100  :247    Mode  :character   Median :4.000
##  Mean   :102.6   BASELINE:231                       Mean   :4.778
##  3rd Qu.:149.0   AX_90   :217                       3rd Qu.:7.000
##  Max.   :200.0   AX_80   :204                       Max.   :7.000
##                  (Other) :717
##    state_idx      state_idx_rescaled age_group gender   robot_experience
##  Min.   :  1.00   Min.   :0.0000     2:226     F: 844   0:1318
##  1st Qu.: 35.00   1st Qu.:0.2000     3:493     M:1282   1: 346
##  Median : 79.00   Median :0.4588     4:497              2: 161
##  Mean   : 86.64   Mean   :0.5037     5:283              3: 169
##  3rd Qu.:159.00   3rd Qu.:0.9294     6:222              4: 132
##  Max.   :171.00   Max.   :1.0000     7:170
##                                      8:235
##  noise_level noise_level_f has_noise    suggestion_type   has_dx
##  0.0:823     0.0:823       FALSE: 823   NONE:231        FALSE: 804
##  1.0:654     1.0:654       TRUE :1303   AX  :573        TRUE :1322
##  2.0:649     2.0:649                    DX  :757
##                                         DXAX:565
##
##
##
##    has_ax     has_ax_only  has_dx_only   has_dxax    scenario_completed
##  FALSE: 988   FALSE:1553   FALSE:1369   FALSE:1561   0: 760
##  TRUE :1138   TRUE : 573   TRUE : 757   TRUE : 565   1:1366
##
##
##
##
##
##   num_actions     optimal_dx     optimal_ax
##  Min.   : 3.0   Min.   :0.0000   0: 795
##  1st Qu.: 8.0   1st Qu.:0.0000   1:1331
##  Median :15.0   Median :1.0000
##  Mean   :13.9   Mean   :0.6261
##  3rd Qu.:20.0   3rd Qu.:1.0000
##  Max.   :20.0   Max.   :1.0000
## </code></pre>
<pre class="r"><code>print(as.matrix(
  plot_df %&gt;%
    filter(optimal_dx == 1) %&gt;%
    group_by(study_condition, num_actions, user_id) %&gt;%
    count() %&gt;%
    ungroup() %&gt;%
    mutate(.value = n/num_actions) %&gt;%
    group_by(study_condition) %&gt;%
    summarise(.value = median(.value))
))</code></pre>
<pre><code>##       study_condition .value
##  [1,] &quot;DX_100&quot;        &quot;0.8590909&quot;
##  [2,] &quot;AX_100&quot;        &quot;0.6904762&quot;
##  [3,] &quot;DXAX_100&quot;      &quot;0.7166667&quot;
##  [4,] &quot;DX_90&quot;         &quot;0.7250000&quot;
##  [5,] &quot;AX_90&quot;         &quot;0.7083333&quot;
##  [6,] &quot;DXAX_90&quot;       &quot;0.8000000&quot;
##  [7,] &quot;DX_80&quot;         &quot;0.5857143&quot;
##  [8,] &quot;AX_80&quot;         &quot;0.6274510&quot;
##  [9,] &quot;DXAX_80&quot;       &quot;0.7500000&quot;
## [10,] &quot;BASELINE&quot;      &quot;0.6428571&quot;</code></pre>
<div id="data-2" class="section level2">
<h2>Data</h2>
<pre class="r"><code># Plot by the study condition
gg_df =
  plot_df %&gt;%
    mutate(noise_level = fct_rev(noise_level)) %&gt;%
    mutate(optimal_dx = factor(optimal_dx)) %&gt;%
    mutate(user_id = factor(user_id)) %&gt;%
    mutate(optimal_dx = fct_recode(optimal_dx, incorrect=&quot;0&quot;, correct = &quot;1&quot;)) %&gt;%
    mutate(.fill_column = suggestion_type) %&gt;%
    count(user_id, study_condition, .fill_column, optimal_dx, num_actions, has_ax_only, has_dx_only, has_dxax, noise_level,
          name = &quot;num_optimal_dx&quot;)

gg_df %&gt;%
  ggplot(aes(user_id, num_optimal_dx / num_actions, fill=optimal_dx)) +
    geom_bar(stat=&quot;identity&quot;) +
    geom_hline(yintercept = 0.5) +
    facet_wrap(vars(study_condition), nrow = 1, scales = &quot;free_y&quot;) +
    labs(y = &quot;Fraction correct diagnoses&quot;) +
    coord_flip() +
    scale_fill_economist()</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-28-1.png' %}" width="1440" /></p>
<pre class="r"><code>gg_df %&gt;%
  filter(optimal_dx == &quot;correct&quot;) %&gt;%
  ggplot(aes(study_condition, num_optimal_dx / num_actions, fill=.fill_column)) +
    geom_violin() +
    geom_boxplot(width = 0.1) +
    labs(y = &quot;Fraction correct diagnoses&quot;, fill = &quot;Suggestion Type&quot;) +
    scale_fill_economist()</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-28-2.png' %}" width="1440" /></p>
<pre class="r"><code># Plot by suggestion type
gg_df =
  plot_df %&gt;%
    mutate(noise_level = fct_rev(noise_level)) %&gt;%
    mutate(optimal_dx = factor(optimal_dx)) %&gt;%
    mutate(user_id = factor(user_id)) %&gt;%
    mutate(optimal_dx = fct_recode(optimal_dx, incorrect=&quot;0&quot;, correct = &quot;1&quot;)) %&gt;%
    count(user_id, suggestion_type, noise_level, optimal_dx, num_actions,
          name = &quot;num_optimal_dx&quot;)

gg_df %&gt;%
  filter(optimal_dx == &quot;correct&quot;) %&gt;%
  ggplot(aes(suggestion_type, num_optimal_dx / num_actions, fill=suggestion_type)) +
    geom_boxplot() +
    geom_count() +
    facet_grid(cols = vars(noise_level), labeller = label_both) +
    labs(y = &quot;Fraction correct diagnoses&quot;) +
    scale_fill_economist() +
    theme(legend.position = &quot;bottom&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-28-3.png' %}" width="1440" /></p>
<pre class="r"><code>gg_df %&gt;%
  filter(optimal_dx == &quot;correct&quot;) %&gt;%
  ggplot(aes(noise_level, num_optimal_dx / num_actions, fill=suggestion_type)) +
    geom_boxplot() +
    geom_count() +
    facet_grid(cols = vars(suggestion_type)) +
    labs(y = &quot;Fraction correct diagnoses&quot;) +
    scale_fill_economist() +
    theme(legend.position = &quot;bottom&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-28-4.png' %}" width="1440" /></p>
<pre class="r"><code>p1 = gg_df %&gt;%
  filter(optimal_dx == &quot;correct&quot;) %&gt;%
  ggplot(aes(suggestion_type, num_optimal_dx / num_actions)) +
    geom_violin() +
    geom_point(aes(color = paste(suggestion_type, noise_level, sep = &quot;:&quot;)),
               size = 2, position = position_jitter(width = .2, height = 0)) +
    stat_summary(fun = median, geom = &quot;point&quot;, shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    legend_none()</code></pre>
<pre><code>## Warning: Ignoring unknown parameters: fun</code></pre>
<pre class="r"><code>p2 = gg_df %&gt;%
  filter(optimal_dx == &quot;correct&quot;) %&gt;%
  ggplot(aes(noise_level, num_optimal_dx / num_actions)) +
    geom_violin() +
    geom_point(aes(color = paste(suggestion_type, noise_level, sep = &quot;:&quot;)),
               size = 2, position = position_jitter(width = .1, height = 0)) +
    stat_summary(fun = median, geom = &quot;point&quot;, shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(color = &quot;Type:Noise&quot;) +
    theme(legend.position = &quot;right&quot;) +
    guides(size = F)</code></pre>
<pre><code>## Warning: Ignoring unknown parameters: fun</code></pre>
<pre class="r"><code>grid.arrange(p1, p2, ncol = 2)</code></pre>
<pre><code>## No summary function supplied, defaulting to `mean_se()
## No summary function supplied, defaulting to `mean_se()</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-28-5.png' %}" width="1440" /></p>
</div>
<div id="model-2" class="section level2">
<h2>Model</h2>
<p>Based on the data, we assume the following structural model:</p>
<p><span class="math display">\[RDX_{ij} = Bernoulli(p_{ij})\]</span> <span class="math display">\[\begin{aligned}
logit(p_{ij}) &amp;= \beta_0 + \beta_{0i} + \beta_{ax}\text{ax_only}_i + \beta_{dx}\text{dx_only}_i + \beta_{dxax}\text{dxax}_i +\\
&amp;\beta_{noise_L}\text{noise}_i + \beta_{noise_Q}\text{noise}_i +\\
&amp;\beta_{no}\text{no}_i + \mathbf{\beta_{demo}X_{demo,i}} + \beta_{state} \text{state}_{ij}
\end{aligned}\]</span> <span class="math display">\[\begin{aligned}
\beta_{.} &amp;\sim Normal(0, 10) \\
\beta_{0i} &amp;\sim Normal(0, \sigma_i) \\
\sigma_i &amp;\sim HalfStudent(3, 0, 10)
\end{aligned}\]</span></p>
<p>The prior for the <span class="math inline">\(\sigma_i\)</span> parameter is the default used in <code>brms</code>; I see no need to change it.</p>
<p>Model fitting:</p>
<pre class="r"><code># The null model
optimal_dx.model.null = brm(
  optimal_dx ~ 0 + Intercept + (1 | user_id),
  family = &quot;bernoulli&quot;,
  prior = c(set_prior(&quot;normal(0, 10)&quot;, class = &quot;b&quot;),
            set_prior(&quot;student_t(3, 0, 10)&quot;, class = &quot;sd&quot;)),
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
optimal_dx.model.null = add_criterion(optimal_dx.model.null, &quot;waic&quot;)
optimal_dx.model.null = add_criterion(optimal_dx.model.null, &quot;loo&quot;, reloo = T)
saveModel(optimal_dx.model.null)

# The trend model to see if there is a trend in the noise level variable
optimal_dx.model.t = brm(
  optimal_dx ~ 0 + Intercept + (suggestion_type + noise_level) + (age_group + robot_experience + gender) + num_optimal + state_idx_rescaled + (1 | user_id),
  family = bernoulli,
  prior = c(set_prior(&quot;normal(0, 10)&quot;, class = &quot;b&quot;),
            set_prior(&quot;student_t(3, 0, 10)&quot;, class = &quot;sd&quot;)),
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
optimal_dx.model.t = add_criterion(optimal_dx.model.t, &quot;waic&quot;)
optimal_dx.model.t = add_criterion(optimal_dx.model.t, &quot;loo&quot;, reloo = T)
saveModel(optimal_dx.model.t)

# The values model, to see if specific values of the noise level variable are significant
optimal_dx.model.v = brm(
  optimal_dx ~ 0 + Intercept + (suggestion_type + noise_level_f) + (age_group + robot_experience + gender) + num_optimal + state_idx_rescaled + (1 | user_id),
  family = bernoulli,
  prior = c(set_prior(&quot;normal(0, 10)&quot;, class = &quot;b&quot;),
            set_prior(&quot;student_t(3, 0, 10)&quot;, class = &quot;sd&quot;)),
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
optimal_dx.model.v = add_criterion(optimal_dx.model.v, &quot;waic&quot;)
optimal_dx.model.v = add_criterion(optimal_dx.model.v, &quot;loo&quot;, reloo = T)
saveModel(optimal_dx.model.v)</code></pre>
<pre class="r"><code># Load the models
optimal_dx.model.null = loadModel(&quot;optimal_dx.model.null&quot;)
optimal_dx.model.t = loadModel(&quot;optimal_dx.model.t&quot;)
optimal_dx.model.v = loadModel(&quot;optimal_dx.model.v&quot;)</code></pre>
<p>Model fitting results:</p>
<pre class="r"><code>print(performance::r2(optimal_dx.model.null))</code></pre>
<pre><code>## # Bayesian R2 with Standard Error
##
##   Conditional R2: 0.228 [0.014]
##      Marginal R2: 0.000 [0.000]</code></pre>
<pre class="r"><code># Print the parameters, and some initial diagnostics
print(model_parameters(optimal_dx.model.t,
                       centrality = &quot;median&quot;,
                       ci = 0.89,
                       ci_method = &quot;hdi&quot;,
                       test = c(&quot;pd&quot;, &quot;rope&quot;),
                       rope_range = c(-0.055, 0.055),
                       rope_ci = 1,
                       effects = &quot;fixed&quot;))</code></pre>
<pre><code>## # Fixed effects
##
## Parameter          | Median |         89% CI |     pd | % in ROPE |  ESS |  Rhat
## --------------------------------------------------------------------------------
## (Intercept)        |   0.14 | [-0.49,  0.77] | 64.15% |    10.22% | 1382 | 1.001
## suggestion_type1   |   0.26 | [-0.07,  0.59] | 89.28% |    10.12% | 1447 | 1.003
## suggestion_type2   |   0.19 | [ 0.04,  0.35] | 97.52% |     7.58% | 1293 | 1.003
## suggestion_type3   |   0.12 | [ 0.02,  0.22] | 97.30% |    15.22% | 1527 | 1.003
## noise_level.L      |  -0.48 | [-0.79, -0.16] | 99.28% |     1.25% | 1769 | 1.000
## noise_level.Q      |   0.03 | [-0.27,  0.35] | 54.73% |    21.80% | 1825 | 1.000
## age_group1         |   0.46 | [-0.02,  0.95] | 93.88% |     4.92% | 1609 | 1.001
## age_group2         |  -0.17 | [-0.53,  0.18] | 78.03% |    15.35% | 1913 | 1.000
## age_group3         |  -0.25 | [-0.59,  0.11] | 86.80% |    11.00% | 1478 | 1.000
## age_group4         |   0.15 | [-0.26,  0.56] | 71.80% |    14.75% | 1936 | 1.000
## age_group5         |  -0.32 | [-0.82,  0.19] | 84.65% |     9.32% | 1788 | 1.001
## age_group6         |   0.33 | [-0.26,  0.86] | 82.70% |     8.03% | 1692 | 1.001
## robot_experience1  |   0.54 | [ 0.23,  0.86] | 99.72% |     0.53% | 1428 | 1.002
## robot_experience2  |   0.27 | [-0.17,  0.68] | 84.10% |    10.30% | 1678 | 1.002
## robot_experience3  |  -0.15 | [-0.70,  0.48] | 65.60% |    10.85% | 1534 | 1.002
## robot_experience4  |  -0.30 | [-0.86,  0.30] | 80.17% |     8.75% | 1768 | 1.000
## gender1            |  -0.16 | [-0.34,  0.00] | 93.00% |    13.65% | 1739 | 1.001
## num_optimal        |   0.02 | [-0.09,  0.14] | 60.95% |    53.57% | 1505 | 1.000
## state_idx_rescaled |   0.46 | [ 0.19,  0.73] | 99.58% |     0.78% | 3732 | 1.000</code></pre>
<pre class="r"><code>print(performance::r2(optimal_dx.model.t))</code></pre>
<pre><code>## # Bayesian R2 with Standard Error
##
##   Conditional R2: 0.239 [0.013]
##      Marginal R2: 0.081 [0.020]</code></pre>
<pre class="r"><code>print(model_parameters(optimal_dx.model.v,
                       centrality = &quot;median&quot;,
                       ci = 0.89,
                       ci_method = &quot;hdi&quot;,
                       test = c(&quot;pd&quot;, &quot;rope&quot;),
                       rope_range = c(-0.055, 0.055),
                       rope_ci = 1,
                       effects = &quot;fixed&quot;))</code></pre>
<pre><code>## # Fixed effects
##
## Parameter          | Median |        89% CI |     pd | % in ROPE |  ESS |  Rhat
## -------------------------------------------------------------------------------
## (Intercept)        |   0.15 | [-0.44, 0.83] | 64.58% |     9.85% | 1176 | 1.000
## suggestion_type1   |   0.26 | [-0.09, 0.60] | 88.67% |     9.65% | 1393 | 1.006
## suggestion_type2   |   0.19 | [ 0.04, 0.35] | 97.35% |     7.30% | 1672 | 1.002
## suggestion_type3   |   0.12 | [ 0.02, 0.22] | 97.67% |    14.72% | 1796 | 1.003
## noise_level_f1     |   0.35 | [ 0.10, 0.62] | 98.45% |     2.65% | 1303 | 1.002
## noise_level_f2     |  -0.03 | [-0.27, 0.24] | 56.62% |    27.15% | 1399 | 1.001
## age_group1         |   0.46 | [-0.03, 0.93] | 93.92% |     4.92% | 1666 | 1.002
## age_group2         |  -0.16 | [-0.53, 0.22] | 76.68% |    14.40% | 1506 | 1.001
## age_group3         |  -0.25 | [-0.59, 0.13] | 86.58% |    10.55% | 1172 | 1.002
## age_group4         |   0.15 | [-0.28, 0.57] | 72.62% |    13.58% | 1720 | 1.000
## age_group5         |  -0.32 | [-0.82, 0.18] | 85.25% |     7.88% | 1336 | 1.003
## age_group6         |   0.33 | [-0.22, 0.91] | 82.03% |     7.55% | 1510 | 1.000
## robot_experience1  |   0.54 | [ 0.24, 0.87] | 99.80% |     0.43% | 1654 | 1.001
## robot_experience2  |   0.26 | [-0.17, 0.70] | 82.62% |    10.25% | 1400 | 1.001
## robot_experience3  |  -0.15 | [-0.75, 0.39] | 66.57% |    11.25% | 1632 | 1.002
## robot_experience4  |  -0.31 | [-0.88, 0.29] | 79.70% |     8.88% | 1481 | 1.001
## gender1            |  -0.17 | [-0.32, 0.03] | 93.10% |    13.68% | 1601 | 1.000
## num_optimal        |   0.02 | [-0.10, 0.12] | 59.90% |    53.60% | 1229 | 1.000
## state_idx_rescaled |   0.46 | [ 0.19, 0.73] | 99.70% |     0.83% | 3880 | 1.000</code></pre>
<pre class="r"><code>print(performance::r2(optimal_dx.model.v))</code></pre>
<pre><code>## # Bayesian R2 with Standard Error
##
##   Conditional R2: 0.239 [0.013]
##      Marginal R2: 0.082 [0.019]</code></pre>
</div>
<div id="diagnostic-2" class="section level2">
<h2>Diagnostic</h2>
<pre class="r"><code># Alternative diagnostics: diagnostic_posterior(scenario_completed.model.v)

# LOO cross-validation results
print(optimal_dx.model.t$criteria$loo$estimates)</code></pre>
<pre><code>##            Estimate        SE
## elpd_loo -1213.7723 21.386040
## p_loo      130.3065  3.703854
## looic     2427.5445 42.772080</code></pre>
<pre class="r"><code>print(optimal_dx.model.v$criteria$loo$estimates)</code></pre>
<pre><code>##            Estimate        SE
## elpd_loo -1214.2413 21.416780
## p_loo      131.0742  3.744262
## looic     2428.4826 42.833560</code></pre>
<pre class="r"><code>print(loo_compare(optimal_dx.model.null, optimal_dx.model.t, optimal_dx.model.v))</code></pre>
<pre><code>##                       elpd_diff se_diff
## optimal_dx.model.null  0.0       0.0
## optimal_dx.model.t    -1.4       4.0
## optimal_dx.model.v    -1.9       4.0</code></pre>
<pre class="r"><code># Diagnostics of the sampling process
par(mfrow=c(1,2))
plot(optimal_dx.model.t$criteria$loo, main = &quot;Trend Model&quot;)
plot(optimal_dx.model.v$criteria$loo, main = &quot;Values Model&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-32-1.png' %}" width="1440" /></p>
<pre class="r"><code># Include posterior predictive checks of the sampling (are sampling according
# to the data that we actually have?)
p0 = pp_check(optimal_dx.model.null, type = &quot;bars&quot;, stat = &quot;median&quot;) + ggtitle(&quot;null model&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars&#39; by default.</code></pre>
<pre><code>## Warning: The following arguments were unrecognized and ignored: stat</code></pre>
<pre class="r"><code>p1 = pp_check(optimal_dx.model.t, type = &quot;bars_grouped&quot;, group = &quot;suggestion_type&quot;, stat = &quot;median&quot;) + ggtitle(&quot;t:suggestion_type&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars_grouped&#39; by default.</code></pre>
<pre><code>## Warning: The following arguments were unrecognized and ignored: stat</code></pre>
<pre class="r"><code>p2 = pp_check(optimal_dx.model.t, type = &quot;bars_grouped&quot;, group = &quot;noise_level&quot;, stat = &quot;median&quot;) + ggtitle(&quot;t:noise_level&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars_grouped&#39; by default.</code></pre>
<pre><code>## Warning: The following arguments were unrecognized and ignored: stat</code></pre>
<pre class="r"><code>grid.arrange(p1, p2, p0, ncol = 2, nrow = 2)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-32-2.png' %}" width="1440" /></p>
<pre class="r"><code>mcmc_intervals(as.matrix(optimal_dx.model.t), regex_pars = &quot;b_|^shape&quot;, prob = 0.90, prob_outer = 0.95) +
  ggtitle(&quot;Trends Model&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-32-3.png' %}" width="1440" /></p>
<pre class="r"><code>p1 = pp_check(optimal_dx.model.v, type = &quot;bars_grouped&quot;, group = &quot;suggestion_type&quot;, stat = &quot;median&quot;) + ggtitle(&quot;v:suggestion_type&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars_grouped&#39; by default.</code></pre>
<pre><code>## Warning: The following arguments were unrecognized and ignored: stat</code></pre>
<pre class="r"><code>p2 = pp_check(optimal_dx.model.v, type = &quot;bars_grouped&quot;, group = &quot;noise_level_f&quot;, stat = &quot;median&quot;) + ggtitle(&quot;v:noise_level&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars_grouped&#39; by default.</code></pre>
<pre><code>## Warning: The following arguments were unrecognized and ignored: stat</code></pre>
<pre class="r"><code>grid.arrange(p1, p2, p0, ncol = 2, nrow = 2)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-32-4.png' %}" width="1440" /></p>
<pre class="r"><code>mcmc_intervals(as.matrix(optimal_dx.model.v), regex_pars = &quot;b_|^shape&quot;, prob = 0.90, prob_outer = 0.95) +
  ggtitle(&quot;Values Model&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-32-5.png' %}" width="1440" /></p>
<pre class="r"><code># Plot predictions vs original
preds_p_df = optimal_dx.model.null %&gt;% predict() %&gt;% as_tibble()
p1 =
  preds_p_df %&gt;%
    add_column(optimal_dx = optimal_dx.model.null$data$optimal_dx) %&gt;%
    ggplot(aes(x = Estimate, y = optimal_dx, color = optimal_dx)) +
      geom_point() +
      labs(x = &quot;Predicted values&quot;) +
      ylim(0, 1) +
      xlim(0, 1) +
      ggtitle(&quot;Null Model&quot;)

preds_p_df = plot_df %&gt;%
  add_predicted_draws(optimal_dx.model.null) %&gt;%
  group_by(optimal_dx, .prediction) %&gt;%
  count()
p2 =
  preds_p_df %&gt;%
    ggplot(aes(x = optimal_dx, y = .prediction, fill = n / sum(preds_p_df$n))) +
      geom_tile() +
      geom_text(aes(label = sprintf(&quot;%1.2f&quot;, n / sum(preds_p_df$n))), size = 10, vjust = 1) +
      scale_fill_gradient(limits = c(0, 1))

preds_p_df = optimal_dx.model.t %&gt;% predict() %&gt;% as_tibble()
p3 =
  preds_p_df %&gt;%
    add_column(optimal_dx = optimal_dx.model.t$data$optimal_dx) %&gt;%
    ggplot(aes(x = Estimate, y = optimal_dx, color = optimal_dx)) +
      geom_point() +
      labs(x = &quot;Predicted values&quot;) +
      ylim(0, 1) +
      xlim(0, 1) +
      ggtitle(&quot;Trends Model&quot;)

preds_p_df = plot_df %&gt;%
  add_predicted_draws(optimal_dx.model.t) %&gt;%
  group_by(optimal_dx, .prediction) %&gt;%
  count()
p4 =
  preds_p_df %&gt;%
    ggplot(aes(x = optimal_dx, y = .prediction, fill = n / sum(preds_p_df$n))) +
      geom_tile() +
      geom_text(aes(label = sprintf(&quot;%1.2f&quot;, n / sum(preds_p_df$n))), size = 10, vjust = 1) +
      scale_fill_gradient(limits = c(0, 1))

# Plot predictions vs original
preds_p_df = optimal_dx.model.v %&gt;% predict() %&gt;% as_tibble()
p5 =
  preds_p_df %&gt;%
    add_column(optimal_dx = optimal_dx.model.v$data$optimal_dx) %&gt;%
    ggplot(aes(x = Estimate, y = optimal_dx, color = optimal_dx)) +
      geom_point() +
      labs(x = &quot;Predicted values&quot;) +
      ylim(0, 1) +
      xlim(0, 1) +
      ggtitle(&quot;Values Model&quot;)

preds_p_df = plot_df %&gt;%
  add_predicted_draws(optimal_dx.model.v) %&gt;%
  group_by(optimal_dx, .prediction) %&gt;%
  count()
p6 =
  preds_p_df %&gt;%
    ggplot(aes(x = optimal_dx, y = .prediction, fill = n / sum(preds_p_df$n))) +
      geom_tile() +
      geom_text(aes(label = sprintf(&quot;%1.2f&quot;, n / sum(preds_p_df$n))), size = 10, vjust = 1) +
      scale_fill_gradient(limits = c(0, 1))

grid.arrange(p1, p3, p5, p2, p4, p6, nrow = 2, ncol = 3)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-32-6.png' %}" width="1440" /></p>
<pre class="r"><code>rm(p0, p1, p2, p3, p4, p5, p6)</code></pre>
<p>We are better able to recreate the data, if only marginally, with the model.</p>
</div>
<div id="inference-2" class="section level2">
<h2>Inference</h2>
<pre class="r"><code># Compute the bayes factor for each parameter value. Note: we compare
# here against the ROPE, and not the degenerate null hypothesis of 0
# Bayes-Factor values would be more significant in the latter case
# According to the Raftery, 1995 rules
# bf = 1 - 3: Weak
# bf = 3 - 20: Positive
# bf = 20 - 150: Strong
# bf &gt; 150: Very strong
print(bayesfactor_rope(optimal_dx.model.null, null = c(-0.055, 0.055)), n = 30)
print(bayesfactor_rope(optimal_dx.model.t, null = c(-0.055, 0.055)), n = 30)
print(bayesfactor_rope(optimal_dx.model.v, null = c(-0.055, 0.055)), n = 30)</code></pre>
<pre class="r"><code>options(digits = 3)
# Check the significance of each hypothesis
h_df = as_tibble(insight::get_parameters(optimal_dx.model.t)) %&gt;%
  transmute(
    Intercept = b_Intercept,
    has_ax_test = 2 * b_suggestion_type1,
    has_dx_test = (3 * b_suggestion_type2) + b_suggestion_type1,
    has_dxax_test = (4 * b_suggestion_type3) + b_suggestion_type1 + b_suggestion_type2,
    noise_levelL_test = b_noise_level.L,
    noise_levelQ_test = b_noise_level.Q
  )

hyp_results = test_hypotheses(h_df, rope_values = c(-0.055, 0.055))
print(hyp_results %&gt;% as.matrix())</code></pre>
<pre><code>##      Parameter           Estimate  CI_low   CI_high  Effect_Size pd
## [1,] &quot;Intercept&quot;         &quot; 0.1351&quot; &quot;-0.493&quot; &quot; 0.767&quot; &quot;0.0745&quot;    &quot;0.641&quot;
## [2,] &quot;has_ax_test&quot;       &quot; 0.5141&quot; &quot;-0.141&quot; &quot; 1.184&quot; &quot;0.2834&quot;    &quot;0.893&quot;
## [3,] &quot;has_dx_test&quot;       &quot; 0.8164&quot; &quot; 0.145&quot; &quot; 1.512&quot; &quot;0.4501&quot;    &quot;0.978&quot;
## [4,] &quot;has_dxax_test&quot;     &quot; 0.9186&quot; &quot; 0.223&quot; &quot; 1.601&quot; &quot;0.5064&quot;    &quot;0.987&quot;
## [5,] &quot;noise_levelL_test&quot; &quot;-0.4774&quot; &quot;-0.791&quot; &quot;-0.163&quot; &quot;0.2632&quot;    &quot;0.993&quot;
## [6,] &quot;noise_levelQ_test&quot; &quot; 0.0261&quot; &quot;-0.268&quot; &quot; 0.346&quot; &quot;0.0144&quot;    &quot;0.547&quot;
##      ROPE_Percentage ROPE_Equivalence
## [1,] &quot;0.10225&quot;       &quot;Undecided&quot;
## [2,] &quot;0.05200&quot;       &quot;Undecided&quot;
## [3,] &quot;0.01600&quot;       &quot;Rejected&quot;
## [4,] &quot;0.00975&quot;       &quot;Rejected&quot;
## [5,] &quot;0.01250&quot;       &quot;Rejected&quot;
## [6,] &quot;0.21800&quot;       &quot;Undecided&quot;</code></pre>
<pre class="r"><code>h_df = as_tibble(insight::get_parameters(optimal_dx.model.v)) %&gt;%
  transmute(
    Intercept = b_Intercept,
    has_ax_test = 2 * b_suggestion_type1,
    has_dx_test = (3 * b_suggestion_type2) + b_suggestion_type1,
    has_dxax_test = (4 * b_suggestion_type3) + b_suggestion_type1 + b_suggestion_type2,
    noise_level1_test = b_noise_level_f1,
    noise_level2_test = b_noise_level_f2,
    noise_level3_test = - b_noise_level_f1 - b_noise_level_f2
  )

hyp_results = test_hypotheses(h_df, rope_values = c(-0.055, 0.055))
print(hyp_results %&gt;% as.matrix())</code></pre>
<pre><code>##      Parameter           Estimate  CI_low    CI_high   Effect_Size pd
## [1,] &quot;Intercept&quot;         &quot; 0.1480&quot; &quot;-0.4355&quot; &quot; 0.8288&quot; &quot;0.0816&quot;    &quot;0.646&quot;
## [2,] &quot;has_ax_test&quot;       &quot; 0.5269&quot; &quot;-0.1840&quot; &quot; 1.2062&quot; &quot;0.2905&quot;    &quot;0.887&quot;
## [3,] &quot;has_dx_test&quot;       &quot; 0.8422&quot; &quot; 0.1453&quot; &quot; 1.5345&quot; &quot;0.4643&quot;    &quot;0.972&quot;
## [4,] &quot;has_dxax_test&quot;     &quot; 0.9406&quot; &quot; 0.2718&quot; &quot; 1.6428&quot; &quot;0.5186&quot;    &quot;0.986&quot;
## [5,] &quot;noise_level1_test&quot; &quot; 0.3537&quot; &quot; 0.0967&quot; &quot; 0.6205&quot; &quot;0.1950&quot;    &quot;0.985&quot;
## [6,] &quot;noise_level2_test&quot; &quot;-0.0265&quot; &quot;-0.2673&quot; &quot; 0.2402&quot; &quot;0.0146&quot;    &quot;0.566&quot;
## [7,] &quot;noise_level3_test&quot; &quot;-0.3263&quot; &quot;-0.5772&quot; &quot;-0.0574&quot; &quot;0.1799&quot;    &quot;0.980&quot;
##      ROPE_Percentage ROPE_Equivalence
## [1,] &quot;0.09850&quot;       &quot;Undecided&quot;
## [2,] &quot;0.04875&quot;       &quot;Undecided&quot;
## [3,] &quot;0.01625&quot;       &quot;Rejected&quot;
## [4,] &quot;0.00825&quot;       &quot;Rejected&quot;
## [5,] &quot;0.02650&quot;       &quot;Undecided&quot;
## [6,] &quot;0.27150&quot;       &quot;Undecided&quot;
## [7,] &quot;0.03700&quot;       &quot;Undecided&quot;</code></pre>
<pre class="r"><code>options(digits = 7)</code></pre>
<p><strong>Results</strong>:</p>
<ol style="list-style-type: decimal">
<li>DX suggestions
<ul>
<li>Positive effect pd = 97.8%</li>
<li>Median = 0.82, 89% CI [0.15, 1.51]</li>
<li>Small, Effect Size = 0.45</li>
<li>1.60% in ROPE</li>
</ul></li>
<li>DXAX suggestions
<ul>
<li>Positive effect pd = 98.7%</li>
<li>Median = 0.92, 89% CI [0.22, 1.60]</li>
<li>Medium, Effect Size = 0.51</li>
<li>0.98% in ROPE</li>
</ul></li>
<li>Noise Level Linear
<ul>
<li>Negative (negative slope) pd = 99.3%</li>
<li>Median = -0.48, 89% CI [-0.79, -0.16]</li>
<li>Small, Effect Size = 0.26</li>
<li>1.25% in ROPE</li>
</ul></li>
</ol>
<p>(The ROPE is defined as [-0.055, 0.055]. It is the range is suggested in the literature for logistic models and it corresponds to a probability range of 0.11. Therefore, a probability change of less than 0.055 is considered no different from a probability change of 0)</p>
</div>
<div id="posterior-plots-2" class="section level2">
<h2>Posterior Plots</h2>
<pre class="r"><code># A visualization data frame
gg_df = plot_df %&gt;%
    mutate(noise_level = fct_rev(noise_level)) %&gt;%
    mutate(optimal_dx = factor(optimal_dx)) %&gt;%
    mutate(user_id = factor(user_id)) %&gt;%
    mutate(optimal_dx = fct_recode(optimal_dx, incorrect=&quot;0&quot;, correct = &quot;1&quot;)) %&gt;%
    count(user_id, optimal_dx, num_actions, has_ax_only, has_dx_only, has_dxax, noise_level,
          name = &quot;num_optimal_dx&quot;)

# Simulate predictions on new data
p_df =
  plot_df %&gt;%
  data_grid(optimal_dx,
            suggestion_type,
            noise_level_f,
            gender,
            age_group,
            robot_experience,
            num_optimal,
            state_idx_rescaled = seq_range(state_idx_rescaled, n = 4),
            .model = optimal_dx.model.v)

fits_p_df = p_df %&gt;%
  add_fitted_draws(optimal_dx.model.v, re_formula = NA, n = 20, seed = default_seed) %&gt;%
  mutate(has_ax_only = (suggestion_type == &quot;AX&quot;),
         has_dx_only = (suggestion_type == &quot;DX&quot;),
         has_dxax = (suggestion_type == &quot;DXAX&quot;)) %&gt;%
  mutate(has_ax_only = factor(has_ax_only, levels = c(F, T)),
         has_dx_only = factor(has_dx_only, levels = c(F, T)),
         has_dxax = factor(has_dxax, levels = c(F, T)))
gc()

pars_p_df = optimal_dx.model.t %&gt;%
  extract_draws(newdata = p_df %&gt;% rename(noise_level = noise_level_f), re_formula = NA, nsamples = 100)
gc()

# Plot the posterior distributions. In each of the following, the reference ROPE band
# can be marked based on 3 methods. Make sure to update that ROPE value baased on the
# method that we want to display.

# If centering ROPE on the posterior distributions. Here we center the ROPE around the posterior
# predicted value for the BASELINE condition
preds_p_df = p_df %&gt;%
  add_predicted_draws(optimal_dx.model.v, re_formula = NA)  %&gt;% # 0, 1 predictions for each predicted value
  ungroup() %&gt;%
  count(suggestion_type, noise_level_f, .prediction) %&gt;%
  mutate(has_ax_only = (suggestion_type == &quot;AX&quot;),
         has_dx_only = (suggestion_type == &quot;DX&quot;),
         has_dxax = (suggestion_type == &quot;DXAX&quot;)) %&gt;%
  mutate(has_ax_only = factor(has_ax_only, levels = c(F, T)),
         has_dx_only = factor(has_dx_only, levels = c(F, T)),
         has_dxax = factor(has_dxax, levels = c(F, T)))
rope_value = median(
  preds_p_df %&gt;%
    filter(suggestion_type == &quot;NONE&quot;) %&gt;%
    spread(.prediction, n) %&gt;%
    mutate(.value = `1` / (`0` + `1`)) %&gt;%
    pull(.value)
)
gc()

# Plot the posterior distributions
gg_df %&gt;%
  filter(optimal_dx == &#39;correct&#39;) %&gt;%
  ggplot(aes(x = has_ax_only, y = num_optimal_dx / num_actions)) +
    geom_rect(ymin = rope_value-0.055, ymax = rope_value+0.055,
              xmin = -Inf, xmax = Inf,
              color = &quot;transparent&quot;, fill = &quot;white&quot;) +
    annotate(&quot;text&quot;, x = 0.5, y = rope_value, label = &quot;ROPE&quot;, angle = 90) +
    stat_halfeye(aes(y = .value), data = fits_p_df, position = position_nudge(x = 0.1)) +
    geom_point(aes(color = paste(has_ax_only, has_dx_only, has_dxax, noise_level, sep = &quot;:&quot;)),
               size = 2, position = position_jitter(width = .1, height = 0)) +
    stat_summary(fun = median, geom = &quot;point&quot;, shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(y = &quot;RDX&quot;, color = &quot;AX:DX:DXAX:Noise&quot;)

gg_df %&gt;%
  filter(optimal_dx == &#39;correct&#39;) %&gt;%
  ggplot(aes(x = has_dx_only, y = num_optimal_dx / num_actions)) +
    geom_rect(ymin = rope_value-0.055, ymax = rope_value+0.055,
              xmin = -Inf, xmax = Inf,
              color = &quot;transparent&quot;, fill = &quot;white&quot;) +
    annotate(&quot;text&quot;, x = 0.5, y = rope_value, label = &quot;ROPE&quot;, angle = 90) +
    stat_halfeye(aes(y = .value), data = fits_p_df, position = position_nudge(x = 0.1)) +
    geom_point(aes(color = paste(has_ax_only, has_dx_only, has_dxax, noise_level, sep = &quot;:&quot;)),
               size = 2, position = position_jitter(width = .1, height = 0)) +
    stat_summary(fun = median, geom = &quot;point&quot;, shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(y = &quot;RDX&quot;, color = &quot;AX:DX:DXAX:Noise&quot;)

gg_df %&gt;%
  filter(optimal_dx == &#39;correct&#39;) %&gt;%
  ggplot(aes(x = has_dxax, y = num_optimal_dx / num_actions)) +
    geom_rect(ymin = rope_value-0.055, ymax = rope_value+0.055,
              xmin = -Inf, xmax = Inf,
              color = &quot;transparent&quot;, fill = &quot;white&quot;) +
    annotate(&quot;text&quot;, x = 0.5, y = rope_value, label = &quot;ROPE&quot;, angle = 90) +
    stat_halfeye(aes(y = .value), data = fits_p_df, position = position_nudge(x = 0.1)) +
    geom_point(aes(color = paste(has_ax_only, has_dx_only, has_dxax, noise_level, sep = &quot;:&quot;)),
               size = 2, position = position_jitter(width = .1, height = 0)) +
    stat_summary(fun = median, geom = &quot;point&quot;, shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(y = &quot;RDX&quot;, color = &quot;AX:DX:DXAX:Noise&quot;)

gg_df %&gt;%
  filter(optimal_dx == &#39;correct&#39;) %&gt;%
  mutate(noise_level = fct_rev(noise_level)) %&gt;%
  ggplot(aes(x = noise_level, y = num_optimal_dx / num_actions)) +
    geom_rect(ymin = rope_value-0.055, ymax = rope_value+0.055,
              xmin = -Inf, xmax = Inf,
              color = &quot;transparent&quot;, fill = &quot;white&quot;) +
    annotate(&quot;text&quot;, x = 0.5, y = rope_value, label = &quot;ROPE&quot;, angle = 90) +
    stat_halfeye(aes(y = .value, x = noise_level_f), data = fits_p_df, position = position_nudge(x = 0.1)) +
    geom_point(aes(color = paste(has_ax_only, has_dx_only, has_dxax, noise_level, sep = &quot;:&quot;)),
               size = 2, position = position_jitter(width = .1, height = 0)) +
    stat_summary(fun = median, geom = &quot;point&quot;, shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(y = &quot;RDX&quot;, color = &quot;AX:DX:DXAX:Noise&quot;)

# Create a plot to show the trend
pars_p_df_X = pars_p_df$dpars$mu$fe$X[,c(&quot;Intercept&quot;, &quot;noise_level.L&quot;)]
pars_p_df_b = pars_p_df$dpars$mu$fe$b[,c(&quot;b_Intercept&quot;, &quot;b_noise_level.L&quot;)]
pars_p_df_y = pars_p_df_X %*% t(pars_p_df_b)
pars_p_df_y = inv_logit_scaled(pars_p_df_y)
p_df =p_df %&gt;%
  bind_cols(as_tibble(pars_p_df_y)) %&gt;%
  gather(key = &quot;.sample&quot;, value = &quot;.value&quot;, V1:V100) %&gt;%
  rename(noise_level = noise_level_f)
gg_df %&gt;%
  filter(optimal_dx == &#39;correct&#39;) %&gt;%
  mutate(noise_level = fct_rev(noise_level)) %&gt;%
  ggplot(aes(x = noise_level, y = num_optimal_dx / num_actions)) +
    geom_rect(ymin = rope_value-0.055, ymax = rope_value+0.055,
              xmin = -Inf, xmax = Inf,
              color = &quot;transparent&quot;, fill = &quot;white&quot;) +
    annotate(&quot;text&quot;, x = 0.5, y = rope_value, label = &quot;ROPE&quot;, angle = 90) +
    stat_halfeye(aes(y = .value, x = noise_level_f), data = fits_p_df, position = position_nudge(x = 0.1)) +
    geom_line(aes(y = .value, group = .sample, alpha = 0.3), data = p_df) +
    geom_point(aes(color = paste(has_ax_only, has_dx_only, has_dxax, noise_level, sep = &quot;:&quot;)),
               size = 2, position = position_jitter(width = .1, height = 0)) +
    stat_summary(fun = median, geom = &quot;point&quot;, shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(y = &quot;RDX&quot;, color = &quot;AX:DX:DXAX:Noise&quot;) +
    guides(alpha = F)

# Remove the giant fits data frames
rm(preds_p_df) # If using the posterior-predictions-based ROPE
rm(fits_p_df, pars_p_df, pars_p_df_b, pars_p_df_X, pars_p_df_y, p_df, gg_df)</code></pre>
<pre class="r"><code>eff = fixef(optimal_dx.model.t)
base_eff = inv_logit_scaled(
  eff[&quot;Intercept&quot;, &quot;Estimate&quot;]
  - eff[&quot;suggestion_type1&quot;, &quot;Estimate&quot;]
  - eff[&quot;suggestion_type2&quot;, &quot;Estimate&quot;]
  - eff[&quot;suggestion_type3&quot;, &quot;Estimate&quot;]
)
ax_eff = inv_logit_scaled(
  eff[&quot;Intercept&quot;, &quot;Estimate&quot;]
  + eff[&quot;suggestion_type1&quot;, &quot;Estimate&quot;]
  - eff[&quot;suggestion_type2&quot;, &quot;Estimate&quot;]
  - eff[&quot;suggestion_type3&quot;, &quot;Estimate&quot;]
)
dx_eff = inv_logit_scaled(
  eff[&quot;Intercept&quot;, &quot;Estimate&quot;]
  + (2 * eff[&quot;suggestion_type2&quot;, &quot;Estimate&quot;])
  - eff[&quot;suggestion_type3&quot;, &quot;Estimate&quot;]
)
dxax_eff = inv_logit_scaled(
  eff[&quot;Intercept&quot;, &quot;Estimate&quot;]
  + (3 * eff[&quot;suggestion_type3&quot;, &quot;Estimate&quot;])
)

gg_df = plot_df %&gt;%
  mutate(scenario_completed = factor(scenario_completed)) %&gt;%
  mutate(scenario_completed = fct_recode(scenario_completed, Unresolved=&quot;0&quot;, Resolved = &quot;1&quot;),
         has_ax_only = fct_recode(has_ax_only, &quot;No AX&quot;=&quot;FALSE&quot;, &quot;AX&quot;=&quot;TRUE&quot;),
         has_dx_only = fct_recode(has_dx_only, &quot;No DX&quot;=&quot;FALSE&quot;, &quot;DX&quot;=&quot;TRUE&quot;),
         has_dxax = fct_recode(has_dxax, &quot;No DXAX&quot;=&quot;FALSE&quot;, &quot;DXAX&quot;=&quot;TRUE&quot;),
         noise_level = fct_recode(noise_level, &quot;Acc: 100%&quot;=&quot;0.0&quot;, &quot;Acc: 90%&quot;=&quot;1.0&quot;, &quot;Acc: 80%&quot;=&quot;2.0&quot;)) %&gt;%
  count(user_id, suggestion_type, optimal_dx, num_actions, has_ax_only, has_dx_only, has_dxax, noise_level,
        name = &quot;num_optimal_dx&quot;) %&gt;%
  mutate(estimate = ifelse(suggestion_type == &quot;NONE&quot;, base_eff, -1)) %&gt;%
  mutate(estimate = ifelse(suggestion_type == &quot;AX&quot;, ax_eff, estimate)) %&gt;%
  mutate(estimate = ifelse(suggestion_type == &quot;DX&quot;, dx_eff, estimate)) %&gt;%
  mutate(estimate = ifelse(suggestion_type == &quot;DXAX&quot;, dxax_eff, estimate))

p1 = gg_df %&gt;%
  filter(optimal_dx == &quot;1&quot;) %&gt;%
  ggplot(aes(suggestion_type, num_optimal_dx / num_actions, group = suggestion_type, colour=suggestion_type)) +
    geom_count(aes(colour = NULL)) +
    geom_boxplot(aes(y = estimate)) +
    geom_signif(y_position = 1.05, xmin = 1, xmax = 3, annotation = &quot;*&quot;, textsize = 8, color = &quot;black&quot;) +
    geom_signif(y_position = 1.15, xmin = 1, xmax = 4, annotation = &quot;**&quot;, textsize = 8, color = &quot;black&quot;) +
    scale_y_continuous(limits = c(0.0, 1.2), breaks = c(0.0, 0.5, 1.0)) +
    labs(y = NULL, x = NULL) +
    scale_colour_manual(values = c(&quot;#e51400&quot;, &quot;#008a00&quot;, &quot;#f0a30a&quot;, &quot;#a4c400&quot;)) +
    theme_economist_white(base_size = 13) +
    theme(plot.background = element_rect(fill = &quot;white&quot;),
          axis.title.y = element_blank()) +
    legend_none()

eff = fixef(optimal_dx.model.v)
noise_0_eff = inv_logit_scaled(eff[&quot;Intercept&quot;, &quot;Estimate&quot;] + eff[&quot;noise_level_f1&quot;,&quot;Estimate&quot;])
noise_1_eff = inv_logit_scaled(eff[&quot;Intercept&quot;, &quot;Estimate&quot;] + eff[&quot;noise_level_f2&quot;,&quot;Estimate&quot;])
noise_2_eff = inv_logit_scaled(eff[&quot;Intercept&quot;, &quot;Estimate&quot;] - eff[&quot;noise_level_f1&quot;,&quot;Estimate&quot;] - eff[&quot;noise_level_f2&quot;,&quot;Estimate&quot;])

gg_df = gg_df %&gt;%
  mutate(estimate = if_else(noise_level == &quot;Acc: 100%&quot;, noise_0_eff, estimate)) %&gt;%
  mutate(estimate = if_else(noise_level == &quot;Acc: 90%&quot;, noise_1_eff, estimate)) %&gt;%
  mutate(estimate = if_else(noise_level == &quot;Acc: 80%&quot;, noise_2_eff, estimate)) %&gt;%
  mutate(noise_level = fct_rev(noise_level))

p2 = gg_df %&gt;%
  filter(optimal_dx == &quot;1&quot;) %&gt;%
  ggplot(aes(noise_level, num_optimal_dx / num_actions, group = noise_level, colour = noise_level)) +
    geom_hline(yintercept = inv_logit_scaled(eff[&quot;Intercept&quot;, &quot;Estimate&quot;]),
               size = .7, linetype = &quot;dashed&quot;, color = &quot;grey&quot;) +
    geom_count(aes(colour = NULL)) +
    geom_boxplot(aes(y = estimate)) +
    annotate(&quot;segment&quot;,
             x = 3, y = noise_0_eff,
             xend = 2, yend = noise_1_eff,
             colour = &quot;black&quot;) +
    annotate(&quot;segment&quot;,
             x = 1, y = noise_2_eff,
             xend = 2, yend = noise_1_eff,
             color = &quot;black&quot;) +
    annotate(&quot;text&quot;,
             x = 3.5,
             y = inv_logit_scaled(eff[&quot;Intercept&quot;, &quot;Estimate&quot;] - 0.35),
             label = &quot;Mean of all levels&quot;,
             angle = 90,
             group = NA,
             colour = &quot;black&quot;) +
    scale_y_continuous(limits = c(0.0, 1.0), breaks = c(0.0, 0.5, 1.0)) +
    labs(y = &quot;RDX&quot;, x = NULL) +
    scale_colour_manual(values = c(&quot;#e51400&quot;, &quot;#008a00&quot;, &quot;#f0a30a&quot;, &quot;#a4c400&quot;)) +
    scale_size_area(max_size = 1) +
    theme_economist_white(base_size = 13) +
    theme(plot.background = element_rect(fill = &quot;white&quot;),
          axis.title.y = element_blank(),
          axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
    legend_none()

grid.arrange(p1, p2, nrow = 1, ncol = 2)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-36-1.png' %}" width="672" /></p>
<pre class="r"><code>rm(p1, p2)</code></pre>
</div>
</div>
<div id="cax-compliance-with-ax-suggestions" class="section level1">
<h1>CAX: Compliance with AX Suggestions</h1>
<p><strong>Did the user follow the AX suggestions that were provided to them?</strong></p>
<p>In the code, the variable might be referred to as <code>chose_ax</code>, <code>follow_ax</code>, etc. depending on the version of the codebase</p>
<pre class="r"><code>plot_df = actions %&gt;%
  filter(has_ax == T) %&gt;%
  select(X1, id, user_id, study_condition, start_condition, num_optimal,
         state_idx, state_idx_rescaled,
         age_group, gender, robot_experience,
         noise_level, noise_level_f, has_noise, suggestion_type,
         has_dx, has_ax, has_ax_only, has_dx_only, has_dxax,
         scenario_completed, num_actions, chose_ax)

# With these dropped levels, the suggestion_type variable is the same as the
# boolean flags for each suggestion type
plot_df$suggestion_type = droplevels(plot_df$suggestion_type)
plot_df$study_condition = droplevels(plot_df$study_condition)

print(summary(plot_df %&gt;%
                mutate(chose_ax = factor(chose_ax),
                       scenario_completed = factor(scenario_completed)) %&gt;%
                select(-X1, -id)
))</code></pre>
<pre><code>##     user_id      study_condition start_condition     num_optimal
##  Min.   :  1.0   AX_100  :152    Length:1138        Min.   :3.000
##  1st Qu.: 62.0   DXAX_100:193    Class :character   1st Qu.:4.000
##  Median :102.0   AX_90   :217    Mode  :character   Median :4.000
##  Mean   :104.3   DXAX_90 :183                       Mean   :4.804
##  3rd Qu.:150.0   AX_80   :204                       3rd Qu.:7.000
##  Max.   :198.0   DXAX_80 :189                       Max.   :7.000
##
##    state_idx      state_idx_rescaled age_group gender  robot_experience
##  Min.   :  1.00   Min.   :0.0000     2:160     F:453   0:665
##  1st Qu.: 35.00   1st Qu.:0.2000     3:323     M:685   1:260
##  Median : 60.00   Median :0.3471     4:224             2: 55
##  Mean   : 82.94   Mean   :0.4820     5:140             3: 89
##  3rd Qu.:159.00   3rd Qu.:0.9294     6:127             4: 69
##  Max.   :171.00   Max.   :1.0000     7: 49
##                                      8:115
##  noise_level noise_level_f has_noise   suggestion_type   has_dx      has_ax
##  0.0:345     0.0:345       FALSE:345   AX  :573        FALSE:573   FALSE:   0
##  1.0:400     1.0:400       TRUE :793   DXAX:565        TRUE :565   TRUE :1138
##  2.0:393     2.0:393
##
##
##
##
##  has_ax_only has_dx_only   has_dxax   scenario_completed  num_actions
##  FALSE:565   FALSE:1138   FALSE:573   0:240              Min.   : 3.00
##  TRUE :573   TRUE :   0   TRUE :565   1:898              1st Qu.: 8.00
##                                                          Median :13.00
##                                                          Mean   :12.51
##                                                          3rd Qu.:19.00
##                                                          Max.   :20.00
##
##  chose_ax
##  0:526
##  1:612
##
##
##
##
## </code></pre>
<pre class="r"><code>print(as.matrix(
  plot_df %&gt;%
    filter(chose_ax == 1) %&gt;%
    group_by(study_condition, num_actions, user_id) %&gt;%
    count() %&gt;%
    ungroup() %&gt;%
    mutate(.value = n/num_actions) %&gt;%
    group_by(study_condition) %&gt;%
    summarise(.value = median(.value))
))</code></pre>
<pre><code>##      study_condition .value
## [1,] &quot;AX_100&quot;        &quot;0.7638889&quot;
## [2,] &quot;DXAX_100&quot;      &quot;0.6833333&quot;
## [3,] &quot;AX_90&quot;         &quot;0.5500000&quot;
## [4,] &quot;DXAX_90&quot;       &quot;0.6201923&quot;
## [5,] &quot;AX_80&quot;         &quot;0.6201923&quot;
## [6,] &quot;DXAX_80&quot;       &quot;0.6333333&quot;</code></pre>
<div id="data-3" class="section level2">
<h2>Data</h2>
<pre class="r"><code># Plot by the study condition
gg_df =
  plot_df %&gt;%
    mutate(noise_level = fct_rev(noise_level)) %&gt;%
    mutate(chose_ax = factor(chose_ax)) %&gt;%
    mutate(user_id = factor(user_id)) %&gt;%
    mutate(chose_ax = fct_recode(chose_ax, no_follow=&quot;0&quot;, follow = &quot;1&quot;)) %&gt;%
    mutate(.fill_column = suggestion_type) %&gt;%
    count(user_id, study_condition, .fill_column, chose_ax, num_actions, has_ax_only, has_dx_only, has_dxax, noise_level,
          name = &quot;num_follow_ax&quot;)

gg_df %&gt;%
  ggplot(aes(user_id, num_follow_ax / num_actions, fill=chose_ax)) +
    geom_bar(stat=&quot;identity&quot;) +
    geom_hline(yintercept = 0.5) +
    facet_wrap(vars(study_condition), nrow = 1, scales = &quot;free_y&quot;) +
    labs(y = &quot;Fraction followed AX&quot;) +
    coord_flip() +
    scale_fill_economist()</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-39-1.png' %}" width="1440" /></p>
<pre class="r"><code>gg_df %&gt;%
  filter(chose_ax == &quot;follow&quot;) %&gt;%
  ggplot(aes(study_condition, num_follow_ax / num_actions, fill=.fill_column)) +
    geom_violin() +
    geom_boxplot(width = 0.1) +
    labs(y = &quot;Fraction followed AX&quot;, fill = &quot;noise:has_suggestions&quot;) +
    scale_fill_economist()</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-39-2.png' %}" width="1440" /></p>
<pre class="r"><code># Plot by suggestion type
gg_df =
  plot_df %&gt;%
    mutate(noise_level = fct_rev(noise_level)) %&gt;%
    mutate(chose_ax = factor(chose_ax)) %&gt;%
    mutate(user_id = factor(user_id)) %&gt;%
    mutate(chose_ax = fct_recode(chose_ax, no_follow=&quot;0&quot;, follow = &quot;1&quot;)) %&gt;%
    count(user_id, suggestion_type, chose_ax, num_actions, noise_level,
          name = &quot;num_follow_ax&quot;)

gg_df %&gt;%
  filter(chose_ax == &quot;follow&quot;) %&gt;%
  ggplot(aes(suggestion_type, num_follow_ax / num_actions, fill=suggestion_type)) +
    geom_boxplot() +
    geom_count() +
    facet_grid(cols = vars(noise_level), labeller = label_both) +
    labs(y = &quot;Fraction followed AX&quot;) +
    scale_fill_economist() +
    theme(legend.position = &quot;bottom&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-39-3.png' %}" width="1440" /></p>
<pre class="r"><code>gg_df %&gt;%
  filter(chose_ax == &quot;follow&quot;) %&gt;%
  ggplot(aes(noise_level, num_follow_ax / num_actions, fill=suggestion_type)) +
    geom_boxplot() +
    geom_count() +
    facet_grid(cols = vars(suggestion_type)) +
    labs(y = &quot;Fraction followed AX&quot;) +
    scale_fill_economist() +
    theme(legend.position = &quot;bottom&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-39-4.png' %}" width="1440" /></p>
<pre class="r"><code>p1 = gg_df %&gt;%
  filter(chose_ax == &quot;follow&quot;) %&gt;%
  ggplot(aes(suggestion_type, num_follow_ax / num_actions)) +
    geom_violin() +
    geom_point(aes(color = paste(suggestion_type, noise_level, sep = &quot;:&quot;)),
               size = 2, position = position_jitter(width = .2, height = 0)) +
    stat_summary(fun = median, geom = &quot;point&quot;, shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    legend_none()</code></pre>
<pre><code>## Warning: Ignoring unknown parameters: fun</code></pre>
<pre class="r"><code>p2 = gg_df %&gt;%
  filter(chose_ax == &quot;follow&quot;) %&gt;%
  ggplot(aes(noise_level, num_follow_ax / num_actions)) +
    geom_violin() +
    geom_point(aes(color = paste(suggestion_type, noise_level, sep = &quot;:&quot;)),
               size = 2, position = position_jitter(width = .1, height = 0)) +
    stat_summary(fun = median, geom = &quot;point&quot;, shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(color = &quot;Type:Noise&quot;) +
    theme(legend.position = &quot;right&quot;) +
    guides(size = F)</code></pre>
<pre><code>## Warning: Ignoring unknown parameters: fun</code></pre>
<pre class="r"><code>grid.arrange(p1, p2, ncol = 2)</code></pre>
<pre><code>## No summary function supplied, defaulting to `mean_se()
## No summary function supplied, defaulting to `mean_se()</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-39-5.png' %}" width="1440" /></p>
</div>
<div id="model-3" class="section level2">
<h2>Model</h2>
<p>Based on the data, we assume the following structural model:</p>
<p><span class="math display">\[CAX_{ij} = Bernoulli(p_{ij})\]</span> <span class="math display">\[\begin{aligned}
logit(p_{ij}) &amp;= \beta_0 + \beta_{0i} + \beta_{dx}\text{dx}_i +\\
&amp;\beta_{noise_L}\text{noise}_i + \beta_{noise_Q}\text{noise}_i +\\
&amp;\beta_{no}\text{no}_i + \mathbf{\beta_{demo}X_{demo,i}} + \beta_{state} \text{state}_{ij}
\end{aligned}\]</span> <span class="math display">\[\begin{aligned}
\beta_{.} &amp;\sim Normal(0, 10) \\
\beta_{0i} &amp;\sim Normal(0, \sigma_i) \\
\sigma_i &amp;\sim HalfStudent(3, 0, 10)
\end{aligned}\]</span></p>
<p>The prior for the <span class="math inline">\(\sigma_i\)</span> parameter is the default used in <code>brms</code>; I see no need to change it.</p>
<p>Model fitting:</p>
<pre class="r"><code># The null model
chose_ax.model.null = brm(
  chose_ax ~ 0 + Intercept + (1 | user_id),
  family = &quot;bernoulli&quot;,
  prior = c(set_prior(&quot;normal(0, 10)&quot;, class = &quot;b&quot;),
            set_prior(&quot;student_t(3, 0, 10)&quot;, class = &quot;sd&quot;)),
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
chose_ax.model.null = add_criterion(chose_ax.model.null, &quot;waic&quot;)
chose_ax.model.null = add_criterion(chose_ax.model.null, &quot;loo&quot;, reloo = T)
saveModel(chose_ax.model.null)

# The trend model to see if there is a trend in the noise level variable
chose_ax.model.t = brm(
  chose_ax ~ 0 + Intercept + (has_dx + noise_level) + (age_group + robot_experience + gender) + num_optimal + state_idx_rescaled + (1 | user_id),
  family = bernoulli,
  prior = c(set_prior(&quot;normal(0, 10)&quot;, class = &quot;b&quot;),
            set_prior(&quot;student_t(3, 0, 10)&quot;, class = &quot;sd&quot;)),
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
chose_ax.model.t = add_criterion(chose_ax.model.t, &quot;waic&quot;)
chose_ax.model.t = add_criterion(chose_ax.model.t, &quot;loo&quot;, reloo = T)
saveModel(chose_ax.model.t)

# The values model, to see if specific values of the noise level variable are significant
chose_ax.model.v = brm(
  chose_ax ~ 0 + Intercept + (has_dx + noise_level_f) + (age_group + robot_experience + gender) + num_optimal + state_idx_rescaled + (1 | user_id),
  family = bernoulli,
  prior = c(set_prior(&quot;normal(0, 10)&quot;, class = &quot;b&quot;),
            set_prior(&quot;student_t(3, 0, 10)&quot;, class = &quot;sd&quot;)),
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
chose_ax.model.v = add_criterion(chose_ax.model.v, &quot;waic&quot;)
chose_ax.model.v = add_criterion(chose_ax.model.v, &quot;loo&quot;, reloo = T)
saveModel(chose_ax.model.v)</code></pre>
<pre class="r"><code># Load the models
chose_ax.model.null = loadModel(&quot;chose_ax.model.null&quot;)
chose_ax.model.t = loadModel(&quot;chose_ax.model.t&quot;)
chose_ax.model.v = loadModel(&quot;chose_ax.model.v&quot;)</code></pre>
<p>Model fitting results:</p>
<pre class="r"><code>print(performance::r2(chose_ax.model.null))</code></pre>
<pre><code>## # Bayesian R2 with Standard Error
##
##   Conditional R2: 0.154 [0.020]
##      Marginal R2: 0.000 [0.000]</code></pre>
<pre class="r"><code># Print the parameters, and some initial diagnostics
print(model_parameters(chose_ax.model.t,
                       centrality = &quot;median&quot;,
                       ci = 0.89,
                       ci_method = &quot;hdi&quot;,
                       test = c(&quot;pd&quot;, &quot;rope&quot;),
                       rope_range = c(-0.055, 0.055),
                       rope_ci = 1,
                       effects = &quot;fixed&quot;))</code></pre>
<pre><code>## # Fixed effects
##
## Parameter          | Median |         89% CI |     pd | % in ROPE |  ESS |  Rhat
## --------------------------------------------------------------------------------
## (Intercept)        |   0.55 | [-0.17,  1.31] | 87.98% |     5.05% | 1076 | 1.009
## has_dx1            |  -0.15 | [-0.36,  0.06] | 87.60% |    16.80% | 1482 | 1.002
## noise_level.L      |  -0.37 | [-0.72,  0.01] | 94.67% |     5.35% | 1578 | 1.001
## noise_level.Q      |   0.18 | [-0.19,  0.53] | 78.17% |    13.98% | 1349 | 1.000
## age_group1         |   0.16 | [-0.37,  0.70] | 68.73% |    11.88% | 1578 | 1.003
## age_group2         |  -0.26 | [-0.65,  0.17] | 84.58% |    10.17% | 1234 | 1.005
## age_group3         |  -0.10 | [-0.54,  0.36] | 63.68% |    14.47% | 1345 | 1.003
## age_group4         |   0.58 | [ 0.09,  1.15] | 95.65% |     2.62% | 1623 | 1.001
## age_group5         |  -0.80 | [-1.40, -0.18] | 97.88% |     1.62% | 1207 | 1.002
## age_group6         |   0.47 | [-0.37,  1.41] | 80.30% |     5.38% | 1545 | 1.000
## robot_experience1  |   0.65 | [ 0.25,  1.07] | 99.28% |     0.75% | 1335 | 1.005
## robot_experience2  |   0.32 | [-0.19,  0.80] | 84.15% |     8.70% | 1211 | 1.003
## robot_experience3  |   0.16 | [-0.73,  0.97] | 62.45% |     8.50% | 1165 | 1.000
## robot_experience4  |  -0.16 | [-0.90,  0.53] | 63.80% |     9.45% | 1318 | 1.001
## gender1            |  -0.23 | [-0.43,  0.01] | 94.38% |     9.03% | 1453 | 1.001
## num_optimal        |   0.07 | [-0.07,  0.19] | 78.62% |    36.65% | 1260 | 1.008
## state_idx_rescaled |  -1.53 | [-1.89, -1.17] |   100% |        0% | 3506 | 1.000</code></pre>
<pre class="r"><code>print(performance::r2(chose_ax.model.t))</code></pre>
<pre><code>## # Bayesian R2 with Standard Error
##
##   Conditional R2: 0.223 [0.018]
##      Marginal R2: 0.125 [0.021]</code></pre>
<pre class="r"><code>print(model_parameters(chose_ax.model.v,
                       centrality = &quot;median&quot;,
                       ci = 0.89,
                       ci_method = &quot;hdi&quot;,
                       test = c(&quot;pd&quot;, &quot;rope&quot;),
                       rope_range = c(-0.055, 0.055),
                       rope_ci = 1,
                       effects = &quot;fixed&quot;))</code></pre>
<pre><code>## # Fixed effects
##
## Parameter          | Median |         89% CI |     pd | % in ROPE |  ESS |  Rhat
## --------------------------------------------------------------------------------
## (Intercept)        |   0.54 | [-0.22,  1.24] | 88.30% |     4.58% | 1145 | 1.002
## has_dx1            |  -0.15 | [-0.36,  0.06] | 87.55% |    16.60% | 1326 | 1.000
## noise_level_f1     |   0.33 | [ 0.02,  0.63] | 96.03% |     5.20% | 1121 | 1.006
## noise_level_f2     |  -0.16 | [-0.47,  0.12] | 79.07% |    16.78% | 1113 | 1.003
## age_group1         |   0.15 | [-0.38,  0.68] | 67.17% |    12.12% | 1444 | 1.003
## age_group2         |  -0.26 | [-0.66,  0.16] | 84.97% |     9.93% | 1535 | 1.002
## age_group3         |  -0.08 | [-0.53,  0.37] | 61.95% |    15.05% | 1425 | 1.002
## age_group4         |   0.59 | [ 0.08,  1.13] | 96.50% |     2.57% | 1653 | 1.000
## age_group5         |  -0.85 | [-1.46, -0.21] | 98.45% |     0.83% | 1257 | 1.001
## age_group6         |   0.50 | [-0.38,  1.44] | 81.53% |     4.83% | 1439 | 1.000
## robot_experience1  |   0.65 | [ 0.24,  1.06] | 99.38% |     0.90% | 1234 | 1.003
## robot_experience2  |   0.32 | [-0.16,  0.81] | 85.17% |     8.25% | 1322 | 1.002
## robot_experience3  |   0.15 | [-0.68,  0.99] | 60.98% |     8.00% | 1263 | 1.005
## robot_experience4  |  -0.17 | [-0.89,  0.47] | 65.83% |     9.03% | 1391 | 1.003
## gender1            |  -0.22 | [-0.43,  0.00] | 94.95% |     8.43% | 1749 | 1.000
## num_optimal        |   0.07 | [-0.07,  0.20] | 79.15% |    36.58% | 1158 | 1.003
## state_idx_rescaled |  -1.54 | [-1.91, -1.18] |   100% |        0% | 3068 | 0.999</code></pre>
<pre class="r"><code>print(performance::r2(chose_ax.model.v))</code></pre>
<pre><code>## # Bayesian R2 with Standard Error
##
##   Conditional R2: 0.223 [0.018]
##      Marginal R2: 0.126 [0.021]</code></pre>
</div>
<div id="diagnostic-3" class="section level2">
<h2>Diagnostic</h2>
<pre class="r"><code># Alternative diagnostics: diagnostic_posterior(scenario_completed.model.v)

# LOO cross-validation results
print(chose_ax.model.t$criteria$loo$estimates)</code></pre>
<pre><code>##            Estimate        SE
## elpd_loo -703.58297 14.076908
## p_loo      79.74941  2.395009
## looic    1407.16594 28.153816</code></pre>
<pre class="r"><code>print(chose_ax.model.v$criteria$loo$estimates)</code></pre>
<pre><code>##           Estimate        SE
## elpd_loo -703.6589 14.078174
## p_loo      79.7338  2.375157
## looic    1407.3179 28.156349</code></pre>
<pre class="r"><code>print(loo_compare(chose_ax.model.null, chose_ax.model.t, chose_ax.model.v))</code></pre>
<pre><code>##                     elpd_diff se_diff
## chose_ax.model.t      0.0       0.0
## chose_ax.model.v     -0.1       0.2
## chose_ax.model.null -25.4       7.7</code></pre>
<pre class="r"><code># Diagnostics of the sampling process
par(mfrow=c(1,2))
plot(chose_ax.model.t$criteria$loo, main = &quot;Trend Model&quot;)
plot(chose_ax.model.v$criteria$loo, main = &quot;Values Model&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-43-1.png' %}" width="1440" /></p>
<pre class="r"><code># Include posterior predictive checks of the sampling (are sampling according
# to the data that we actually have?)
p0 = pp_check(chose_ax.model.null, type = &quot;bars&quot;, stat = &quot;median&quot;) + ggtitle(&quot;null model&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars&#39; by default.</code></pre>
<pre><code>## Warning: The following arguments were unrecognized and ignored: stat</code></pre>
<pre class="r"><code>p1 = pp_check(chose_ax.model.t, type = &quot;bars_grouped&quot;, group = &quot;has_dx&quot;, stat = &quot;median&quot;) + ggtitle(&quot;t:has_dx&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars_grouped&#39; by default.</code></pre>
<pre><code>## Warning: The following arguments were unrecognized and ignored: stat</code></pre>
<pre class="r"><code>p2 = pp_check(chose_ax.model.t, type = &quot;bars_grouped&quot;, group = &quot;noise_level&quot;, stat = &quot;median&quot;) + ggtitle(&quot;t:noise_level&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars_grouped&#39; by default.</code></pre>
<pre><code>## Warning: The following arguments were unrecognized and ignored: stat</code></pre>
<pre class="r"><code>grid.arrange(p1, p2, p0, nrow = 2, ncol = 2)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-43-2.png' %}" width="1440" /></p>
<pre class="r"><code>mcmc_intervals(as.matrix(chose_ax.model.t), regex_pars = &quot;b_|^shape&quot;, prob = 0.90, prob_outer = 0.95) +
  ggtitle(&quot;Trends Model&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-43-3.png' %}" width="1440" /></p>
<pre class="r"><code>p1 = pp_check(chose_ax.model.v, type = &quot;bars_grouped&quot;, group = &quot;has_dx&quot;, stat = &quot;median&quot;) + ggtitle(&quot;v:has_dx&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars_grouped&#39; by default.</code></pre>
<pre><code>## Warning: The following arguments were unrecognized and ignored: stat</code></pre>
<pre class="r"><code>p2 = pp_check(chose_ax.model.v, type = &quot;bars_grouped&quot;, group = &quot;noise_level_f&quot;, stat = &quot;median&quot;) + ggtitle(&quot;v:noise_level&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars_grouped&#39; by default.</code></pre>
<pre><code>## Warning: The following arguments were unrecognized and ignored: stat</code></pre>
<pre class="r"><code>grid.arrange(p1, p2, p0, nrow = 2, ncol = 2)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-43-4.png' %}" width="1440" /></p>
<pre class="r"><code>mcmc_intervals(as.matrix(chose_ax.model.v), regex_pars = &quot;b_|^shape&quot;, prob = 0.90, prob_outer = 0.95) +
  ggtitle(&quot;Values Model&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-43-5.png' %}" width="1440" /></p>
<pre class="r"><code># Plot predictions vs original
preds_p_df = chose_ax.model.null %&gt;% predict() %&gt;% as_tibble()
p1 =
  preds_p_df %&gt;%
    add_column(chose_ax = chose_ax.model.null$data$chose_ax) %&gt;%
    ggplot(aes(x = Estimate, y = chose_ax, color = chose_ax)) +
      geom_point() +
      labs(x = &quot;Predicted values&quot;) +
      ylim(0, 1) +
      xlim(0, 1) +
      ggtitle(&quot;Null Model&quot;)

preds_p_df = plot_df %&gt;%
  add_predicted_draws(chose_ax.model.null) %&gt;%
  group_by(chose_ax, .prediction) %&gt;%
  count()
p2 =
  preds_p_df %&gt;%
    ggplot(aes(x = chose_ax, y = .prediction, fill = n / sum(preds_p_df$n))) +
      geom_tile() +
      geom_text(aes(label = sprintf(&quot;%1.2f&quot;, n / sum(preds_p_df$n))), size = 10, vjust = 1) +
      scale_fill_gradient(limits = c(0, 1))

preds_p_df = chose_ax.model.t %&gt;% predict() %&gt;% as_tibble()
p3 =
  preds_p_df %&gt;%
    add_column(chose_ax = chose_ax.model.t$data$chose_ax) %&gt;%
    ggplot(aes(x = Estimate, y = chose_ax, color = chose_ax)) +
      geom_point() +
      labs(x = &quot;Predicted values&quot;) +
      ylim(0, 1) +
      xlim(0, 1) +
      ggtitle(&quot;Trends Model&quot;)

preds_p_df = plot_df %&gt;%
  add_predicted_draws(chose_ax.model.t) %&gt;%
  group_by(chose_ax, .prediction) %&gt;%
  count()
p4 =
  preds_p_df %&gt;%
    ggplot(aes(x = chose_ax, y = .prediction, fill = n / sum(preds_p_df$n))) +
      geom_tile() +
      geom_text(aes(label = sprintf(&quot;%1.2f&quot;, n / sum(preds_p_df$n))), size = 10, vjust = 1) +
      scale_fill_gradient(limits = c(0, 1))

# Plot predictions vs original
preds_p_df = chose_ax.model.v %&gt;% predict() %&gt;% as_tibble()
p5 =
  preds_p_df %&gt;%
    add_column(chose_ax = chose_ax.model.v$data$chose_ax) %&gt;%
    ggplot(aes(x = Estimate, y = chose_ax, color = chose_ax)) +
      geom_point() +
      labs(x = &quot;Predicted values&quot;) +
      ylim(0, 1) +
      xlim(0, 1) +
      ggtitle(&quot;Values Model&quot;)

preds_p_df = plot_df %&gt;%
  add_predicted_draws(chose_ax.model.v) %&gt;%
  group_by(chose_ax, .prediction) %&gt;%
  count()
p6 =
  preds_p_df %&gt;%
    ggplot(aes(x = chose_ax, y = .prediction, fill = n / sum(preds_p_df$n))) +
      geom_tile() +
      geom_text(aes(label = sprintf(&quot;%1.2f&quot;, n / sum(preds_p_df$n))), size = 10, vjust = 1) +
      scale_fill_gradient(limits = c(0, 1))

grid.arrange(p1, p3, p5, p2, p4, p6, nrow = 2, ncol = 3)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-43-6.png' %}" width="1440" /></p>
<p>We are better able to recreate the data, if only marginally, with the model.</p>
</div>
<div id="inference-3" class="section level2">
<h2>Inference</h2>
<pre class="r"><code># Compute the bayes factor for each parameter value. Note: we compare
# here against the ROPE, and not the degenerate null hypothesis of 0
# Bayes-Factor values would be more significant in the latter case
# According to the Raftery, 1995 rules
# bf = 1 - 3: Weak
# bf = 3 - 20: Positive
# bf = 20 - 150: Strong
# bf &gt; 150: Very strong
print(bayesfactor_rope(chose_ax.model.null, null = c(-0.055, 0.055)), n = 30)
print(bayesfactor_rope(chose_ax.model.t, null = c(-0.055, 0.055)), n = 30)
print(bayesfactor_rope(chose_ax.model.v, null = c(-0.055, 0.055)), n = 30)</code></pre>
<pre class="r"><code>options(digits = 3)
# Check the significance of each hypothesis
h_df = as_tibble(insight::get_parameters(chose_ax.model.t)) %&gt;%
  transmute(
    Intercept = b_Intercept,
    has_dx_test = -2 * b_has_dx1,
    noise_levelL_test = b_noise_level.L,
    noise_levelQ_test = b_noise_level.Q
  )

hyp_results = test_hypotheses(h_df, rope_values = c(-0.055, 0.055))
print(hyp_results %&gt;% as.matrix())</code></pre>
<pre><code>##      Parameter           Estimate CI_low   CI_high  Effect_Size pd
## [1,] &quot;Intercept&quot;         &quot; 0.551&quot; &quot;-0.175&quot; &quot;1.3102&quot; &quot;0.3036&quot;    &quot;0.880&quot;
## [2,] &quot;has_dx_test&quot;       &quot; 0.301&quot; &quot;-0.111&quot; &quot;0.7224&quot; &quot;0.1657&quot;    &quot;0.876&quot;
## [3,] &quot;noise_levelL_test&quot; &quot;-0.367&quot; &quot;-0.718&quot; &quot;0.0141&quot; &quot;0.2022&quot;    &quot;0.947&quot;
## [4,] &quot;noise_levelQ_test&quot; &quot; 0.180&quot; &quot;-0.195&quot; &quot;0.5308&quot; &quot;0.0994&quot;    &quot;0.782&quot;
##      ROPE_Percentage ROPE_Equivalence
## [1,] &quot;0.0505&quot;        &quot;Undecided&quot;
## [2,] &quot;0.0870&quot;        &quot;Undecided&quot;
## [3,] &quot;0.0535&quot;        &quot;Undecided&quot;
## [4,] &quot;0.1398&quot;        &quot;Undecided&quot;</code></pre>
<pre class="r"><code>h_df = as_tibble(insight::get_parameters(chose_ax.model.v)) %&gt;%
  transmute(
    Intercept = b_Intercept,
    has_dx_test = -2 * b_has_dx1,
    noise_level1_test = b_noise_level_f1,
    noise_level2_test = b_noise_level_f2,
    noise_level3_test = - b_noise_level_f1 - b_noise_level_f2
  )

hyp_results = test_hypotheses(h_df, rope_values = c(-0.055, 0.055))
print(hyp_results %&gt;% as.matrix())</code></pre>
<pre><code>##      Parameter           Estimate CI_low    CI_high Effect_Size pd
## [1,] &quot;Intercept&quot;         &quot; 0.537&quot; &quot;-0.2249&quot; &quot;1.245&quot; &quot;0.2958&quot;    &quot;0.883&quot;
## [2,] &quot;has_dx_test&quot;       &quot; 0.305&quot; &quot;-0.1186&quot; &quot;0.728&quot; &quot;0.1679&quot;    &quot;0.875&quot;
## [3,] &quot;noise_level1_test&quot; &quot; 0.330&quot; &quot; 0.0172&quot; &quot;0.634&quot; &quot;0.1818&quot;    &quot;0.960&quot;
## [4,] &quot;noise_level2_test&quot; &quot;-0.155&quot; &quot;-0.4697&quot; &quot;0.124&quot; &quot;0.0857&quot;    &quot;0.791&quot;
## [5,] &quot;noise_level3_test&quot; &quot;-0.185&quot; &quot;-0.4682&quot; &quot;0.104&quot; &quot;0.1018&quot;    &quot;0.837&quot;
##      ROPE_Percentage ROPE_Equivalence
## [1,] &quot;0.0457&quot;        &quot;Undecided&quot;
## [2,] &quot;0.0745&quot;        &quot;Undecided&quot;
## [3,] &quot;0.0520&quot;        &quot;Undecided&quot;
## [4,] &quot;0.1678&quot;        &quot;Undecided&quot;
## [5,] &quot;0.1500&quot;        &quot;Undecided&quot;</code></pre>
<pre class="r"><code>options(digits = 7)</code></pre>
<p><strong>Results</strong>:</p>
<p>Not really significant result of Linear noise.</p>
<p>(The ROPE is defined as [-0.055, 0.055]. It is the range is suggested in the literature for logistic models and it corresponds to a probability range of 0.11. Therefore, a probability change of less than 0.055 is considered no different from a probability change of 0)</p>
</div>
<div id="posterior-plots-3" class="section level2">
<h2>Posterior Plots</h2>
<pre class="r"><code># A visualization data frame
gg_df = plot_df %&gt;%
    mutate(noise_level = fct_rev(noise_level)) %&gt;%
    mutate(chose_ax = factor(chose_ax)) %&gt;%
    mutate(user_id = factor(user_id)) %&gt;%
    mutate(chose_ax = fct_recode(chose_ax, no_follow=&quot;0&quot;, follow = &quot;1&quot;)) %&gt;%
    count(user_id, chose_ax, num_actions, suggestion_type, has_dx, noise_level,
          name = &quot;num_follow_ax&quot;)

# Simulate predictions on new data
p_df =
  plot_df %&gt;%
  data_grid(chose_ax,
            has_dx,
            noise_level_f,
            gender,
            age_group,
            robot_experience,
            num_optimal,
            state_idx_rescaled = seq_range(state_idx_rescaled, n = 4),
            .model = chose_ax.model.v)

fits_p_df = p_df %&gt;%
  add_fitted_draws(chose_ax.model.v, re_formula = NA, seed = default_seed) %&gt;%
  mutate(suggestion_type = ifelse(has_dx == F, &quot;AX&quot;, &quot;DXAX&quot;)) %&gt;%
  mutate(suggestion_type = factor(suggestion_type, levels = c(&quot;AX&quot;, &quot;DXAX&quot;)))
gc()

pars_p_df = chose_ax.model.t %&gt;%
  extract_draws(newdata = p_df %&gt;% rename(noise_level = noise_level_f), re_formula = NA, nsamples = 100)
gc()

# Plot the posterior distributions. In each of the following, the reference ROPE band
# can be marked based on 3 methods. Make sure to update that ROPE value baased on the
# method that we want to display.

# If centering ROPE on the posterior distributions. Here we center the ROPE around the posterior
# predicted value for the AX condition
preds_p_df = p_df %&gt;%
  add_predicted_draws(chose_ax.model.v, re_formula = NA)  %&gt;% # 0, 1 predictions for each predicted value
  ungroup() %&gt;%
  count(has_dx, noise_level_f, .prediction) %&gt;%
  mutate(suggestion_type = ifelse(has_dx == F, &quot;AX&quot;, &quot;DXAX&quot;)) %&gt;%
  mutate(suggestion_type = factor(suggestion_type, levels = c(&quot;AX&quot;, &quot;DXAX&quot;)))
rope_value = median(
  preds_p_df %&gt;%
    filter(suggestion_type == &quot;AX&quot;) %&gt;%
    spread(.prediction, n) %&gt;%
    mutate(.value = `1` / (`0` + `1`)) %&gt;%
    pull(.value)
)
gc()

gg_df %&gt;%
  filter(chose_ax == &#39;follow&#39;) %&gt;%
  ggplot(aes(x = has_dx, y = num_follow_ax / num_actions)) +
    geom_rect(ymin = rope_value-0.055, ymax = rope_value+0.055,
              xmin = -Inf, xmax = Inf,
              color = &quot;transparent&quot;, fill = &quot;white&quot;) +
    annotate(&quot;text&quot;, x = 0.5, y = rope_value, label = &quot;ROPE&quot;, angle = 90) +
    stat_halfeye(aes(y = .value), data = fits_p_df, position = position_nudge(x = 0.1)) +
    geom_point(aes(color = paste(has_dx, noise_level, sep = &quot;:&quot;)),
               size = 2, position = position_jitter(width = .1, height = 0)) +
    stat_summary(fun = median, geom = &quot;point&quot;, shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(y = &quot;CAX&quot;, color = &quot;HasDX:Noise&quot;)

gg_df %&gt;%
  filter(chose_ax == &#39;follow&#39;) %&gt;%
  mutate(noise_level = fct_rev(noise_level)) %&gt;%
  ggplot(aes(x = noise_level, y = num_follow_ax / num_actions)) +
    geom_rect(ymin = rope_value-0.055, ymax = rope_value+0.055,
              xmin = -Inf, xmax = Inf,
              color = &quot;transparent&quot;, fill = &quot;white&quot;) +
    annotate(&quot;text&quot;, x = 0.5, y = rope_value, label = &quot;ROPE&quot;, angle = 90) +
    stat_halfeye(aes(y = .value, x = noise_level_f), data = fits_p_df, position = position_nudge(x = 0.1)) +
    geom_point(aes(color = paste(has_dx, noise_level, sep = &quot;:&quot;)),
               size = 2, position = position_jitter(width = .1, height = 0)) +
    stat_summary(fun = median, geom = &quot;point&quot;, shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(y = &quot;CAX&quot;, color = &quot;HasDX:Noise&quot;)

# Create a plot to show the trend
pars_p_df_X = pars_p_df$dpars$mu$fe$X[,c(&quot;Intercept&quot;, &quot;noise_level.L&quot;)]
pars_p_df_b = pars_p_df$dpars$mu$fe$b[,c(&quot;b_Intercept&quot;, &quot;b_noise_level.L&quot;)]
pars_p_df_y = pars_p_df_X %*% t(pars_p_df_b)
pars_p_df_y = inv_logit_scaled(pars_p_df_y)
p_df =p_df %&gt;%
  bind_cols(as_tibble(pars_p_df_y)) %&gt;%
  gather(key = &quot;.sample&quot;, value = &quot;.value&quot;, V1:V100) %&gt;%
  rename(noise_level = noise_level_f)
gg_df %&gt;%
  filter(chose_ax == &#39;follow&#39;) %&gt;%
  mutate(noise_level = fct_rev(noise_level)) %&gt;%
  ggplot(aes(x = noise_level, y = num_follow_ax / num_actions)) +
    geom_rect(ymin = rope_value-0.055, ymax = rope_value+0.055,
              xmin = -Inf, xmax = Inf,
              color = &quot;transparent&quot;, fill = &quot;white&quot;) +
    annotate(&quot;text&quot;, x = 0.5, y = rope_value, label = &quot;ROPE&quot;, angle = 90) +
    stat_halfeye(aes(y = .value, x = noise_level_f), data = fits_p_df, position = position_nudge(x = 0.1)) +
    geom_line(aes(y = .value, group = .sample, alpha = 0.3), data = p_df) +
    geom_point(aes(color = paste(has_dx, noise_level, sep = &quot;:&quot;)),
               size = 2, position = position_jitter(width = .1, height = 0)) +
    stat_summary(fun = median, geom = &quot;point&quot;, shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(y = &quot;CAX&quot;, color = &quot;HasDX:Noise&quot;) +
    guides(alpha = F)

# Remove the giant fits data frames
rm(preds_p_df) # If using the posterior-predictions-based ROPE
rm(fits_p_df, pars_p_df, pars_p_df_b, pars_p_df_X, pars_p_df_y, p_df, gg_df)</code></pre>
<pre class="r"><code>eff = fixef(chose_ax.model.t)
ax_eff = inv_logit_scaled(
  eff[&quot;Intercept&quot;, &quot;Estimate&quot;]
  + eff[&quot;has_dx1&quot;, &quot;Estimate&quot;]
)
dxax_eff = inv_logit_scaled(
  eff[&quot;Intercept&quot;, &quot;Estimate&quot;]
  - eff[&quot;has_dx1&quot;, &quot;Estimate&quot;]
)

gg_df = plot_df %&gt;%
  mutate(chose_ax = factor(chose_ax)) %&gt;%
  mutate(chose_ax = fct_recode(chose_ax, no_follow=&quot;0&quot;, follow=&quot;1&quot;),
         has_dx = fct_recode(has_dx, &quot;No DX&quot;=&quot;FALSE&quot;, &quot;DX&quot;=&quot;TRUE&quot;),
         noise_level = fct_recode(noise_level, &quot;Acc: 100%&quot;=&quot;0.0&quot;, &quot;Acc: 90%&quot;=&quot;1.0&quot;, &quot;Acc: 80%&quot;=&quot;2.0&quot;)) %&gt;%
  count(user_id, suggestion_type, chose_ax, num_actions, has_dx, noise_level,
        name = &quot;num_follow_ax&quot;) %&gt;%
  mutate(estimate = ifelse(suggestion_type == &quot;AX&quot;, ax_eff, -1)) %&gt;%
  mutate(estimate = ifelse(suggestion_type == &quot;DXAX&quot;, dxax_eff, estimate))

p1 = gg_df %&gt;%
  filter(chose_ax == &quot;follow&quot;) %&gt;%
  ggplot(aes(suggestion_type, num_follow_ax / num_actions, group = suggestion_type, colour=suggestion_type)) +
    geom_count(aes(colour = NULL)) +
    geom_boxplot(aes(y = estimate)) +
    scale_y_continuous(limits = c(0.0, 1.2), breaks = c(0.0, 0.5, 1.0)) +
    labs(y = NULL, x = NULL) +
    scale_colour_manual(values = c(&quot;#e51400&quot;, &quot;#008a00&quot;, &quot;#f0a30a&quot;, &quot;#a4c400&quot;)) +
    theme_economist_white(base_size = 13) +
    theme(plot.background = element_rect(fill = &quot;white&quot;),
          axis.title.y = element_blank()) +
    legend_none()

eff = fixef(chose_ax.model.v)
noise_0_eff = inv_logit_scaled(eff[&quot;Intercept&quot;, &quot;Estimate&quot;] + eff[&quot;noise_level_f1&quot;,&quot;Estimate&quot;])
noise_1_eff = inv_logit_scaled(eff[&quot;Intercept&quot;, &quot;Estimate&quot;] + eff[&quot;noise_level_f2&quot;,&quot;Estimate&quot;])
noise_2_eff = inv_logit_scaled(eff[&quot;Intercept&quot;, &quot;Estimate&quot;] - eff[&quot;noise_level_f1&quot;,&quot;Estimate&quot;] - eff[&quot;noise_level_f2&quot;,&quot;Estimate&quot;])

gg_df = gg_df %&gt;%
  mutate(estimate = if_else(noise_level == &quot;Acc: 100%&quot;, noise_0_eff, estimate)) %&gt;%
  mutate(estimate = if_else(noise_level == &quot;Acc: 90%&quot;, noise_1_eff, estimate)) %&gt;%
  mutate(estimate = if_else(noise_level == &quot;Acc: 80%&quot;, noise_2_eff, estimate)) %&gt;%
  mutate(noise_level = fct_rev(noise_level))

p2 = gg_df %&gt;%
  filter(chose_ax == &quot;follow&quot;) %&gt;%
  ggplot(aes(noise_level, num_follow_ax / num_actions, group = noise_level, colour = noise_level)) +
    geom_hline(yintercept = inv_logit_scaled(eff[&quot;Intercept&quot;, &quot;Estimate&quot;]),
               size = .7, linetype = &quot;dashed&quot;, color = &quot;grey&quot;) +
    geom_count(aes(colour = NULL)) +
    geom_boxplot(aes(y = estimate)) +
    annotate(&quot;segment&quot;,
             x = 3, y = noise_0_eff,
             xend = 2, yend = noise_1_eff,
             colour = &quot;black&quot;) +
    annotate(&quot;segment&quot;,
             x = 1, y = noise_2_eff,
             xend = 2, yend = noise_1_eff,
             color = &quot;black&quot;) +
    annotate(&quot;text&quot;,
             x = 3.5,
             y = inv_logit_scaled(eff[&quot;Intercept&quot;, &quot;Estimate&quot;] - 0.35),
             label = &quot;Mean of all levels&quot;,
             angle = 90,
             group = NA,
             colour = &quot;black&quot;) +
    scale_y_continuous(limits = c(0.0, 1.0), breaks = c(0.0, 0.5, 1.0)) +
    labs(y = &quot;RDX&quot;, x = NULL) +
    scale_colour_manual(values = c(&quot;#e51400&quot;, &quot;#008a00&quot;, &quot;#f0a30a&quot;, &quot;#a4c400&quot;)) +
    scale_size_area(max_size = 1) +
    theme_economist_white(base_size = 13) +
    theme(plot.background = element_rect(fill = &quot;white&quot;),
          axis.title.y = element_blank(),
          axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
    legend_none()

grid.arrange(p1, p2, nrow = 1, ncol = 2)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-47-1.png' %}" width="672" /></p>
<pre class="r"><code>rm(p1, p2)</code></pre>
</div>
</div>
<div id="cdx-compliance-with-dx-suggestions" class="section level1">
<h1>CDX: Compliance with DX Suggestions</h1>
<p><strong>Did the user take an follow the DX suggestions that were provided to them?</strong></p>
<p>In the code, the variable might be referred to as <code>chose_dx</code>, <code>follow_dx</code>, etc. depending on the version of the codebase</p>
<pre class="r"><code>plot_df = actions %&gt;%
  filter(has_dx == T) %&gt;%
  select(X1, id, user_id, study_condition, start_condition, num_optimal,
         state_idx, state_idx_rescaled,
         age_group, gender, robot_experience,
         noise_level, noise_level_f, has_noise, suggestion_type,
         has_dx, has_ax, has_ax_only, has_dx_only, has_dxax,
         scenario_completed, num_actions, chose_dx)

# With these dropped levels, the suggestion_type variable is the same as the
# boolean flags for each suggestion type
plot_df$suggestion_type = droplevels(plot_df$suggestion_type)
plot_df$study_condition = droplevels(plot_df$study_condition)

print(summary(plot_df %&gt;%
                mutate(chose_dx = factor(chose_dx),
                       scenario_completed = factor(scenario_completed)) %&gt;%
                select(-X1, -id)
))</code></pre>
<pre><code>##     user_id      study_condition start_condition     num_optimal
##  Min.   :  1.0   DX_100  :247    Length:1322        Min.   :3.00
##  1st Qu.: 64.0   DXAX_100:193    Class :character   1st Qu.:4.00
##  Median :123.5   DX_90   :254    Mode  :character   Median :4.00
##  Mean   :109.5   DXAX_90 :183                       Mean   :4.72
##  3rd Qu.:160.0   DX_80   :256                       3rd Qu.:7.00
##  Max.   :200.0   DXAX_80 :189                       Max.   :7.00
##
##    state_idx      state_idx_rescaled age_group gender  robot_experience
##  Min.   :  1.00   Min.   :0.0000     2:112     F:544   0:840
##  1st Qu.: 37.00   1st Qu.:0.2118     3:318     M:778   1:190
##  Median : 91.00   Median :0.5294     4:321             2:107
##  Mean   : 89.96   Mean   :0.5233     5:169             3: 69
##  3rd Qu.:159.00   3rd Qu.:0.9294     6:137             4:116
##  Max.   :171.00   Max.   :1.0000     7: 87
##                                      8:178
##  noise_level noise_level_f has_noise   suggestion_type   has_dx       has_ax
##  0.0:440     0.0:440       FALSE:440   DX  :757        FALSE:   0   FALSE:757
##  1.0:437     1.0:437       TRUE :882   DXAX:565        TRUE :1322   TRUE :565
##  2.0:445     2.0:445
##
##
##
##
##  has_ax_only  has_dx_only  has_dxax   scenario_completed  num_actions
##  FALSE:1322   FALSE:565   FALSE:757   0:560              Min.   : 3.0
##  TRUE :   0   TRUE :757   TRUE :565   1:762              1st Qu.: 9.0
##                                                          Median :16.0
##                                                          Mean   :14.2
##                                                          3rd Qu.:20.0
##                                                          Max.   :20.0
##
##  chose_dx
##  0:424
##  1:898
##
##
##
##
## </code></pre>
<pre class="r"><code>print(as.matrix(
  plot_df %&gt;%
    filter(chose_dx == 1) %&gt;%
    group_by(study_condition, num_actions, user_id) %&gt;%
    count() %&gt;%
    ungroup() %&gt;%
    mutate(.value = n/num_actions) %&gt;%
    group_by(study_condition) %&gt;%
    summarise(.value = median(.value))
))</code></pre>
<pre><code>##      study_condition .value
## [1,] &quot;DX_100&quot;        &quot;0.8796791&quot;
## [2,] &quot;DXAX_100&quot;      &quot;0.8090909&quot;
## [3,] &quot;DX_90&quot;         &quot;0.7500000&quot;
## [4,] &quot;DXAX_90&quot;       &quot;0.7735043&quot;
## [5,] &quot;DX_80&quot;         &quot;0.6583333&quot;
## [6,] &quot;DXAX_80&quot;       &quot;0.7321429&quot;</code></pre>
<div id="data-4" class="section level2">
<h2>Data</h2>
<pre class="r"><code># Plot by the study condition
gg_df =
  plot_df %&gt;%
    mutate(noise_level = fct_rev(noise_level)) %&gt;%
    mutate(chose_dx = factor(chose_dx)) %&gt;%
    mutate(user_id = factor(user_id)) %&gt;%
    mutate(chose_dx = fct_recode(chose_dx, no_follow=&quot;0&quot;, follow = &quot;1&quot;)) %&gt;%
    mutate(.fill_column = suggestion_type) %&gt;%
    count(user_id, study_condition, .fill_column, chose_dx, num_actions, has_ax_only, has_dx_only, has_dxax, noise_level,
          name = &quot;num_follow_dx&quot;)

gg_df %&gt;%
  ggplot(aes(user_id, num_follow_dx / num_actions, fill=chose_dx)) +
    geom_bar(stat=&quot;identity&quot;) +
    geom_hline(yintercept = 0.5) +
    facet_wrap(vars(study_condition), nrow = 1, scales = &quot;free_y&quot;) +
    labs(y = &quot;Fraction followed DX&quot;) +
    coord_flip() +
    scale_fill_economist()</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-50-1.png' %}" width="1440" /></p>
<pre class="r"><code>gg_df %&gt;%
  filter(chose_dx == &quot;follow&quot;) %&gt;%
  ggplot(aes(study_condition, num_follow_dx / num_actions, fill=.fill_column)) +
    geom_violin() +
    geom_boxplot(width = 0.1) +
    labs(y = &quot;Fraction followed DX&quot;, fill = &quot;noise:has_suggestions&quot;) +
    scale_fill_economist()</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-50-2.png' %}" width="1440" /></p>
<pre class="r"><code># Plot by suggestion type
gg_df =
  plot_df %&gt;%
    mutate(noise_level = fct_rev(noise_level)) %&gt;%
    mutate(chose_dx = factor(chose_dx)) %&gt;%
    mutate(user_id = factor(user_id)) %&gt;%
    mutate(chose_ax = fct_recode(chose_dx, no_follow=&quot;0&quot;, follow = &quot;1&quot;)) %&gt;%
    count(user_id, suggestion_type, chose_dx, num_actions, noise_level,
          name = &quot;num_follow_dx&quot;)

gg_df %&gt;%
  filter(chose_dx == &quot;1&quot;) %&gt;%
  ggplot(aes(suggestion_type, num_follow_dx / num_actions, fill=suggestion_type)) +
    geom_boxplot() +
    geom_count() +
    facet_grid(cols = vars(noise_level), labeller = label_both) +
    labs(y = &quot;Fraction followed DX&quot;) +
    scale_fill_economist() +
    theme(legend.position = &quot;bottom&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-50-3.png' %}" width="1440" /></p>
<pre class="r"><code>gg_df %&gt;%
  filter(chose_dx == &quot;1&quot;) %&gt;%
  ggplot(aes(noise_level, num_follow_dx / num_actions, fill=suggestion_type)) +
    geom_boxplot() +
    geom_count() +
    facet_grid(cols = vars(suggestion_type)) +
    labs(y = &quot;Fraction followed DX&quot;) +
    scale_fill_economist() +
    theme(legend.position = &quot;bottom&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-50-4.png' %}" width="1440" /></p>
<pre class="r"><code>p1 = gg_df %&gt;%
  filter(chose_dx == &quot;1&quot;) %&gt;%
  ggplot(aes(suggestion_type, num_follow_dx / num_actions)) +
    geom_violin() +
    geom_point(aes(color = paste(suggestion_type, noise_level, sep = &quot;:&quot;)),
               size = 2, position = position_jitter(width = .2, height = 0)) +
    stat_summary(fun = median, geom = &quot;point&quot;, shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    legend_none()</code></pre>
<pre><code>## Warning: Ignoring unknown parameters: fun</code></pre>
<pre class="r"><code>p2 = gg_df %&gt;%
  filter(chose_dx == &quot;1&quot;) %&gt;%
  ggplot(aes(noise_level, num_follow_dx / num_actions)) +
    geom_violin() +
    geom_point(aes(color = paste(suggestion_type, noise_level, sep = &quot;:&quot;)),
               size = 2, position = position_jitter(width = .1, height = 0)) +
    stat_summary(fun = median, geom = &quot;point&quot;, shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(color = &quot;Type:Noise&quot;) +
    theme(legend.position = &quot;right&quot;) +
    guides(size = F)</code></pre>
<pre><code>## Warning: Ignoring unknown parameters: fun</code></pre>
<pre class="r"><code>grid.arrange(p1, p2, ncol = 2)</code></pre>
<pre><code>## No summary function supplied, defaulting to `mean_se()
## No summary function supplied, defaulting to `mean_se()</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-50-5.png' %}" width="1440" /></p>
</div>
<div id="model-4" class="section level2">
<h2>Model</h2>
<p>Based on the data, we assume the following structural model:</p>
<p><span class="math display">\[cdx_{ij} = Bernoulli(p_{ij})\]</span> <span class="math display">\[\begin{aligned}
logit(p_{ij}) &amp;= \beta_0 + \beta_{0i} + \beta_{ax}\text{ax}_i +\\
&amp;\beta_{noise_L}\text{noise}_i + \beta_{noise_Q}\text{noise}_i +\\
&amp;\beta_{no}\text{no}_i + \mathbf{\beta_{demo}X_{demo,i}} + \beta_{state} \text{state}_{ij}
\end{aligned}\]</span> <span class="math display">\[\begin{aligned}
\beta_{.} &amp;\sim Normal(0, 10) \\
\beta_{0i} &amp;\sim Normal(0, \sigma_i) \\
\sigma_i &amp;\sim HalfStudent(3, 0, 10)
\end{aligned}\]</span></p>
<p>The prior for the <span class="math inline">\(\sigma_i\)</span> parameter is the default used in <code>brms</code>; I see no need to change it.</p>
<p>Model fitting:</p>
<pre class="r"><code># The null model
chose_dx.model.null = brm(
  chose_dx ~ 0 + Intercept + (1 | user_id),
  family = &quot;bernoulli&quot;,
  prior = set_prior(&quot;normal(0, 10)&quot;, class = &quot;b&quot;),
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
chose_dx.model.null = add_criterion(chose_dx.model.null, &quot;waic&quot;)
chose_dx.model.null = add_criterion(chose_dx.model.null, &quot;loo&quot;, reloo = T)
saveModel(chose_dx.model.null)

# The trend model to see if there is a trend in the noise level variable
chose_dx.model.t = brm(
  chose_dx ~ 0 + Intercept + (has_ax + noise_level) + (age_group + robot_experience + gender) + num_optimal + state_idx_rescaled + (1 | user_id),
  family = bernoulli,
  prior = c(set_prior(&quot;normal(0, 10)&quot;, class = &quot;b&quot;),
            set_prior(&quot;student_t(3, 0, 10)&quot;, class = &quot;sd&quot;)),
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
chose_dx.model.t = add_criterion(chose_dx.model.t, &quot;waic&quot;)
chose_dx.model.t = add_criterion(chose_dx.model.t, &quot;loo&quot;, reloo = T)
saveModel(chose_dx.model.t)

# The values model, to see if specific values of the noise level variable are significant
chose_dx.model.v = brm(
  chose_dx ~ 0 + Intercept + (has_ax + noise_level_f) + (age_group + robot_experience + gender) + num_optimal + state_idx_rescaled + (1 | user_id),
  family = bernoulli,
  prior = c(set_prior(&quot;normal(0, 10)&quot;, class = &quot;b&quot;),
            set_prior(&quot;student_t(3, 0, 10)&quot;, class = &quot;sd&quot;)),
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
chose_dx.model.v = add_criterion(chose_dx.model.v, &quot;waic&quot;)
chose_dx.model.v = add_criterion(chose_dx.model.v, &quot;loo&quot;, reloo = T)
saveModel(chose_dx.model.v)</code></pre>
<pre class="r"><code># Load the models
chose_dx.model.null = loadModel(&quot;chose_dx.model.null&quot;)
chose_dx.model.t = loadModel(&quot;chose_dx.model.t&quot;)
chose_dx.model.v = loadModel(&quot;chose_dx.model.v&quot;)</code></pre>
<p>Model fitting results:</p>
<pre class="r"><code>print(performance::r2(chose_dx.model.null))</code></pre>
<pre><code>## # Bayesian R2 with Standard Error
##
##   Conditional R2: 0.182 [0.019]
##      Marginal R2: 0.000 [0.000]</code></pre>
<pre class="r"><code># Print the parameters, and some initial diagnostics
print(model_parameters(chose_dx.model.t,
                       centrality = &quot;median&quot;,
                       ci = 0.89,
                       ci_method = &quot;hdi&quot;,
                       test = c(&quot;pd&quot;, &quot;rope&quot;),
                       rope_range = c(-0.055, 0.055),
                       rope_ci = 1,
                       effects = &quot;fixed&quot;))</code></pre>
<pre><code>## # Fixed effects
##
## Parameter          | Median |         89% CI |     pd | % in ROPE |  ESS |  Rhat
## --------------------------------------------------------------------------------
## (Intercept)        |   1.21 | [ 0.45,  1.93] | 99.48% |     0.27% | 1093 | 1.001
## has_ax1            |  -0.01 | [-0.22,  0.21] | 53.05% |    31.25% | 1212 | 1.004
## noise_level.L      |  -0.77 | [-1.11, -0.39] | 99.95% |     0.07% | 1439 | 1.003
## noise_level.Q      |   0.13 | [-0.22,  0.49] | 71.28% |    16.98% | 1140 | 1.000
## age_group1         |   0.34 | [-0.32,  1.02] | 78.65% |     7.40% | 1536 | 1.001
## age_group2         |  -0.43 | [-0.84,  0.01] | 94.27% |     4.80% | 1429 | 1.001
## age_group3         |  -0.31 | [-0.73,  0.09] | 89.25% |     8.38% | 1623 | 1.000
## age_group4         |   0.17 | [-0.33,  0.68] | 71.08% |    11.28% | 1392 | 1.002
## age_group5         |  -0.12 | [-0.72,  0.46] | 63.50% |    11.53% | 1345 | 1.002
## age_group6         |   0.59 | [-0.12,  1.35] | 90.58% |     3.95% | 1379 | 1.000
## robot_experience1  |   0.18 | [-0.20,  0.57] | 77.48% |    13.75% | 1054 | 1.006
## robot_experience2  |   0.02 | [-0.52,  0.51] | 52.30% |    13.43% | 1300 | 1.004
## robot_experience3  |  -0.42 | [-1.16,  0.27] | 82.03% |     5.70% | 1235 | 1.006
## robot_experience4  |   0.63 | [-0.12,  1.41] | 90.28% |     4.20% | 1255 | 1.007
## gender1            |   0.10 | [-0.13,  0.32] | 75.40% |    24.10% | 1457 | 1.000
## num_optimal        |  -0.03 | [-0.16,  0.11] | 63.12% |    47.45% | 1166 | 1.001
## state_idx_rescaled |   0.25 | [-0.11,  0.59] | 88.22% |     9.75% | 3764 | 1.000</code></pre>
<pre class="r"><code>print(performance::r2(chose_dx.model.t))</code></pre>
<pre><code>## # Bayesian R2 with Standard Error
##
##   Conditional R2: 0.200 [0.018]
##      Marginal R2: 0.084 [0.023]</code></pre>
<pre class="r"><code>print(model_parameters(chose_dx.model.v,
                       centrality = &quot;median&quot;,
                       ci = 0.89,
                       ci_method = &quot;hdi&quot;,
                       test = c(&quot;pd&quot;, &quot;rope&quot;),
                       rope_range = c(-0.055, 0.055),
                       rope_ci = 1,
                       effects = &quot;fixed&quot;))</code></pre>
<pre><code>## # Fixed effects
##
## Parameter          | Median |         89% CI |     pd | % in ROPE |  ESS |  Rhat
## --------------------------------------------------------------------------------
## (Intercept)        |   1.20 | [ 0.47,  1.89] | 99.70% |     0.20% | 1210 | 1.001
## has_ax1            |  -0.01 | [-0.21,  0.20] | 52.80% |    32.88% | 1495 | 1.006
## noise_level_f1     |   0.58 | [ 0.31,  0.89] | 99.92% |     0.20% | 1525 | 1.002
## noise_level_f2     |  -0.10 | [-0.39,  0.19] | 70.85% |    20.97% | 1508 | 1.001
## age_group1         |   0.33 | [-0.37,  1.00] | 77.18% |     7.20% | 1541 | 1.001
## age_group2         |  -0.43 | [-0.87, -0.03] | 94.88% |     4.55% | 1492 | 1.001
## age_group3         |  -0.32 | [-0.75,  0.11] | 87.60% |     8.20% | 1459 | 1.000
## age_group4         |   0.16 | [-0.35,  0.64] | 69.15% |    11.68% | 1477 | 1.002
## age_group5         |  -0.11 | [-0.71,  0.49] | 62.10% |    11.55% | 1490 | 1.000
## age_group6         |   0.62 | [-0.10,  1.35] | 91.22% |     3.62% | 1404 | 1.002
## robot_experience1  |   0.18 | [-0.21,  0.53] | 77.38% |    13.10% | 1459 | 1.000
## robot_experience2  |   0.02 | [-0.49,  0.53] | 52.38% |    13.38% | 1233 | 1.003
## robot_experience3  |  -0.43 | [-1.10,  0.37] | 81.45% |     6.90% | 1011 | 1.007
## robot_experience4  |   0.62 | [-0.14,  1.38] | 90.77% |     4.30% | 1234 | 1.001
## gender1            |   0.10 | [-0.13,  0.32] | 76.90% |    23.60% | 1342 | 1.000
## num_optimal        |  -0.03 | [-0.16,  0.11] | 63.20% |    47.25% | 1189 | 1.003
## state_idx_rescaled |   0.27 | [-0.07,  0.66] | 87.67% |    10.15% | 3012 | 1.000</code></pre>
<pre class="r"><code>print(performance::r2(chose_dx.model.v))</code></pre>
<pre><code>## # Bayesian R2 with Standard Error
##
##   Conditional R2: 0.199 [0.018]
##      Marginal R2: 0.082 [0.022]</code></pre>
</div>
<div id="diagnostic-4" class="section level2">
<h2>Diagnostic</h2>
<pre class="r"><code># Alternative diagnostics: diagnostic_posterior(scenario_completed.model.v)

# LOO cross-validation results
print(chose_dx.model.t$criteria$loo$estimates)</code></pre>
<pre><code>##            Estimate        SE
## elpd_loo -745.97989 18.072626
## p_loo      75.20302  2.877191
## looic    1491.95978 36.145251</code></pre>
<pre class="r"><code>print(chose_dx.model.v$criteria$loo$estimates)</code></pre>
<pre><code>##           Estimate       SE
## elpd_loo -746.4159 18.05051
## p_loo      75.5310  2.86636
## looic    1492.8318 36.10103</code></pre>
<pre class="r"><code>print(loo_compare(chose_dx.model.null, chose_dx.model.t, chose_dx.model.v))</code></pre>
<pre><code>##                     elpd_diff se_diff
## chose_dx.model.null  0.0       0.0
## chose_dx.model.t    -3.1       3.0
## chose_dx.model.v    -3.5       3.0</code></pre>
<pre class="r"><code># Diagnostics of the sampling process
par(mfrow=c(1,2))
plot(chose_dx.model.t$criteria$loo, main = &quot;Trend Model&quot;)
plot(chose_dx.model.v$criteria$loo, main = &quot;Values Model&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-54-1.png' %}" width="1440" /></p>
<pre class="r"><code># Include posterior predictive checks of the sampling (are sampling according
# to the data that we actually have?)
p0 = pp_check(chose_dx.model.null, type = &quot;bars&quot;, stat = &quot;median&quot;) + ggtitle(&quot;null model&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars&#39; by default.</code></pre>
<pre><code>## Warning: The following arguments were unrecognized and ignored: stat</code></pre>
<pre class="r"><code>p1 = pp_check(chose_dx.model.t, type = &quot;bars_grouped&quot;, group = &quot;has_ax&quot;, stat = &quot;median&quot;) + ggtitle(&quot;t:has_ax&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars_grouped&#39; by default.</code></pre>
<pre><code>## Warning: The following arguments were unrecognized and ignored: stat</code></pre>
<pre class="r"><code>p2 = pp_check(chose_dx.model.t, type = &quot;bars_grouped&quot;, group = &quot;noise_level&quot;, stat = &quot;median&quot;) + ggtitle(&quot;t:noise_level&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars_grouped&#39; by default.</code></pre>
<pre><code>## Warning: The following arguments were unrecognized and ignored: stat</code></pre>
<pre class="r"><code>grid.arrange(p1, p2, p0, nrow = 2, ncol = 2)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-54-2.png' %}" width="1440" /></p>
<pre class="r"><code>mcmc_intervals(as.matrix(chose_dx.model.t), regex_pars = &quot;b_|^shape&quot;, prob = 0.90, prob_outer = 0.95) +
  ggtitle(&quot;Trends Model&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-54-3.png' %}" width="1440" /></p>
<pre class="r"><code>p1 = pp_check(chose_dx.model.v, type = &quot;bars_grouped&quot;, group = &quot;has_ax&quot;, stat = &quot;median&quot;) + ggtitle(&quot;v:has_ax&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars_grouped&#39; by default.</code></pre>
<pre><code>## Warning: The following arguments were unrecognized and ignored: stat</code></pre>
<pre class="r"><code>p2 = pp_check(chose_dx.model.v, type = &quot;bars_grouped&quot;, group = &quot;noise_level_f&quot;, stat = &quot;median&quot;) + ggtitle(&quot;v:noise_level&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars_grouped&#39; by default.</code></pre>
<pre><code>## Warning: The following arguments were unrecognized and ignored: stat</code></pre>
<pre class="r"><code>grid.arrange(p1, p2, p0, nrow = 2, ncol = 2)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-54-4.png' %}" width="1440" /></p>
<pre class="r"><code>mcmc_intervals(as.matrix(chose_dx.model.v), regex_pars = &quot;b_|^shape&quot;, prob = 0.90, prob_outer = 0.95) +
  ggtitle(&quot;Values Model&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-54-5.png' %}" width="1440" /></p>
<pre class="r"><code># Plot predictions vs original
preds_p_df = chose_dx.model.null %&gt;% predict() %&gt;% as_tibble()
p1 =
  preds_p_df %&gt;%
    add_column(chose_dx = chose_dx.model.null$data$chose_dx) %&gt;%
    ggplot(aes(x = Estimate, y = chose_dx, color = chose_dx)) +
      geom_point() +
      labs(x = &quot;Predicted values&quot;) +
      ylim(0, 1) +
      xlim(0, 1) +
      ggtitle(&quot;Null Model&quot;)

preds_p_df = plot_df %&gt;%
  add_predicted_draws(chose_dx.model.null) %&gt;%
  group_by(chose_dx, .prediction) %&gt;%
  count()
p2 =
  preds_p_df %&gt;%
    ggplot(aes(x = chose_dx, y = .prediction, fill = n / sum(preds_p_df$n))) +
      geom_tile() +
      geom_text(aes(label = sprintf(&quot;%1.2f&quot;, n / sum(preds_p_df$n))), size = 10, vjust = 1) +
      scale_fill_gradient(limits = c(0, 1))

preds_p_df = chose_dx.model.t %&gt;% predict() %&gt;% as_tibble()
p3 =
  preds_p_df %&gt;%
    add_column(chose_dx = chose_dx.model.t$data$chose_dx) %&gt;%
    ggplot(aes(x = Estimate, y = chose_dx, color = chose_dx)) +
      geom_point() +
      labs(x = &quot;Predicted values&quot;) +
      ylim(0, 1) +
      xlim(0, 1) +
      ggtitle(&quot;Trends Model&quot;)

preds_p_df = plot_df %&gt;%
  add_predicted_draws(chose_dx.model.t) %&gt;%
  group_by(chose_dx, .prediction) %&gt;%
  count()
p4 =
  preds_p_df %&gt;%
    ggplot(aes(x = chose_dx, y = .prediction, fill = n / sum(preds_p_df$n))) +
      geom_tile() +
      geom_text(aes(label = sprintf(&quot;%1.2f&quot;, n / sum(preds_p_df$n))), size = 10, vjust = 1) +
      scale_fill_gradient(limits = c(0, 1))

# Plot predictions vs original
preds_p_df = chose_dx.model.v %&gt;% predict() %&gt;% as_tibble()
p5 =
  preds_p_df %&gt;%
    add_column(chose_dx = chose_dx.model.v$data$chose_dx) %&gt;%
    ggplot(aes(x = Estimate, y = chose_dx, color = chose_dx)) +
      geom_point() +
      labs(x = &quot;Predicted values&quot;) +
      ylim(0, 1) +
      xlim(0, 1) +
      ggtitle(&quot;Values Model&quot;)

preds_p_df = plot_df %&gt;%
  add_predicted_draws(chose_dx.model.v) %&gt;%
  group_by(chose_dx, .prediction) %&gt;%
  count()
p6 =
  preds_p_df %&gt;%
    ggplot(aes(x = chose_dx, y = .prediction, fill = n / sum(preds_p_df$n))) +
      geom_tile() +
      geom_text(aes(label = sprintf(&quot;%1.2f&quot;, n / sum(preds_p_df$n))), size = 10, vjust = 1) +
      scale_fill_gradient(limits = c(0, 1))

grid.arrange(p1, p3, p5, p2, p4, p6, nrow = 2, ncol = 3)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-54-6.png' %}" width="1440" /></p>
<p>We are able to recreate the data with the model; although no differently than from the null model.</p>
</div>
<div id="inference-4" class="section level2">
<h2>Inference</h2>
<pre class="r"><code># Compute the bayes factor for each parameter value. Note: we compare
# here against the ROPE, and not the degenerate null hypothesis of 0
# Bayes-Factor values would be more significant in the latter case
# According to the Raftery, 1995 rules
# bf = 1 - 3: Weak
# bf = 3 - 20: Positive
# bf = 20 - 150: Strong
# bf &gt; 150: Very strong
print(bayesfactor_rope(chose_dx.model.null, null = c(-0.055, 0.055)), n = 30)
print(bayesfactor_rope(chose_dx.model.t, null = c(-0.055, 0.055)), n = 30)
print(bayesfactor_rope(chose_dx.model.v, null = c(-0.055, 0.055)), n = 30)</code></pre>
<pre class="r"><code>options(digits = 3)
# Check the significance of each hypothesis
h_df = as_tibble(insight::get_parameters(chose_dx.model.t)) %&gt;%
  transmute(
    Intercept = b_Intercept,
    has_ax_test = -2 * b_has_ax1,
    noise_levelL_test = b_noise_level.L,
    noise_levelQ_test = b_noise_level.Q
  )

hyp_results = test_hypotheses(h_df, rope_values = c(-0.055, 0.055))
print(hyp_results %&gt;% as.matrix())</code></pre>
<pre><code>##      Parameter           Estimate  CI_low   CI_high  Effect_Size pd
## [1,] &quot;Intercept&quot;         &quot; 1.2099&quot; &quot; 0.446&quot; &quot; 1.935&quot; &quot;0.6670&quot;    &quot;0.995&quot;
## [2,] &quot;has_ax_test&quot;       &quot; 0.0198&quot; &quot;-0.427&quot; &quot; 0.443&quot; &quot;0.0109&quot;    &quot;0.530&quot;
## [3,] &quot;noise_levelL_test&quot; &quot;-0.7676&quot; &quot;-1.110&quot; &quot;-0.389&quot; &quot;0.4232&quot;    &quot;1.000&quot;
## [4,] &quot;noise_levelQ_test&quot; &quot; 0.1260&quot; &quot;-0.224&quot; &quot; 0.494&quot; &quot;0.0695&quot;    &quot;0.713&quot;
##      ROPE_Percentage ROPE_Equivalence
## [1,] &quot;0.00275&quot;       &quot;Rejected&quot;
## [2,] &quot;0.15900&quot;       &quot;Undecided&quot;
## [3,] &quot;0.00075&quot;       &quot;Rejected&quot;
## [4,] &quot;0.16975&quot;       &quot;Undecided&quot;</code></pre>
<pre class="r"><code>h_df = as_tibble(insight::get_parameters(chose_dx.model.v)) %&gt;%
  transmute(
    Intercept = b_Intercept,
    has_ax_test = -2 * b_has_ax1,
    noise_level1_test = b_noise_level_f1,
    noise_level2_test = b_noise_level_f2,
    noise_level3_test = - b_noise_level_f1 - b_noise_level_f2
  )

hyp_results = test_hypotheses(h_df, rope_values = c(-0.055, 0.055))
print(hyp_results %&gt;% as.matrix())</code></pre>
<pre><code>##      Parameter           Estimate  CI_low   CI_high  Effect_Size pd
## [1,] &quot;Intercept&quot;         &quot; 1.2041&quot; &quot; 0.475&quot; &quot; 1.888&quot; &quot;0.6638&quot;    &quot;0.997&quot;
## [2,] &quot;has_ax_test&quot;       &quot; 0.0187&quot; &quot;-0.402&quot; &quot; 0.429&quot; &quot;0.0103&quot;    &quot;0.528&quot;
## [3,] &quot;noise_level1_test&quot; &quot; 0.5798&quot; &quot; 0.311&quot; &quot; 0.887&quot; &quot;0.3197&quot;    &quot;0.999&quot;
## [4,] &quot;noise_level2_test&quot; &quot;-0.1016&quot; &quot;-0.386&quot; &quot; 0.191&quot; &quot;0.0560&quot;    &quot;0.709&quot;
## [5,] &quot;noise_level3_test&quot; &quot;-0.4805&quot; &quot;-0.758&quot; &quot;-0.172&quot; &quot;0.2649&quot;    &quot;0.996&quot;
##      ROPE_Percentage ROPE_Equivalence
## [1,] &quot;0.00200&quot;       &quot;Rejected&quot;
## [2,] &quot;0.16800&quot;       &quot;Undecided&quot;
## [3,] &quot;0.00200&quot;       &quot;Rejected&quot;
## [4,] &quot;0.20975&quot;       &quot;Undecided&quot;
## [5,] &quot;0.00825&quot;       &quot;Rejected&quot;</code></pre>
<pre class="r"><code>options(digits = 7)</code></pre>
<p><strong>Results</strong>:</p>
<ol style="list-style-type: decimal">
<li>Noise Level Linear
<ul>
<li>Negative (negative slope) pd = 100%</li>
<li>Median = -0.77, 89% CI [-1.11, -0.39]</li>
<li>Small, Effect Size = 0.42</li>
<li>0.08% in ROPE</li>
</ul></li>
</ol>
<p>(The ROPE is defined as [-0.055, 0.055]. It is the range is suggested in the literature for logistic models and it corresponds to a probability range of 0.11. Therefore, a probability change of less than 0.055 is considered no different from a probability change of 0)</p>
</div>
<div id="posterior-plots-4" class="section level2">
<h2>Posterior Plots</h2>
<pre class="r"><code># A visualization data frame
gg_df = plot_df %&gt;%
    mutate(noise_level = fct_rev(noise_level)) %&gt;%
    mutate(chose_dx = factor(chose_dx)) %&gt;%
    mutate(user_id = factor(user_id)) %&gt;%
    mutate(chose_dx = fct_recode(chose_dx, no_follow=&quot;0&quot;, follow = &quot;1&quot;)) %&gt;%
    count(user_id, suggestion_type, chose_dx, num_actions, has_ax, noise_level,
          name = &quot;num_follow_dx&quot;)

# Simulate predictions on new data
p_df =
  plot_df %&gt;%
  data_grid(chose_dx,
            has_ax,
            noise_level_f,
            gender,
            age_group,
            robot_experience,
            num_optimal,
            state_idx_rescaled = seq_range(state_idx_rescaled, n = 4),
            .model = chose_dx.model.v)

fits_p_df = p_df %&gt;%
  add_fitted_draws(chose_dx.model.v, re_formula = NA, n = 100, seed = default_seed) %&gt;%
  mutate(suggestion_type = ifelse(has_ax == F, &quot;DX&quot;, &quot;DXAX&quot;)) %&gt;%
  mutate(suggestion_type = factor(suggestion_type, levels = c(&quot;DX&quot;, &quot;DXAX&quot;)))
gc()

pars_p_df = chose_dx.model.t %&gt;%
  extract_draws(newdata = p_df %&gt;% rename(noise_level = noise_level_f), re_formula = NA, nsamples = 100)
gc()

# Plot the posterior distributions. In each of the following, the reference ROPE band
# can be marked based on 3 methods. Make sure to update that ROPE value baased on the
# method that we want to display.

# If centering ROPE on the posterior distributions. Here we center the ROPE around the posterior
# predicted value for the DX condition
preds_p_df = p_df %&gt;%
  add_predicted_draws(chose_dx.model.v, re_formula = NA)  %&gt;% # 0, 1 predictions for each predicted value
  ungroup() %&gt;%
  count(has_ax, noise_level_f, .prediction) %&gt;%
  mutate(suggestion_type = ifelse(has_ax == F, &quot;DX&quot;, &quot;DXAX&quot;)) %&gt;%
  mutate(suggestion_type = factor(suggestion_type, levels = c(&quot;DX&quot;, &quot;DXAX&quot;)))
rope_value = median(
  preds_p_df %&gt;%
    filter(suggestion_type == &quot;DX&quot;) %&gt;%
    spread(.prediction, n) %&gt;%
    mutate(.value = `1` / (`0` + `1`)) %&gt;%
    pull(.value)
)
gc()

gg_df %&gt;%
  filter(chose_dx == &#39;follow&#39;) %&gt;%
  ggplot(aes(x = has_ax, y = num_follow_dx / num_actions)) +
    geom_rect(ymin = rope_value-0.055, ymax = rope_value+0.055,
              xmin = -Inf, xmax = Inf,
              color = &quot;transparent&quot;, fill = &quot;white&quot;) +
    annotate(&quot;text&quot;, x = 0.5, y = rope_value, label = &quot;ROPE&quot;, angle = 90) +
    stat_halfeye(aes(y = .value), data = fits_p_df, position = position_nudge(x = 0.1)) +
    geom_point(aes(color = paste(has_ax, noise_level, sep = &quot;:&quot;)),
               size = 2, position = position_jitter(width = .1, height = 0)) +
    stat_summary(fun = median, geom = &quot;point&quot;, shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(y = &quot;CDX&quot;, color = &quot;HasAX:Noise&quot;)

gg_df %&gt;%
  filter(chose_dx == &#39;follow&#39;) %&gt;%
  mutate(noise_level = fct_rev(noise_level)) %&gt;%
  ggplot(aes(x = noise_level, y = num_follow_dx / num_actions)) +
    geom_rect(ymin = rope_value-0.055, ymax = rope_value+0.055,
              xmin = -Inf, xmax = Inf,
              color = &quot;transparent&quot;, fill = &quot;white&quot;) +
    annotate(&quot;text&quot;, x = 0.5, y = rope_value, label = &quot;ROPE&quot;, angle = 90) +
    stat_halfeye(aes(y = .value, x = noise_level_f), data = fits_p_df, position = position_nudge(x = 0.1)) +
    geom_point(aes(color = paste(has_ax, noise_level, sep = &quot;:&quot;)),
               size = 2, position = position_jitter(width = .1, height = 0)) +
    stat_summary(fun = median, geom = &quot;point&quot;, shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(y = &quot;CDX&quot;, color = &quot;HasAX:Noise&quot;)

# Create a plot to show the trend
pars_p_df_X = pars_p_df$dpars$mu$fe$X[,c(&quot;Intercept&quot;, &quot;noise_level.L&quot;)]
pars_p_df_b = pars_p_df$dpars$mu$fe$b[,c(&quot;b_Intercept&quot;, &quot;b_noise_level.L&quot;)]
pars_p_df_y = pars_p_df_X %*% t(pars_p_df_b)
pars_p_df_y = inv_logit_scaled(pars_p_df_y)
p_df =p_df %&gt;%
  bind_cols(as_tibble(pars_p_df_y)) %&gt;%
  gather(key = &quot;.sample&quot;, value = &quot;.value&quot;, V1:V100) %&gt;%
  rename(noise_level = noise_level_f)
gg_df %&gt;%
  filter(chose_dx == &#39;follow&#39;) %&gt;%
  mutate(noise_level = fct_rev(noise_level)) %&gt;%
  ggplot(aes(x = noise_level, y = num_follow_dx / num_actions)) +
    geom_rect(ymin = rope_value-0.055, ymax = rope_value+0.055,
              xmin = -Inf, xmax = Inf,
              color = &quot;transparent&quot;, fill = &quot;white&quot;) +
    annotate(&quot;text&quot;, x = 0.5, y = rope_value, label = &quot;ROPE&quot;, angle = 90) +
    stat_halfeye(aes(y = .value, x = noise_level_f), data = fits_p_df, position = position_nudge(x = 0.1)) +
    geom_line(aes(y = .value, group = .sample, alpha = 0.3), data = p_df) +
    geom_point(aes(color = paste(has_ax, noise_level, sep = &quot;:&quot;)),
               size = 2, position = position_jitter(width = .1, height = 0)) +
    stat_summary(fun = median, geom = &quot;point&quot;, shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(y = &quot;CDX&quot;, color = &quot;HasAX:Noise&quot;) +
    guides(alpha = F)

# Remove the giant fits data frames
rm(preds_p_df) # If using the posterior-predictions-based ROPE
rm(fits_p_df, pars_p_df, pars_p_df_b, pars_p_df_X, pars_p_df_y, p_df, gg_df)</code></pre>
<pre class="r"><code>eff = fixef(chose_dx.model.t)
dx_eff = inv_logit_scaled(
  eff[&quot;Intercept&quot;, &quot;Estimate&quot;]
  + eff[&quot;has_ax1&quot;, &quot;Estimate&quot;]
)
dxax_eff = inv_logit_scaled(
  eff[&quot;Intercept&quot;, &quot;Estimate&quot;]
  - eff[&quot;has_ax1&quot;, &quot;Estimate&quot;]
)

gg_df = plot_df %&gt;%
  mutate(chose_dx = factor(chose_dx)) %&gt;%
  mutate(chose_dx = fct_recode(chose_dx, no_follow=&quot;0&quot;, follow=&quot;1&quot;),
         has_ax = fct_recode(has_ax, &quot;No AX&quot;=&quot;FALSE&quot;, &quot;AX&quot;=&quot;TRUE&quot;),
         noise_level = fct_recode(noise_level, &quot;Acc: 100%&quot;=&quot;0.0&quot;, &quot;Acc: 90%&quot;=&quot;1.0&quot;, &quot;Acc: 80%&quot;=&quot;2.0&quot;)) %&gt;%
  count(user_id, suggestion_type, chose_dx, num_actions, has_ax, noise_level,
        name = &quot;num_follow_dx&quot;) %&gt;%
  mutate(estimate = ifelse(suggestion_type == &quot;DX&quot;, dx_eff, -1)) %&gt;%
  mutate(estimate = ifelse(suggestion_type == &quot;DXAX&quot;, dxax_eff, estimate))

p1 = gg_df %&gt;%
  filter(chose_dx == &quot;follow&quot;) %&gt;%
  ggplot(aes(suggestion_type, num_follow_dx / num_actions, group = suggestion_type, colour=suggestion_type)) +
    geom_count(aes(colour = NULL)) +
    geom_boxplot(aes(y = estimate)) +
    scale_y_continuous(limits = c(0.0, 1.2), breaks = c(0.0, 0.5, 1.0)) +
    labs(y = NULL, x = NULL) +
    scale_colour_manual(values = c(&quot;#e51400&quot;, &quot;#008a00&quot;, &quot;#f0a30a&quot;, &quot;#a4c400&quot;)) +
    theme_economist_white(base_size = 13) +
    theme(plot.background = element_rect(fill = &quot;white&quot;),
          axis.title.y = element_blank()) +
    legend_none()

eff = fixef(chose_dx.model.v)
noise_0_eff = inv_logit_scaled(eff[&quot;Intercept&quot;, &quot;Estimate&quot;] + eff[&quot;noise_level_f1&quot;,&quot;Estimate&quot;])
noise_1_eff = inv_logit_scaled(eff[&quot;Intercept&quot;, &quot;Estimate&quot;] + eff[&quot;noise_level_f2&quot;,&quot;Estimate&quot;])
noise_2_eff = inv_logit_scaled(eff[&quot;Intercept&quot;, &quot;Estimate&quot;] - eff[&quot;noise_level_f1&quot;,&quot;Estimate&quot;] - eff[&quot;noise_level_f2&quot;,&quot;Estimate&quot;])

gg_df = gg_df %&gt;%
  mutate(estimate = if_else(noise_level == &quot;Acc: 100%&quot;, noise_0_eff, estimate)) %&gt;%
  mutate(estimate = if_else(noise_level == &quot;Acc: 90%&quot;, noise_1_eff, estimate)) %&gt;%
  mutate(estimate = if_else(noise_level == &quot;Acc: 80%&quot;, noise_2_eff, estimate)) %&gt;%
  mutate(noise_level = fct_rev(noise_level))

p2 = gg_df %&gt;%
  filter(chose_dx == &quot;follow&quot;) %&gt;%
  ggplot(aes(noise_level, num_follow_dx / num_actions, group = noise_level, colour = noise_level)) +
    geom_hline(yintercept = inv_logit_scaled(eff[&quot;Intercept&quot;, &quot;Estimate&quot;]),
               size = .7, linetype = &quot;dashed&quot;, color = &quot;grey&quot;) +
    geom_count(aes(colour = NULL)) +
    geom_boxplot(aes(y = estimate)) +
    annotate(&quot;segment&quot;,
             x = 3, y = noise_0_eff,
             xend = 2, yend = noise_1_eff,
             colour = &quot;black&quot;) +
    annotate(&quot;segment&quot;,
             x = 1, y = noise_2_eff,
             xend = 2, yend = noise_1_eff,
             color = &quot;black&quot;) +
    annotate(&quot;text&quot;,
             x = 3.5,
             y = inv_logit_scaled(eff[&quot;Intercept&quot;, &quot;Estimate&quot;] - 0.35),
             label = &quot;Mean of all levels&quot;,
             angle = 90,
             group = NA,
             colour = &quot;black&quot;) +
    scale_y_continuous(limits = c(0.0, 1.0), breaks = c(0.0, 0.5, 1.0)) +
    labs(y = &quot;RDX&quot;, x = NULL) +
    scale_colour_manual(values = c(&quot;#e51400&quot;, &quot;#008a00&quot;, &quot;#f0a30a&quot;, &quot;#a4c400&quot;)) +
    scale_size_area(max_size = 1) +
    theme_economist_white(base_size = 13) +
    theme(plot.background = element_rect(fill = &quot;white&quot;),
          axis.title.y = element_blank(),
          axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
    legend_none()

grid.arrange(p1, p2, nrow = 1, ncol = 2)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-58-1.png' %}" width="672" /></p>
<pre class="r"><code>rm(p1, p2)</code></pre>
</div>
</div>
<div id="sus-system-usability-scale" class="section level1">
<h1>SUS: System Usability Scale</h1>
<pre class="r"><code>plot_df = users %&gt;%
  select(X1, id, study_condition, start_condition, num_optimal,
         age_group, gender, robot_experience,
         noise_level, noise_level_f, has_noise, suggestion_type,
         has_dx, has_ax, has_ax_only, has_dx_only, has_dxax,
         sus, scenario_completed) %&gt;%
  mutate(scenario_completed = factor(scenario_completed))

print(summary(plot_df %&gt;%
                mutate(scenario_completed = factor(scenario_completed)) %&gt;%
                select(-X1, -id)
))</code></pre>
<pre><code>##  study_condition start_condition     num_optimal   age_group gender
##  DX_100  :20     Length:200         Min.   :3.00   2:21      F: 73
##  AX_100  :20     Class :character   1st Qu.:3.75   3:47      M:127
##  DXAX_100:20     Mode  :character   Median :4.00   4:44
##  DX_90   :20                        Mean   :4.50   5:33
##  AX_90   :20                        3rd Qu.:4.75   6:19
##  DXAX_90 :20                        Max.   :7.00   7:16
##  (Other) :80                                       8:20
##  robot_experience noise_level noise_level_f has_noise   suggestion_type
##  0:133            0.0:80      0.0:80        FALSE: 80   NONE:20
##  1: 33            1.0:60      1.0:60        TRUE :120   AX  :60
##  2: 12            2.0:60      2.0:60                    DX  :60
##  3: 12                                                  DXAX:60
##  4: 10
##
##
##    has_dx      has_ax    has_ax_only has_dx_only  has_dxax        sus
##  FALSE: 80   FALSE: 80   FALSE:140   FALSE:140   FALSE:140   Min.   :  0.00
##  TRUE :120   TRUE :120   TRUE : 60   TRUE : 60   TRUE : 60   1st Qu.: 55.00
##                                                              Median : 72.50
##                                                              Mean   : 67.05
##                                                              3rd Qu.: 82.50
##                                                              Max.   :100.00
##
##  scenario_completed
##  0: 38
##  1:162
##
##
##
##
## </code></pre>
<pre class="r"><code>print(as.matrix(
  plot_df %&gt;%
    group_by(study_condition) %&gt;%
    summarise(.value = median(sus))
))</code></pre>
<pre><code>##       study_condition .value
##  [1,] &quot;DX_100&quot;        &quot;73.75&quot;
##  [2,] &quot;AX_100&quot;        &quot;72.50&quot;
##  [3,] &quot;DXAX_100&quot;      &quot;67.50&quot;
##  [4,] &quot;DX_90&quot;         &quot;66.25&quot;
##  [5,] &quot;AX_90&quot;         &quot;75.00&quot;
##  [6,] &quot;DXAX_90&quot;       &quot;76.25&quot;
##  [7,] &quot;DX_80&quot;         &quot;70.00&quot;
##  [8,] &quot;AX_80&quot;         &quot;71.25&quot;
##  [9,] &quot;DXAX_80&quot;       &quot;73.75&quot;
## [10,] &quot;BASELINE&quot;      &quot;67.50&quot;</code></pre>
<div id="data-5" class="section level2">
<h2>Data</h2>
<pre class="r"><code># Plot by the study condition
plot_df %&gt;%
  ggplot(aes(study_condition, sus, fill = suggestion_type)) +
    geom_boxplot() +
    geom_count() +
    labs(y = &quot;Number unnecessary actions&quot;) +
    scale_fill_economist()</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-61-1.png' %}" width="1440" /></p>
<pre class="r"><code># Plot by suggestion type
plot_df %&gt;%
  mutate(noise_level = fct_rev(noise_level)) %&gt;%
  ggplot(aes(suggestion_type, sus, fill=suggestion_type)) +
    geom_boxplot() +
    geom_count() +
    facet_grid(cols = vars(noise_level), labeller = label_both) +
    labs(y = &quot;Number unnecessary actions&quot;) +
    scale_fill_economist()</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-61-2.png' %}" width="1440" /></p>
<pre class="r"><code>plot_df %&gt;%
  mutate(noise_level = fct_rev(noise_level)) %&gt;%
  ggplot(aes(noise_level, sus, fill=suggestion_type)) +
    geom_boxplot() +
    geom_count() +
    facet_grid(cols = vars(suggestion_type), labeller = label_both) +
    labs(y = &quot;Number unnecessary actions&quot;) +
    scale_fill_economist()</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-61-3.png' %}" width="1440" /></p>
<pre class="r"><code># Plot distribution by suggestion type
p1 = plot_df %&gt;%
  ggplot(aes(sus)) +
    geom_histogram(binwidth = 10) +
    facet_grid(rows = vars(suggestion_type)) +
    ylim(0, 20) +
    scale_fill_economist() +
    legend_none()

p2 = plot_df %&gt;%
  ggplot(aes(sus)) +
    geom_histogram(binwidth = 10) +
    facet_grid(rows = vars(noise_level)) +
    ylim(0, 20) +
    scale_fill_economist() +
    theme(legend.position = &quot;right&quot;)

grid.arrange(p1, p2, ncol = 2)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-61-4.png' %}" width="1440" /></p>
</div>
<div id="model-5" class="section level2">
<h2>Model</h2>
<p>Based on the data, we assume a skew-normal linear model:</p>
<p><span class="math display">\[SUS_i = SkewNormal(\mu_i, \sigma, \alpha)\]</span> <span class="math display">\[\begin{aligned}
\mu_i &amp;= \beta_0 + \beta_{ax}\text{ax_only}_i + \beta_{dx}\text{dx_only}_i + \beta_{dxax}\text{dxax}_i +\\ &amp;\beta_{noise_L}\text{noise}_i + \beta_{noise_Q}\text{noise}_i +\\
&amp;\beta_{no}\text{no}_i + \mathbf{\beta_{demo}X_{demo,i}}
\end{aligned}\]</span> <span class="math display">\[\begin{aligned}
\beta_{.} &amp;\sim Normal(0, 10) \\
\sigma &amp;\sim HalfStudent(3, 0, 22) \\
\alpha &amp;\sim Normal(0, 4)
\end{aligned}\]</span></p>
<p>The prior for the <span class="math inline">\(\sigma, \alpha\)</span> parameters are the default used in <code>brms</code>; I see no need to change it.</p>
<p>Model fitting:</p>
<pre class="r"><code># A null model to compare against
sus.model.null = brm(
  sus ~ 0 + Intercept,
  family = &quot;skew_normal&quot;,
  prior = c(set_prior(&quot;normal(0, 10)&quot;, class = &quot;b&quot;),
            set_prior(&quot;student_t(3, 0, 22)&quot;, class = &quot;sigma&quot;),
            set_prior(&quot;normal(0, 4)&quot;, class = &quot;alpha&quot;)),
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
sus.model.null = add_criterion(sus.model.null, &quot;waic&quot;)
sus.model.null = add_criterion(sus.model.null, &quot;loo&quot;, reloo = T)
saveModel(sus.model.null)

# The trend model to see if there is a trend in the noise level variable
sus.model.t = brm(
  sus ~ 0 + Intercept + (suggestion_type + noise_level) + (gender + age_group + robot_experience) + num_optimal,
  family = &quot;skew_normal&quot;,
  prior = c(set_prior(&quot;normal(0, 10)&quot;, class = &quot;b&quot;),
            set_prior(&quot;student_t(3, 0, 22)&quot;, class = &quot;sigma&quot;),
            set_prior(&quot;normal(0, 4)&quot;, class = &quot;alpha&quot;)),
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
sus.model.t = add_criterion(sus.model.t, &quot;waic&quot;)
sus.model.t = add_criterion(sus.model.t, &quot;loo&quot;, reloo = T)
saveModel(sus.model.t)

# The values model, to see if specific values of the noise level variable are significant
sus.model.v = brm(
  sus ~ 0 + Intercept + (suggestion_type + noise_level_f) + (gender + age_group + robot_experience) + num_optimal,
  family = &quot;skew_normal&quot;,
  prior = c(set_prior(&quot;normal(0, 10)&quot;, class = &quot;b&quot;),
            set_prior(&quot;student_t(3, 0, 22)&quot;, class = &quot;sigma&quot;),
            set_prior(&quot;normal(0, 4)&quot;, class = &quot;alpha&quot;)),
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
sus.model.v = add_criterion(sus.model.v, &quot;waic&quot;)
sus.model.v = add_criterion(sus.model.v, &quot;loo&quot;, reloo = T)
saveModel(sus.model.v)</code></pre>
<pre class="r"><code># Load the models
sus.model.null = loadModel(&quot;sus.model.null&quot;)
sus.model.t = loadModel(&quot;sus.model.t&quot;)
sus.model.v = loadModel(&quot;sus.model.v&quot;)</code></pre>
<p>Model fitting results:</p>
<pre class="r"><code>print(performance::r2(sus.model.null))</code></pre>
<pre><code>## # Bayesian R2 with Standard Error
##
##   Conditional R2: 0.000 [0.000]</code></pre>
<pre class="r"><code># Print the parameters, and some initial diagnostics
print(model_parameters(sus.model.t,
                       centrality = &quot;median&quot;,
                       ci = 0.89,
                       ci_method = &quot;hdi&quot;,
                       test = c(&quot;pd&quot;, &quot;rope&quot;),
                       rope_range = c(-0.055, 0.055),
                       rope_ci = 1,
                       effects = &quot;fixed&quot;))</code></pre>
<pre><code>## # Fixed effects
##
## Parameter         | Median |          89% CI |     pd | % in ROPE |  ESS |  Rhat
## --------------------------------------------------------------------------------
## (Intercept)       |  56.05 | [ 48.55, 62.65] |   100% |        0% | 2897 | 1.000
## suggestion_type1  |   5.84 | [  1.91,  9.50] | 98.88% |     0.10% | 4580 | 1.000
## suggestion_type2  |   0.75 | [ -0.99,  2.48] | 76.02% |     3.28% | 3997 | 1.000
## suggestion_type3  |   0.17 | [ -1.10,  1.40] | 58.75% |     5.50% | 4435 | 0.999
## noise_level.L     |  -1.31 | [ -5.00,  2.28] | 70.73% |     1.80% | 4918 | 0.999
## noise_level.Q     |   0.87 | [ -2.94,  4.37] | 64.88% |     1.82% | 5121 | 1.000
## gender1           |   2.04 | [ -0.00,  4.24] | 93.80% |     0.95% | 4988 | 1.000
## age_group1        |  -2.67 | [ -7.87,  2.56] | 79.07% |     1.15% | 5313 | 0.999
## age_group2        |   4.53 | [  0.44,  8.77] | 96.60% |     0.33% | 4885 | 1.000
## age_group3        |   3.71 | [ -0.56,  7.54] | 92.62% |     0.60% | 4676 | 1.000
## age_group4        |   2.31 | [ -2.39,  7.29] | 78.47% |     0.90% | 4653 | 0.999
## age_group5        |   0.20 | [ -5.01,  5.87] | 52.42% |     1.43% | 4912 | 0.999
## age_group6        |   0.05 | [ -5.51,  6.10] | 50.58% |     1.00% | 4718 | 0.999
## robot_experience1 |   3.42 | [ -0.39,  7.03] | 92.85% |     0.60% | 4344 | 1.000
## robot_experience2 |   4.36 | [ -0.68,  9.18] | 91.90% |     0.50% | 4266 | 1.000
## robot_experience3 |   1.31 | [ -4.78,  8.95] | 62.75% |     1.03% | 4009 | 1.000
## robot_experience4 |  -9.71 | [-16.41, -2.85] | 97.97% |     0.12% | 3982 | 0.999
## num_optimal       |   1.02 | [ -0.18,  2.53] | 90.28% |     2.65% | 3378 | 1.000</code></pre>
<pre class="r"><code>print(performance::r2(sus.model.t))</code></pre>
<pre><code>## # Bayesian R2 with Standard Error
##
##   Conditional R2: 0.104 [0.025]</code></pre>
<pre class="r"><code>print(model_parameters(sus.model.v,
                       centrality = &quot;median&quot;,
                       ci = 0.89,
                       ci_method = &quot;hdi&quot;,
                       test = c(&quot;pd&quot;, &quot;rope&quot;),
                       rope_range = c(-0.055, 0.055),
                       rope_ci = 1,
                       effects = &quot;fixed&quot;))</code></pre>
<pre><code>## # Fixed effects
##
## Parameter         | Median |          89% CI |     pd | % in ROPE |  ESS |  Rhat
## --------------------------------------------------------------------------------
## (Intercept)       |  55.94 | [ 49.30, 63.33] |   100% |        0% | 2835 | 1.000
## suggestion_type1  |   5.86 | [  2.03,  9.71] | 98.92% |     0.07% | 4087 | 1.000
## suggestion_type2  |   0.78 | [ -1.04,  2.41] | 76.12% |     2.90% | 4754 | 0.999
## suggestion_type3  |   0.20 | [ -1.04,  1.36] | 60.45% |     5.90% | 4300 | 1.000
## noise_level_f1    |   1.31 | [ -1.39,  4.34] | 76.38% |     1.68% | 3843 | 1.000
## noise_level_f2    |  -0.73 | [ -3.56,  1.96] | 65.80% |     2.00% | 4239 | 1.000
## gender1           |   2.11 | [ -0.13,  4.14] | 93.23% |     1.27% | 4436 | 0.999
## age_group1        |  -2.75 | [ -8.00,  2.36] | 79.62% |     0.95% | 4259 | 0.999
## age_group2        |   4.64 | [  0.68,  8.64] | 96.65% |     0.33% | 4745 | 1.000
## age_group3        |   3.65 | [ -0.85,  7.60] | 91.97% |     0.88% | 5113 | 0.999
## age_group4        |   2.37 | [ -2.37,  7.26] | 78.60% |     1.18% | 4315 | 1.000
## age_group5        |   0.23 | [ -5.17,  5.57] | 52.83% |     1.40% | 4634 | 1.000
## age_group6        |   0.09 | [ -5.05,  5.93] | 50.85% |     1.23% | 4106 | 1.000
## robot_experience1 |   3.49 | [ -0.26,  6.86] | 94.35% |     0.65% | 3735 | 0.999
## robot_experience2 |   4.32 | [ -0.40,  9.21] | 93.23% |     0.55% | 4413 | 1.000
## robot_experience3 |   1.26 | [ -5.41,  8.14] | 62.52% |     1.03% | 4132 | 1.000
## robot_experience4 |  -9.72 | [-16.08, -3.10] | 98.17% |     0.07% | 3728 | 1.001
## num_optimal       |   1.05 | [ -0.35,  2.36] | 89.80% |     2.97% | 3026 | 1.000</code></pre>
<pre class="r"><code>print(performance::r2(sus.model.v))</code></pre>
<pre><code>## # Bayesian R2 with Standard Error
##
##   Conditional R2: 0.103 [0.026]</code></pre>
</div>
<div id="diagnostic-5" class="section level2">
<h2>Diagnostic</h2>
<pre class="r"><code># Alternative diagnostics: diagnostic_posterior(scenario_completed.model.v)

# LOO cross-validation results
print(sus.model.t$criteria$loo$estimates)</code></pre>
<pre><code>##            Estimate        SE
## elpd_loo -900.05983  9.814403
## p_loo      16.61808  2.743520
## looic    1800.11967 19.628806</code></pre>
<pre class="r"><code>print(sus.model.v$criteria$loo$estimates)</code></pre>
<pre><code>##            Estimate        SE
## elpd_loo -900.26335  9.966228
## p_loo      16.89399  3.058219
## looic    1800.52670 19.932456</code></pre>
<pre class="r"><code>print(loo_compare(sus.model.null,
                  sus.model.t,
                  sus.model.v))</code></pre>
<pre><code>##                elpd_diff se_diff
## sus.model.null  0.0       0.0
## sus.model.t    -8.9       4.6
## sus.model.v    -9.1       4.8</code></pre>
<pre class="r"><code># Diagnostics of the sampling process
par(mfrow=c(1,2))
plot(sus.model.t$criteria$loo, main = &quot;Trend Model&quot;)
plot(sus.model.v$criteria$loo, main = &quot;Values Model&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-65-1.png' %}" width="1440" /></p>
<pre class="r"><code># Include posterior predictive checks of the sampling (are sampling according
# to the data that we actually have?)
p0 = pp_check(sus.model.null, stat = &quot;median&quot;) + ggtitle(&quot;null model&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;dens_overlay&#39; by default.</code></pre>
<pre><code>## Warning: The following arguments were unrecognized and ignored: stat</code></pre>
<pre class="r"><code>p1 = pp_check(sus.model.t, group = &quot;suggestion_type&quot;, stat = &quot;median&quot;) + ggtitle(&quot;t:suggestion_type&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;dens_overlay&#39; by default.</code></pre>
<pre><code>## Warning: The following arguments were unrecognized and ignored: stat, group</code></pre>
<pre class="r"><code>p2 = pp_check(sus.model.t, group = &quot;noise_level&quot;, stat = &quot;median&quot;) + ggtitle(&quot;t:noise_level&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;dens_overlay&#39; by default.</code></pre>
<pre><code>## Warning: The following arguments were unrecognized and ignored: stat, group</code></pre>
<pre class="r"><code>grid.arrange(p1, p2, p0, nrow = 2, ncol = 2)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-65-2.png' %}" width="1440" /></p>
<pre class="r"><code>mcmc_intervals(as.matrix(sus.model.t), regex_pars = &quot;b_&quot;, prob = 0.90, prob_outer = 0.95) +
  ggtitle(&quot;Trends Model&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-65-3.png' %}" width="1440" /></p>
<pre class="r"><code>p1 = pp_check(sus.model.v, group = &quot;suggestion_type&quot;, stat = &quot;median&quot;) + ggtitle(&quot;v:suggestion_type&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;dens_overlay&#39; by default.</code></pre>
<pre><code>## Warning: The following arguments were unrecognized and ignored: stat, group</code></pre>
<pre class="r"><code>p2 = pp_check(sus.model.v, group = &quot;noise_level_f&quot;) + ggtitle(&quot;v:noise_level&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;dens_overlay&#39; by default.</code></pre>
<pre><code>## Warning: The following arguments were unrecognized and ignored: group</code></pre>
<pre class="r"><code>grid.arrange(p1, p2, p0, nrow = 2, ncol = 2)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-65-4.png' %}" width="1440" /></p>
<pre class="r"><code>mcmc_intervals(as.matrix(sus.model.v), regex_pars = &quot;b_&quot;, prob = 0.90, prob_outer = 0.95) +
  ggtitle(&quot;Values Model&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-65-5.png' %}" width="1440" /></p>
<pre class="r"><code># Plot predictions vs original
preds_p_df = sus.model.null %&gt;% predict() %&gt;% as_tibble()
p1 =
  preds_p_df %&gt;%
    add_column(sus = sus.model.null$data$sus) %&gt;%
    ggplot(aes(x = sus, y = Estimate)) +
      geom_point() +
      labs(y = &quot;Predicted values&quot;) +
      ggtitle(&quot;Null Model&quot;) +
      lims(x = c(0, 100), y = c(0, 100))

preds_p_df = sus.model.t %&gt;% predict() %&gt;% as_tibble()
p3 =
  preds_p_df %&gt;%
    add_column(sus = sus.model.t$data$sus) %&gt;%
    ggplot(aes(x = sus, y = Estimate)) +
      geom_point() +
      labs(y = &quot;Predicted values&quot;) +
      ggtitle(&quot;Trends Model&quot;) +
      lims(x = c(0, 100), y = c(0, 100))

preds_p_df = sus.model.v %&gt;% predict() %&gt;% as_tibble()
p5 =
  preds_p_df %&gt;%
    add_column(sus = sus.model.v$data$sus) %&gt;%
    ggplot(aes(x = sus, y = Estimate)) +
      geom_point() +
      labs(y = &quot;Predicted values&quot;) +
      ggtitle(&quot;Values Model&quot;) +
      lims(x = c(0, 100), y = c(0, 100))

grid.arrange(p1, p3, p5, nrow = 1, ncol = 3)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-65-6.png' %}" width="1440" /></p>
<pre class="r"><code>rm(p0, p1, p2, p3, p5)</code></pre>
<p>We don’t model the data perfectly, but it is a better trend than the null model.</p>
</div>
<div id="inference-5" class="section level2">
<h2>Inference</h2>
<pre class="r"><code># Compute the bayes factor for each parameter value. Note: we compare
# here against the ROPE, and not the degenerate null hypothesis of 0
# Bayes-Factor values would be more significant in the latter case
# According to the Raftery, 1995 rules
# bf = 1 - 3: Weak
# bf = 3 - 20: Positive
# bf = 20 - 150: Strong
# bf &gt; 150: Very strong
print(bayesfactor_rope(sus.model.null), n = 30)
print(bayesfactor_rope(sus.model.t), n = 30)
print(bayesfactor_rope(sus.model.v), n = 30)</code></pre>
<pre class="r"><code>options(digits = 3)
# Check the significance of each hypothesis
h_df = as_tibble(insight::get_parameters(sus.model.t)) %&gt;%
  transmute(
    Intercept = b_Intercept,
    has_ax_test = 2 * b_suggestion_type1,
    has_dx_test = (3 * b_suggestion_type2) + b_suggestion_type1,
    has_dxax_test = (4 * b_suggestion_type3) + b_suggestion_type1 + b_suggestion_type2,
    noise_levelL_test = b_noise_level.L,
    noise_levelQ_test = b_noise_level.Q
  )

hyp_results = test_hypotheses(h_df, rope_values = c(-2.3, 2.3), log_odds_effects = F)
print(hyp_results %&gt;% as.matrix())</code></pre>
<pre><code>##      Parameter           Estimate CI_low   CI_high Effect_Size pd
## [1,] &quot;Intercept&quot;         &quot;56.054&quot; &quot;48.548&quot; &quot;62.65&quot; &quot;2.4639&quot;    &quot;1.000&quot;
## [2,] &quot;has_ax_test&quot;       &quot;11.675&quot; &quot; 3.821&quot; &quot;19.00&quot; &quot;0.5551&quot;    &quot;0.989&quot;
## [3,] &quot;has_dx_test&quot;       &quot; 8.160&quot; &quot; 1.169&quot; &quot;16.04&quot; &quot;0.3825&quot;    &quot;0.952&quot;
## [4,] &quot;has_dxax_test&quot;     &quot; 7.258&quot; &quot;-0.403&quot; &quot;15.20&quot; &quot;0.3546&quot;    &quot;0.928&quot;
## [5,] &quot;noise_levelL_test&quot; &quot;-1.309&quot; &quot;-5.000&quot; &quot; 2.28&quot; &quot;0.0307&quot;    &quot;0.707&quot;
## [6,] &quot;noise_levelQ_test&quot; &quot; 0.866&quot; &quot;-2.944&quot; &quot; 4.37&quot; &quot;0.0189&quot;    &quot;0.649&quot;
##      ROPE_Percentage ROPE_Equivalence
## [1,] &quot;0.0000&quot;        &quot;Rejected&quot;
## [2,] &quot;0.0302&quot;        &quot;Undecided&quot;
## [3,] &quot;0.0948&quot;        &quot;Undecided&quot;
## [4,] &quot;0.1253&quot;        &quot;Undecided&quot;
## [5,] &quot;0.6032&quot;        &quot;Undecided&quot;
## [6,] &quot;0.6613&quot;        &quot;Undecided&quot;</code></pre>
<pre class="r"><code>h_df = as_tibble(insight::get_parameters(sus.model.v)) %&gt;%
  transmute(
    Intercept = b_Intercept,
    has_ax_test = 2 * b_suggestion_type1,
    has_dx_test = (3 * b_suggestion_type2) + b_suggestion_type1,
    has_dxax_test = (4 * b_suggestion_type3) + b_suggestion_type1 + b_suggestion_type2,
    noise_level1_test = b_noise_level_f1,
    noise_level2_test = b_noise_level_f2,
    noise_level3_test = - b_noise_level_f1 - b_noise_level_f2
  )

hyp_results = test_hypotheses(h_df, rope_values = c(-2.3, 2.3), log_odds_effects = F)
print(hyp_results %&gt;% as.matrix())</code></pre>
<pre><code>##      Parameter           Estimate CI_low   CI_high Effect_Size pd
## [1,] &quot;Intercept&quot;         &quot;55.938&quot; &quot;49.297&quot; &quot;63.33&quot; &quot;2.4791&quot;    &quot;1.000&quot;
## [2,] &quot;has_ax_test&quot;       &quot;11.723&quot; &quot; 4.056&quot; &quot;19.42&quot; &quot;0.5513&quot;    &quot;0.989&quot;
## [3,] &quot;has_dx_test&quot;       &quot; 8.253&quot; &quot; 0.247&quot; &quot;15.16&quot; &quot;0.3826&quot;    &quot;0.954&quot;
## [4,] &quot;has_dxax_test&quot;     &quot; 7.443&quot; &quot;-0.490&quot; &quot;14.81&quot; &quot;0.3483&quot;    &quot;0.935&quot;
## [5,] &quot;noise_level1_test&quot; &quot; 1.315&quot; &quot;-1.393&quot; &quot; 4.34&quot; &quot;0.0239&quot;    &quot;0.764&quot;
## [6,] &quot;noise_level2_test&quot; &quot;-0.731&quot; &quot;-3.559&quot; &quot; 1.96&quot; &quot;0.0129&quot;    &quot;0.658&quot;
## [7,] &quot;noise_level3_test&quot; &quot;-0.658&quot; &quot;-3.583&quot; &quot; 2.25&quot; &quot;0.0120&quot;    &quot;0.637&quot;
##      ROPE_Percentage ROPE_Equivalence
## [1,] &quot;0.000&quot;         &quot;Rejected&quot;
## [2,] &quot;0.028&quot;         &quot;Undecided&quot;
## [3,] &quot;0.094&quot;         &quot;Undecided&quot;
## [4,] &quot;0.120&quot;         &quot;Undecided&quot;
## [5,] &quot;0.686&quot;         &quot;Undecided&quot;
## [6,] &quot;0.769&quot;         &quot;Undecided&quot;
## [7,] &quot;0.766&quot;         &quot;Undecided&quot;</code></pre>
<pre class="r"><code>options(digits = 7)</code></pre>
<p><strong>Results</strong>:</p>
<p>Effects exist, but are non-significant given the ROPE:</p>
<ol style="list-style-type: decimal">
<li>AX suggestions
<ul>
<li>Positive effect pd = 98.9%</li>
<li>Median = 11.68, 89% CI [3.82, 19.0]</li>
<li>Medium, Effect Size = 0.56</li>
<li>3.02% in ROPE (n.s.)</li>
</ul></li>
<li>DX suggestions
<ul>
<li>Positive effect pd = 95.2%</li>
<li>Median = 8.16, 89% CI [1.17, 16.04]</li>
<li>Small, Effect Size = 0.38</li>
<li>9.48% in ROPE (n.s.)</li>
</ul></li>
</ol>
<p>(The ROPE is defined as [-2.3, 2.3]. It corresponds to <code>0.1 * SD</code> of the output)</p>
</div>
<div id="posterior-plots-5" class="section level2">
<h2>Posterior Plots</h2>
<pre class="r"><code>eff = fixef(sus.model.t)
base_eff = (
  eff[&quot;Intercept&quot;, &quot;Estimate&quot;]
  - eff[&quot;suggestion_type1&quot;, &quot;Estimate&quot;]
  - eff[&quot;suggestion_type2&quot;, &quot;Estimate&quot;]
  - eff[&quot;suggestion_type3&quot;, &quot;Estimate&quot;]
)
ax_eff = (
  eff[&quot;Intercept&quot;, &quot;Estimate&quot;]
  + eff[&quot;suggestion_type1&quot;, &quot;Estimate&quot;]
  - eff[&quot;suggestion_type2&quot;, &quot;Estimate&quot;]
  - eff[&quot;suggestion_type3&quot;, &quot;Estimate&quot;]
)
dx_eff = (
  eff[&quot;Intercept&quot;, &quot;Estimate&quot;]
  + (2 * eff[&quot;suggestion_type2&quot;, &quot;Estimate&quot;])
  - eff[&quot;suggestion_type3&quot;, &quot;Estimate&quot;]
)
dxax_eff = (
  eff[&quot;Intercept&quot;, &quot;Estimate&quot;]
  + (3 * eff[&quot;suggestion_type3&quot;, &quot;Estimate&quot;])
)

gg_df = plot_df %&gt;%
  mutate(has_ax_only = fct_recode(has_ax_only, &quot;No AX&quot;=&quot;FALSE&quot;, &quot;AX&quot;=&quot;TRUE&quot;),
         has_dx_only = fct_recode(has_dx_only, &quot;No DX&quot;=&quot;FALSE&quot;, &quot;DX&quot;=&quot;TRUE&quot;),
         has_dxax = fct_recode(has_dxax, &quot;No DXAX&quot;=&quot;FALSE&quot;, &quot;DXAX&quot;=&quot;TRUE&quot;),
         noise_level = fct_recode(noise_level, &quot;Acc: 100%&quot;=&quot;0.0&quot;, &quot;Acc: 90%&quot;=&quot;1.0&quot;, &quot;Acc: 80%&quot;=&quot;2.0&quot;)) %&gt;%
  mutate(estimate = ifelse(suggestion_type == &quot;NONE&quot;, base_eff, -1)) %&gt;%
  mutate(estimate = ifelse(suggestion_type == &quot;AX&quot;, ax_eff, estimate)) %&gt;%
  mutate(estimate = ifelse(suggestion_type == &quot;DX&quot;, dx_eff, estimate)) %&gt;%
  mutate(estimate = ifelse(suggestion_type == &quot;DXAX&quot;, dxax_eff, estimate))

p1 = gg_df %&gt;%
  ggplot(aes(suggestion_type, sus, group = suggestion_type, colour=suggestion_type)) +
    geom_count(aes(colour = NULL)) +
    geom_boxplot(aes(y = estimate)) +
    # scale_y_continuous(limits = c(0.0, 1.2), breaks = c(0.0, 0.5, 1.0)) +
    labs(y = NULL, x = NULL) +
    scale_colour_manual(values = c(&quot;#e51400&quot;, &quot;#008a00&quot;, &quot;#f0a30a&quot;, &quot;#a4c400&quot;)) +
    theme_economist_white(base_size = 13) +
    theme(plot.background = element_rect(fill = &quot;white&quot;),
          axis.title.y = element_blank()) +
    legend_none()

eff = fixef(sus.model.v)
noise_0_eff = (eff[&quot;Intercept&quot;, &quot;Estimate&quot;] + eff[&quot;noise_level_f1&quot;,&quot;Estimate&quot;])
noise_1_eff = (eff[&quot;Intercept&quot;, &quot;Estimate&quot;] + eff[&quot;noise_level_f2&quot;,&quot;Estimate&quot;])
noise_2_eff = (eff[&quot;Intercept&quot;, &quot;Estimate&quot;] - eff[&quot;noise_level_f1&quot;,&quot;Estimate&quot;] - eff[&quot;noise_level_f2&quot;,&quot;Estimate&quot;])

gg_df = gg_df %&gt;%
  mutate(estimate = if_else(noise_level == &quot;Acc: 100%&quot;, noise_0_eff, estimate)) %&gt;%
  mutate(estimate = if_else(noise_level == &quot;Acc: 90%&quot;, noise_1_eff, estimate)) %&gt;%
  mutate(estimate = if_else(noise_level == &quot;Acc: 80%&quot;, noise_2_eff, estimate)) %&gt;%
  mutate(noise_level = fct_rev(noise_level))

p2 = gg_df %&gt;%
  ggplot(aes(noise_level, sus, group = noise_level, colour = noise_level)) +
    geom_hline(yintercept = (eff[&quot;Intercept&quot;, &quot;Estimate&quot;]),
               size = .7, linetype = &quot;dashed&quot;, color = &quot;grey&quot;) +
    geom_count(aes(colour = NULL)) +
    geom_boxplot(aes(y = estimate)) +
    annotate(&quot;segment&quot;,
             x = 3, y = noise_0_eff,
             xend = 2, yend = noise_1_eff,
             colour = &quot;black&quot;) +
    annotate(&quot;segment&quot;,
             x = 1, y = noise_2_eff,
             xend = 2, yend = noise_1_eff,
             color = &quot;black&quot;) +
    annotate(&quot;text&quot;,
             x = 3.5,
             y = (eff[&quot;Intercept&quot;, &quot;Estimate&quot;]),
             label = &quot;Mean of all levels&quot;,
             angle = 90,
             group = NA,
             colour = &quot;black&quot;) +
    labs(y = &quot;Fraction of participants that resolved fault&quot;, x = NULL) +
    scale_colour_manual(values = c(&quot;#e51400&quot;, &quot;#008a00&quot;, &quot;#f0a30a&quot;, &quot;#a4c400&quot;)) +
    scale_size_area(max_size = 1) +
    theme_economist_white(base_size = 13) +
    theme(plot.background = element_rect(fill = &quot;white&quot;),
          axis.title.y = element_blank(),
          axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
    legend_none()

grid.arrange(p1, p2, nrow = 1, ncol = 2)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-68-1.png' %}" width="672" /></p>
<pre class="r"><code>rm(p1, p2)</code></pre>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->
<script>
$(document).ready(function () {
  window.initializeCodeFolding("hide" === "show");
});
</script>

<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3,h4",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
