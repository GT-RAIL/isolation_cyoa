{% load static %}

<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>RO-MAN 2020</title>

<script src="{% static 'roman2020_libs/jquery-1.11.3/jquery.min.js' %}"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="{% static 'roman2020_libs/bootstrap-3.3.5/css/bootstrap.min.css' %}" rel="stylesheet" />
<script src="{% static 'roman2020_libs/bootstrap-3.3.5/js/bootstrap.min.js' %}"></script>
<script src="{% static 'roman2020_libs/bootstrap-3.3.5/shim/html5shiv.min.js' %}"></script>
<script src="{% static 'roman2020_libs/bootstrap-3.3.5/shim/respond.min.js' %}"></script>
<script src="{% static 'roman2020_libs/jqueryui-1.11.4/jquery-ui.min.js' %}"></script>
<link href="{% static 'roman2020_libs/tocify-1.9.1/jquery.tocify.css' %}" rel="stylesheet" />
<script src="{% static 'roman2020_libs/tocify-1.9.1/jquery.tocify.js' %}"></script>
<script src="{% static 'roman2020_libs/navigation-1.1/tabsets.js' %}"></script>
<script src="{% static 'roman2020_libs/navigation-1.1/codefolding.js' %}"></script>
<link href="{% static 'roman2020_libs/highlightjs-9.12.0/default.css' %}" rel="stylesheet" />
<script src="{% static 'roman2020_libs/highlightjs-9.12.0/highlight.js' %}"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>



<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->
<style type="text/css">
.code-folding-btn { margin-bottom: 4px; }
</style>



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="fluid-row" id="header">

<div class="btn-group pull-right">
<button type="button" class="btn btn-default btn-xs dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"><span>Code</span> <span class="caret"></span></button>
<ul class="dropdown-menu" style="min-width: 50px;">
<li><a id="rmd-show-all-code" href="#">Show All Code</a></li>
<li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li>
</ul>
</div>



<h1 class="title toc-ignore">RO-MAN 2020</h1>

</div>


<pre class="r"><code>library(car)
library(stats)
library(grid)
library(gridExtra)
library(fitdistrplus)
# library(dplyr)
# library(forcats)
# library(tibble)
# library(tidyr)
# library(readr)
# library(ggplot2)
# library(stringr)
library(tidyverse)
library(ggsignif)
library(ggthemes)
library(jtools)
library(broom)
library(modelr)
library(loo)
# library(rstanarm)
library(brms)
library(bayesplot)
library(GGally)
library(tidybayes)
library(bayestestR)
library(sjstats)
library(report)
library(see)

# Setup for multiprocessing
options(mc.cores = 4)
# library(future)
# plan(multiprocess)

# Make the default coding of contrasts sum-coding; just in case
options(contrasts = c(&quot;contr.sum&quot;, &quot;contr.poly&quot;))</code></pre>
<p>Global options:</p>
<pre class="r"><code># Script execution globals
train_models = F     # Retrain the MCMC models. Also infer unknown demographics
plot_posteriors = T  # Plot the posterior distributions
plot_diagnostics = T # Plot the diagnostics
report_bayesfactors = F  # Report the Bayes Factor scores of model parameters
default_seed = 0x1331 # For repeatable models and diagnostics
data_folder = &quot;~/Documents/GT/Research/Data/arbitration/2019-12-09/results&quot;</code></pre>
<p>Helper functions:</p>
<pre class="r"><code># Create a simple coding contrasts matrix
contr.deviation = function(nlevels) {
  x1 = contr.treatment(nlevels)
  x2 = matrix(rep(1/nlevels, (nlevels-1) * nlevels), ncol = (nlevels-1))
  return(x1-x2)
}

# NOTE: For now we&#39;re ignoring this advice
# # Use a simple coding contrasts matrix to add our own contrasts columns
# # Explained in: http://talklab.psy.gla.ac.uk/tvw/catpred/
# create_simple_contrasts = function(d, column_name) {
#  factor_column = d %&gt;% pull(column_name)
#  contrast_matrix = contr.deviation(length(levels(factor_column)))
# }

# Save a model
saveModel = function(model, folder = data_folder) {
  saveRDS(model, file = file.path(folder, paste(substitute(model), &quot;.rds&quot;, sep = &#39;&#39;)))
}

# Load a model
loadModel = function(model_name, folder = data_folder) {
  return(readRDS(file.path(folder, paste(model_name, &#39;.rds&#39;, sep = &#39;&#39;))))
}

# Test hypotheses. The NULL arguments need to be provided in this case
test_hypotheses = function(h_df = NULL, rope_values = NULL, hypotheses_list = NULL, model = NULL) {
  # Option 1: Use the hypothesis function in brms
  if (!is.null(hypotheses_list)) {
    hyp_results = hypothesis(model, hypothesis = hypotheses_list, seed = default_seed)
    return(hyp_results)
  }

  # Option 2: Use the ROPE &amp; Overlap amount
  else {
    if (is.null(rope_values)) {
      rope_values = c(-0.1, 0.1)
    }
    hyp_results =
      h_df %&gt;%
        equivalence_test(range = rope_values, ci = 1.0) %&gt;%
        as_tibble() %&gt;%
        bind_cols(h_df %&gt;% pd() %&gt;% select(pd))
    return(hyp_results)
  }
}

# Fit distributions to vectors and print the results. Also return the fit, if necessary
fit_and_print_dist = function(vec, distr, string, use_mass = F) {
  if (!use_mass) {
    f = fitdist(vec, distr)
  } else {
    f = MASS::fitdistr(vec, distr)
  }
  print(paste(string, f$loglik, f$aic))
  return(f)
}</code></pre>
<p>Data loading:</p>
<pre class="r"><code># Load the CSV files. If the age_group fill model is run again and we get a different output,
# then remember to update the value here
loadCSV = function(filename, age_group_fill, contrast_func) {
  dat = read_csv(
    file.path(data_folder, filename),
    col_types = cols(
      study_condition = col_factor(),
      noise_level = col_factor(ordered = T),
      gender = col_factor(levels = c(&quot;F&quot;, &quot;M&quot;, &quot;U&quot;)),
      age_group = col_factor(levels = seq(from = 0, to = 8)),
      robot_experience = col_factor(levels = seq(from = 0, to = 4))
    )
  )

  # Relabel the factors
  dat = dat %&gt;%
    mutate(study_condition = fct_recode(study_condition,
                                        BASELINE=&quot;1&quot;,
                                        DX_100=&quot;2&quot;, AX_100=&quot;3&quot;, DXAX_100=&quot;4&quot;,
                                        DX_90=&quot;5&quot;, AX_90=&quot;6&quot;, DXAX_90=&quot;7&quot;,
                                        DX_80=&quot;8&quot;, AX_80=&quot;9&quot;, DXAX_80=&quot;10&quot;)) %&gt;%
    mutate(study_condition = fct_relevel(study_condition, c(&quot;DX_100&quot;, &quot;AX_100&quot;, &quot;DXAX_100&quot;,
                                                            &quot;DX_90&quot;, &quot;AX_90&quot;, &quot;DXAX_90&quot;,
                                                            &quot;DX_80&quot;, &quot;AX_80&quot;, &quot;DXAX_80&quot;)))

  # Relevel the non-binary gender
  dat$gender[dat$gender == &#39;U&#39;] = &#39;M&#39;

  # Change binary responses to integers
  dat$scenario_completed = as.integer(dat$scenario_completed)

  # Relevel age_group
  dat[dat$age_group == 0,]$age_group = age_group_fill

  # Create an unordered noise_level. Also drop unused levels
  dat$age_group = droplevels(dat$age_group)
  dat$gender = droplevels(dat$gender)
  dat$noise_level_f = factor(dat$noise_level, ordered = F)

  # Set the contrasts for everything
  contrasts(dat$age_group) = contrast_func(length(levels(dat$age_group)))
  contrasts(dat$robot_experience) = contrast_func(length(levels(dat$robot_experience)))
  contrasts(dat$gender) = contrast_func(length(levels(dat$gender)))

  contrasts(dat$has_ax) = contrast_func(2)
  contrasts(dat$has_dx) = contrast_func(2)
  contrasts(dat$has_noise) = contrast_func(2)
  contrasts(dat$has_dxax) = contrast_func(2)

  contrasts(dat$noise_level_f) = contrast_func(length(levels(dat$noise_level_f)))

  # Return the data frame
  return(dat)
}

# Get the users df and the actions df
users = loadCSV(&quot;users.csv&quot;, 3, contr.sum)
actions = loadCSV(&quot;actions.csv&quot;, 3, contr.sum)

# Change more binary responses to integers
actions$optimal_ax = as.integer(actions$optimal_ax)
actions$chose_ax = as.integer(actions$chose_ax)
actions$optimal_dx = as.integer(actions$optimal_dx)
actions$chose_dx = as.integer(actions$optimal_dx)

# Relabel user ids to be in the range 1-200. Otherwise, we&#39;re using DB ids
# The user ID column in the users table that we can now join on is X1
actions$user_id = actions %&gt;% group_indices(user_id)

# Rescale state ids
actions$state_idx_rescaled = scales::rescale(actions$state_idx)

# Code to relevel the age group factor to remove unused levels. Also, predict the
# value of the 1 unknown age from the other indicators of demographics
# NOTE: If we retrain the model and the output doesn&#39;t match the hardcoded
# values, then update those values
if (train_models) {
  plot_df = subset(users, users$age_group != 0)
  plot_df$age_group = factor(plot_df$age_group, levels = seq(from = 0, to = 8), ordered = T)
  plot_df$age_group = droplevels(plot_df$age_group)
  age_group_model = brm(age_group ~ gender + robot_experience, data = plot_df, family = &quot;cumulative&quot;)
  data_to_predict = users %&gt;% filter(age_group == 0) %&gt;% select(c(&quot;robot_experience&quot;, &quot;gender&quot;))
  as_tibble(predict(age_group_model, data_to_predict))
# # A tibble: 1 x 7
#   `P(Y = 2)` `P(Y = 3)` `P(Y = 4)` `P(Y = 5)` `P(Y = 6)` `P(Y = 7)` `P(Y = 8)`
#        &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;
# 1     0.0707      0.230      0.218      0.174      0.105       0.09      0.113
  rm(age_group_model)
}</code></pre>
<p>Note that in this notebook, the accuracy variable from the paper is coded as a noise level variable:</p>
<table>
<thead>
<tr class="header">
<th>Accuracy</th>
<th>Noise Level</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>100%</td>
<td>0</td>
</tr>
<tr class="even">
<td>90%</td>
<td>1</td>
</tr>
<tr class="odd">
<td>80%</td>
<td>2</td>
</tr>
</tbody>
</table>
<p>Since noise level increases as accuracy decreases, the “sign” of any linear trends observed in the following analyses should be reversed. Everything else stays pretty much the same.</p>
<p>For each of our dependent variables, there are 2 models:</p>
<ol style="list-style-type: decimal">
<li>With the noise variable as an ordered factor, so that we can make inferences on the trends in the variable</li>
<li>With the noise variable as an unordered factor, so that we can make inferences on the values of the variable</li>
</ol>
<p>We use Deviation / Sum coding in the coding of non-ordered factors so that we can test the influence of individual factor levels over and above the grand mean.</p>
<p>In all our models:</p>
<ul>
<li><span class="math inline">\(\beta_0\)</span> is assumed to be the intercept. In a mixed effects model, this is additionally indexed by <span class="math inline">\(i\)</span>, the user; i.e. <span class="math inline">\(\beta_{0i}\)</span>.</li>
<li><span class="math inline">\(\text{ax}_i\)</span> denotes if sample <span class="math inline">\(i\)</span> received AX suggestions or not</li>
<li><span class="math inline">\(\text{dx}_i\)</span> denotes if sample <span class="math inline">\(i\)</span> received DX suggestions or not</li>
<li><span class="math inline">\(\text{noise}_i\)</span> denotes the level of noise in the suggestions that sample <span class="math inline">\(i\)</span> received. 0 (100% accuracy) if none was present.</li>
<li><span class="math inline">\(\mathbf{X_{demo,i}}\)</span> is a vector of demographic information. For one participant, this information is imputed from a simple linear model of the other participants. Think of it almost as Propensity Score matching.</li>
<li><span class="math inline">\(\text{no}_i\)</span> denotes the number of optimal actions for the scenario present in sample <span class="math inline">\(i\)</span>. This is a proxy for a factor encoding of the start scenario</li>
<li><span class="math inline">\(\text{state}_{ij}\)</span> denotes the state the user <span class="math inline">\(i\)</span> visited on action number <span class="math inline">\(j\)</span>. The sample, in this case, is indexed by <span class="math inline">\(j\)</span>. The states are indexed according to the frequency of users visits (0 = most visited state), and then all the indices are rescaled into the range 0-1.</li>
</ul>
<p>We test the following hypotheses (the explanations are a statement of the null hypotheses; the coefficients are from the expected regression parameters, given sum coding):</p>
<ul>
<li><span class="math inline">\((\beta_0 - \beta_{ax_0}) - (\beta_0 + \beta_{ax_0}) = -2\beta_{ax_0} = 0 \Rightarrow \beta_{ax_0} = 0\)</span>: The main difference in effects from having action suggestions vs. not is not negligible (ceterus paribus)</li>
<li><span class="math inline">\((\beta_0 - \beta_{dx_0}) - (\beta_0 + \beta_{dx_0}) = -2\beta_{dx_0} = 0 \Rightarrow \beta_{dx_0} = 0\)</span>: The main difference in effects from having diagnosis suggestions vs. not is not neglible (ceterus paribus)</li>
<li><span class="math inline">\(\beta_{noise_L} = 0; \beta_{noise_Q} = 0\)</span>: Noise does not have a linear / quadratic effect on the outcome.</li>
</ul>
<p>We do not test the interaction effects (because it’s hard to make sense of what those mean).</p>
<p>The method of reporting and testing is based on the following papers:</p>
<ol style="list-style-type: decimal">
<li><a href="https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.12577">A protocol for conducting and presenting results of regression‐type analyses</a></li>
<li><a href="https://www.frontiersin.org/articles/10.3389/fpsyg.2019.02767/full">Indices of Effect Existence and Significance in the Bayesian Framework</a>. The paper is associated with <a href="https://easystats.github.io/bayestestR/articles/guidelines.html">this post</a> on how to present results, and <a href="https://easystats.github.io/bayestestR/articles/region_of_practical_equivalence.html">this post</a> giving a quick overview of terms (the posts are part of a package I’m using heavily in these analyses)</li>
</ol>
<p>Note, that unlike the previous version of this HTML page, and some of the references, we are NOT going to perform model-selection here. Based on what I’ve read, we’re doing confirmatory hypothesis testing, which is not where one should use model selection paradigms.</p>
<div id="frr-fault-resolution-rate" class="section level1">
<h1>FRR: Fault Resolution Rate</h1>
<p><strong>Did the person complete the scenario or not?</strong></p>
<p>In the code, this variable is called <code>scenario_completed</code>.</p>
<pre class="r"><code>plot_df = users %&gt;%
  select(X1, id, study_condition, start_condition, num_optimal,
         age_group, gender, robot_experience,
         noise_level, noise_level_f, has_noise, has_dx, has_ax,
         scenario_completed)
text_short(report(plot_df))</code></pre>
<pre><code>## Warning in report.numeric(model[[col]], median = median, centrality =
## centrality, : Variable `model[[col]]` contains only two different values.
## Consider converting it to a factor.</code></pre>
<pre><code>## The data contains 200 observations of the following variables:
##   - X1: Mean = 99.50, SD = 57.88, range = [0, 199], 0 missing
##   - id: Mean = 339.73, SD = 157.51, range = [75, 590], 0 missing
##   - study_condition: 10 levels: DX_100, n = 20; AX_100, n = 20; DXAX_100, n = 20; DX_90, n = 20; AX_90, n = 20; DXAX_90, n = 20; DX_80, n = 20; AX_80, n = 20; DXAX_80, n = 20 and BASELINE, n = 20
##   - start_condition: 4 entries: dt.kc.default.default.default.empty.kc, n = 50; kc.dt.default.default.default.empty.dt, n = 50; kc.dt.occluding.above_mug.default.empty.dt, n = 50 and 1 other
##   - num_optimal: Mean = 4.50, SD = 1.50, range = [3, 7], 0 missing
##   - age_group: 7 levels: 2, n = 21; 3, n = 49; 4, n = 42; 5, n = 33; 6, n = 19; 7, n = 16 and 8, n = 20
##   - gender: 2 levels: F, n = 73 and M, n = 127
##   - robot_experience: 5 levels: 0, n = 135; 1, n = 32; 2, n = 12; 3, n = 12 and 4, n = 9
##   - noise_level: 3 levels: 0.0, n = 80; 1.0, n = 60 and 2.0, n = 60
##   - noise_level_f: 3 levels: 0.0, n = 80; 1.0, n = 60 and 2.0, n = 60
##   - has_noise: 2 levels: FALSE, n = 80 and TRUE, n = 120
##   - has_dx: 2 levels: FALSE, n = 80 and TRUE, n = 120
##   - has_ax: 2 levels: FALSE, n = 80 and TRUE, n = 120
##   - scenario_completed: Mean = 0.81, SD = 0.40, range = [0, 1], 0 missing</code></pre>
<div id="data" class="section level2">
<h2>Data</h2>
<pre class="r"><code># Plot by the study condition
plot_df %&gt;%
  mutate(scenario_completed = factor(scenario_completed)) %&gt;%
  mutate(scenario_completed = fct_recode(scenario_completed, incomplete=&quot;0&quot;, complete = &quot;1&quot;)) %&gt;%
  count(study_condition, scenario_completed) %&gt;%
  ggplot(aes(study_condition, n / 20, fill=scenario_completed)) +
    geom_bar(stat=&quot;identity&quot;) +
    labs(y = &quot;Fraction completed&quot;) +
    scale_fill_economist()</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-6-1.png' %}" width="1440" /></p>
<pre class="r"><code># Visualize the data by the three variables that we care about
gg_df = plot_df %&gt;%
  mutate(scenario_completed = factor(scenario_completed)) %&gt;%
  mutate(scenario_completed = fct_recode(scenario_completed, incomplete=&quot;0&quot;, complete = &quot;1&quot;)) %&gt;%
  mutate(noise_level = fct_rev(noise_level)) %&gt;%
  count(has_ax, has_dx, noise_level, scenario_completed)

p1 = gg_df %&gt;%
  ggplot(aes(has_ax, n / 20, fill=scenario_completed)) +
    geom_bar(stat = &quot;identity&quot;) +
    facet_grid(rows = vars(noise_level), cols = vars(has_dx), labeller = label_both) +
    labs(y = &quot;Fraction completed&quot;) +
    legend_none() +
    scale_fill_economist()

p2 = gg_df %&gt;%
  ggplot(aes(has_dx, n / 20, fill=scenario_completed)) +
    geom_bar(stat = &quot;identity&quot;) +
    facet_grid(rows = vars(noise_level), cols = vars(has_ax), labeller = label_both) +
    labs(y = &quot;Fraction completed&quot;) +
    theme(legend.position = &quot;bottom&quot;) +
    scale_fill_economist()

p3 = gg_df %&gt;%
  mutate(noise_level = fct_rev(noise_level)) %&gt;%
  mutate(has_ax = factor(has_ax)) %&gt;%
  mutate(has_ax = fct_rev(has_ax)) %&gt;%
  ggplot(aes(noise_level, n / 20, fill=scenario_completed)) +
    geom_bar(stat = &quot;identity&quot;) +
    facet_grid(rows = vars(has_ax), cols = vars(has_dx), labeller = label_both) +
    labs(y = &quot;Fraction completed&quot;) +
    legend_none() +
    scale_fill_economist()

grid.arrange(p1, p2, p3, ncol = 3)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-6-2.png' %}" width="1440" /></p>
<pre class="r"><code># Visualize also the variation in each of the three levels that we care about
p1 = gg_df %&gt;%
  filter(scenario_completed == &#39;complete&#39;) %&gt;%
  ggplot(aes(has_ax, n/20)) +
    geom_violin() +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep=&quot;:&quot;), size = 2)) +
    stat_summary(fun.y = median, geom=&quot;point&quot;, shape=23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(color = &quot;AX:DX:Noise&quot;) +
    theme(legend.position = &quot;left&quot;) +
    guides(size = F)

p2 = gg_df %&gt;%
  filter(scenario_completed == &#39;complete&#39;) %&gt;%
  ggplot(aes(has_dx, n/20)) +
    geom_violin() +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep=&quot;:&quot;), size = 2)) +
    stat_summary(fun.y = median, geom=&quot;point&quot;, shape=23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    legend_none()

p3 = gg_df %&gt;%
  filter(scenario_completed == &#39;complete&#39;) %&gt;%
  ggplot(aes(noise_level, n/20)) +
    geom_violin() +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep=&quot;:&quot;), size = 2)) +
    stat_summary(fun.y = median, geom=&quot;point&quot;, shape=23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    scale_x_discrete(limits = rev(levels(gg_df$noise_level))) +
    legend_none()

grid.arrange(p1, p2, p3, ncol = 3)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-6-3.png' %}" width="1440" /></p>
</div>
<div id="model" class="section level2">
<h2>Model</h2>
<p>Based on the data, we assume the following structural model:</p>
<p><span class="math display">\[frr_i = Bernoulli(p_i)\]</span> <span class="math display">\[\begin{aligned}
logit^{-1}(p_i) &amp;= \beta_0 + \beta_{ax}\text{ax}_i + \beta_{dx}\text{dx}_i + \beta_{noise}\text{noise}_i +\\ &amp;\beta_{ax:noise}\text{ax}_i\text{noise}_i + \beta_{dx:noise}\text{dx}_i\text{noise}_i + \\
&amp;\beta_{no}\text{no}_i + \mathbf{\beta_{demo}X_{demo,i}}
\end{aligned}\]</span> <span class="math display">\[\beta_{.} \sim Normal(0, 10)\]</span></p>
<p>Model fitting:</p>
<pre class="r"><code># A null model to compare against
scenario_completed.model.null = brm(
  scenario_completed ~ 0 + Intercept,
  family = &quot;bernoulli&quot;,
  prior = set_prior(&quot;normal(0, 10)&quot;, class = &quot;b&quot;),
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
scenario_completed.model.null = add_criterion(scenario_completed.model.null, &quot;waic&quot;)
scenario_completed.model.null = add_criterion(scenario_completed.model.null, &quot;loo&quot;, reloo = T)
saveModel(scenario_completed.model.null)

# The trend model to see if there is a trend in the noise level variable
scenario_completed.model.t = brm(
  scenario_completed ~ 0 + Intercept + ((has_dx + has_ax) * noise_level) + (gender + age_group + robot_experience) + num_optimal,
  family = &quot;bernoulli&quot;,
  prior = set_prior(&quot;normal(0, 10)&quot;, class = &quot;b&quot;),
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
scenario_completed.model.t = add_criterion(scenario_completed.model.t, &quot;waic&quot;)
scenario_completed.model.t = add_criterion(scenario_completed.model.t, &quot;loo&quot;, reloo = T)
saveModel(scenario_completed.model.t)

# The values model, to see if specific values of the noise level variable are significant
scenario_completed.model.v = brm(
  scenario_completed ~ 0 + Intercept + ((has_dx + has_ax) * noise_level_f) + (gender + age_group + robot_experience) + num_optimal,
  family = &quot;bernoulli&quot;,
  prior = set_prior(&quot;normal(0, 10)&quot;, class = &quot;b&quot;),
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
scenario_completed.model.v = add_criterion(scenario_completed.model.v, &quot;waic&quot;)
scenario_completed.model.v = add_criterion(scenario_completed.model.v, &quot;loo&quot;, reloo = T)
saveModel(scenario_completed.model.v)
# More model plots are available at:
# https://bookdown.org/ajkurz/Statistical_Rethinking_recoded/counting-and-classification.html

# Attempted RStanarm models: The estimate contrasts doesn&#39;t seem to be as helpful?
# scenario_completed.model.v = stan_glm(
#   scenario_completed ~ ((has_dx + has_ax) * noise_level) + (gender + age_group + robot_experience) + num_optimal,
#   family = binomial(link = &quot;logit&quot;),
#   prior = normal(0, 10),
#   prior_intercept = normal(0, 10),
#   data = plot_df,
#   QR = T,
#   seed = default_seed
# )</code></pre>
<pre class="r"><code># Load the models
scenario_completed.model.null = loadModel(&quot;scenario_completed.model.null&quot;)
scenario_completed.model.t = loadModel(&quot;scenario_completed.model.t&quot;)
scenario_completed.model.v = loadModel(&quot;scenario_completed.model.v&quot;)</code></pre>
<p>Model fitting results:</p>
<pre class="r"><code># print(summary(scenario_completed.model.null))
print(performance::r2(scenario_completed.model.null))</code></pre>
<pre><code>## # Bayesian R2 with Standard Error
##
##   Conditional R2: 0.000 [0.000]</code></pre>
<pre class="r"><code># Print the parameters, and some initial diagnostics
print(tidy_stan(scenario_completed.model.t))</code></pre>
<pre><code>## Warning: &#39;tidy_stan&#39; is deprecated.
## Use &#39;parameters::model_parameters()&#39; instead.
## See help(&quot;Deprecated&quot;)</code></pre>
<pre><code>##
## Summary Statistics of Stan-Model
##
## # Fixed effects
##
##              Parameter Estimate Std.Error       HDI(89%)     pd  ESS Rhat MCSE
##            (Intercept)     2.31      0.85 [ 0.85,  3.67]  99.6% 3044 1.00 0.02
##                has_dx1     0.27      0.32 [-0.27,  0.76]  79.8% 3151 1.00 0.01
##                has_ax1    -1.18      0.30 [-1.66, -0.71] 100.0% 2992 1.00 0.01
##          noise_level.L    -0.36      0.60 [-1.35,  0.60]  72.8% 2671 1.00 0.01
##          noise_level.Q     1.37      0.51 [ 0.63,  2.24]  99.8% 3049 1.00 0.01
##                gender1    -0.12      0.24 [-0.51,  0.28]  69.4% 4338 1.00 0.00
##             age_group1     0.57      0.76 [-0.71,  1.71]  77.6% 4668 1.00 0.01
##             age_group2    -0.63      0.47 [-1.36,  0.16]  90.0% 4611 1.00 0.01
##             age_group3    -0.40      0.46 [-1.15,  0.36]  79.6% 4342 1.00 0.01
##             age_group4     1.22      0.70 [ 0.10,  2.34]  97.3% 4040 1.00 0.01
##             age_group5    -1.15      0.62 [-2.16, -0.26]  97.0% 4786 1.00 0.01
##             age_group6     1.16      0.86 [-0.18,  2.61]  92.8% 3685 1.00 0.01
##      robot_experience1     1.27      0.44 [ 0.55,  1.98]  99.7% 4246 1.00 0.01
##      robot_experience2     0.81      0.59 [-0.16,  1.77]  91.9% 4531 1.00 0.01
##      robot_experience3    -1.25      0.76 [-2.45, -0.05]  95.5% 4070 1.00 0.01
##      robot_experience4    -1.13      0.70 [-2.34, -0.08]  94.7% 4479 1.00 0.01
##            num_optimal    -0.23      0.16 [-0.48,  0.01]  93.2% 3462 1.00 0.00
##  has_dx1.noise_level.L    -0.03      0.59 [-0.95,  0.96]  51.6% 2685 1.00 0.01
##  has_dx1.noise_level.Q     0.82      0.56 [-0.07,  1.75]  93.7% 2824 1.00 0.01
##  has_ax1.noise_level.L     0.40      0.48 [-0.34,  1.24]  80.2% 4041 1.00 0.01
##  has_ax1.noise_level.Q    -0.05      0.51 [-0.82,  0.80]  53.5% 4002 1.00 0.01</code></pre>
<pre class="r"><code>print(performance::r2(scenario_completed.model.t))</code></pre>
<pre><code>## # Bayesian R2 with Standard Error
##
##   Conditional R2: 0.269 [0.038]</code></pre>
<pre class="r"><code>print(tidy_stan(scenario_completed.model.v))</code></pre>
<pre><code>## Warning: &#39;tidy_stan&#39; is deprecated.
## Use &#39;parameters::model_parameters()&#39; instead.
## See help(&quot;Deprecated&quot;)</code></pre>
<pre><code>##
## Summary Statistics of Stan-Model
##
## # Fixed effects
##
##               Parameter Estimate Std.Error       HDI(89%)     pd  ESS Rhat MCSE
##             (Intercept)     2.28      0.87 [ 0.82,  3.58]  99.6% 3032 1.00 0.02
##                 has_dx1     0.27      0.32 [-0.29,  0.78]  79.2% 2820 1.00 0.01
##                 has_ax1    -1.18      0.29 [-1.69, -0.74] 100.0% 3123 1.00 0.01
##          noise_level_f1     0.79      0.45 [ 0.05,  1.50]  96.2% 2582 1.00 0.01
##          noise_level_f2    -1.14      0.41 [-1.81, -0.49]  99.9% 3208 1.00 0.01
##                 gender1    -0.12      0.24 [-0.48,  0.27]  69.3% 5743 1.00 0.00
##              age_group1     0.57      0.75 [-0.62,  1.74]  78.3% 4339 1.00 0.01
##              age_group2    -0.63      0.47 [-1.39,  0.13]  90.9% 4492 1.00 0.01
##              age_group3    -0.39      0.47 [-1.10,  0.39]  78.8% 4904 1.00 0.01
##              age_group4     1.24      0.69 [ 0.14,  2.43]  96.9% 4085 1.00 0.01
##              age_group5    -1.18      0.61 [-2.19, -0.25]  97.2% 4628 1.00 0.01
##              age_group6     1.14      0.85 [-0.22,  2.51]  93.8% 3947 1.00 0.01
##       robot_experience1     1.26      0.46 [ 0.54,  1.95]  99.8% 3949 1.00 0.01
##       robot_experience2     0.83      0.58 [-0.16,  1.71]  91.7% 4613 1.00 0.01
##       robot_experience3    -1.23      0.76 [-2.47, -0.01]  94.7% 4535 1.00 0.01
##       robot_experience4    -1.14      0.73 [-2.34, -0.04]  94.2% 4414 1.00 0.01
##             num_optimal    -0.22      0.16 [-0.48,  0.01]  92.8% 3552 1.00 0.00
##  has_dx1:noise_level_f1     0.36      0.40 [-0.29,  0.99]  80.9% 2742 1.00 0.01
##  has_dx1:noise_level_f2    -0.71      0.46 [-1.43,  0.04]  94.4% 3317 1.00 0.01
##  has_ax1:noise_level_f1    -0.27      0.40 [-0.94,  0.33]  76.2% 2754 1.00 0.01
##  has_ax1:noise_level_f2     0.02      0.43 [-0.69,  0.67]  52.2% 3082 1.00 0.01</code></pre>
<pre class="r"><code>print(performance::r2(scenario_completed.model.v))</code></pre>
<pre><code>## # Bayesian R2 with Standard Error
##
##   Conditional R2: 0.268 [0.038]</code></pre>
</div>
<div id="diagnostic" class="section level2">
<h2>Diagnostic</h2>
<pre class="r"><code># Alternative diagnostics: diagnostic_posterior(scenario_completed.model.v)

# LOO cross-validation results
print(scenario_completed.model.null$criteria$loo$estimates)</code></pre>
<pre><code>##            Estimate         SE
## elpd_loo -99.687599  8.0556274
## p_loo      1.008959  0.1099671
## looic    199.375197 16.1112549</code></pre>
<pre class="r"><code>print(scenario_completed.model.t$criteria$loo$estimates)</code></pre>
<pre><code>##            Estimate        SE
## elpd_loo -106.85736 11.812881
## p_loo      30.87335  4.896366
## looic     213.71473 23.625762</code></pre>
<pre class="r"><code>print(scenario_completed.model.v$criteria$loo$estimates)</code></pre>
<pre><code>##            Estimate        SE
## elpd_loo -107.04312 12.043537
## p_loo      31.08861  5.317938
## looic     214.08623 24.087074</code></pre>
<pre class="r"><code>print(loo_compare(scenario_completed.model.null,
                  scenario_completed.model.t,
                  scenario_completed.model.v))</code></pre>
<pre><code>##                               elpd_diff se_diff
## scenario_completed.model.null  0.0       0.0
## scenario_completed.model.t    -7.2       8.5
## scenario_completed.model.v    -7.4       8.8</code></pre>
<pre class="r"><code># Diagnostics of the sampling process
par(mfrow=c(1,2))
plot(scenario_completed.model.t$criteria$loo, main = &quot;Trend Model&quot;)
plot(scenario_completed.model.v$criteria$loo, main = &quot;Values Model&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-10-1.png' %}" width="1440" /></p>
<pre class="r"><code># Include posterior predictive checks of the sampling (are sampling according
# to the data that we actually have?)
mcmc_intervals(as.matrix(scenario_completed.model.null), regex_pars = &quot;b_&quot;, prob = 0.90, prob_outer = 0.95) +
  ggtitle(&quot;Null Model&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-10-2.png' %}" width="1440" /></p>
<pre class="r"><code>p1 = pp_check(scenario_completed.model.t, type = &quot;bars_grouped&quot;, group = &quot;has_ax&quot;, stat = &quot;median&quot;) + ggtitle(&quot;t:has_ax&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars_grouped&#39; by default.</code></pre>
<pre><code>## Warning: The following arguments were unrecognized and ignored: stat</code></pre>
<pre class="r"><code>p2 = pp_check(scenario_completed.model.t, type = &quot;bars_grouped&quot;, group = &quot;has_dx&quot;) + ggtitle(&quot;t:has_dx&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars_grouped&#39; by default.</code></pre>
<pre class="r"><code>p3 = pp_check(scenario_completed.model.t, type = &quot;bars_grouped&quot;, group = &quot;noise_level&quot;) + ggtitle(&quot;t:noise_level&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars_grouped&#39; by default.</code></pre>
<pre class="r"><code>grid.arrange(p1, p2, p3, nrow = 3)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-10-3.png' %}" width="1440" /></p>
<pre class="r"><code>mcmc_intervals(as.matrix(scenario_completed.model.t), regex_pars = &quot;b_&quot;, prob = 0.90, prob_outer = 0.95) +
  ggtitle(&quot;Trends Model&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-10-4.png' %}" width="1440" /></p>
<pre class="r"><code>p1 = pp_check(scenario_completed.model.v, type = &quot;bars_grouped&quot;, group = &quot;has_ax&quot;) + ggtitle(&quot;v:has_ax&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars_grouped&#39; by default.</code></pre>
<pre class="r"><code>p2 = pp_check(scenario_completed.model.v, type = &quot;bars_grouped&quot;, group = &quot;has_dx&quot;) + ggtitle(&quot;v:has_dx&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars_grouped&#39; by default.</code></pre>
<pre class="r"><code>p3 = pp_check(scenario_completed.model.v, type = &quot;bars_grouped&quot;, group = &quot;noise_level_f&quot;) + ggtitle(&quot;v:noise_level&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars_grouped&#39; by default.</code></pre>
<pre class="r"><code>grid.arrange(p1, p2, p3, nrow = 3)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-10-5.png' %}" width="1440" /></p>
<pre class="r"><code>mcmc_intervals(as.matrix(scenario_completed.model.v), regex_pars = &quot;b_&quot;, prob = 0.90, prob_outer = 0.95) +
  ggtitle(&quot;Values Model&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-10-6.png' %}" width="1440" /></p>
<pre class="r"><code># Plot predictions vs original
preds_p_df = scenario_completed.model.null %&gt;% predict() %&gt;% as_tibble()
p1 =
  preds_p_df %&gt;%
    add_column(scenario_completed = scenario_completed.model.null$data$scenario_completed) %&gt;%
    ggplot(aes(x = Estimate, y = scenario_completed, color = scenario_completed)) +
      geom_point() +
      labs(x = &quot;Predicted values&quot;) +
      ylim(0, 1) +
      xlim(0, 1) +
      ggtitle(&quot;Null Model&quot;)

preds_p_df = plot_df %&gt;%
  add_predicted_draws(scenario_completed.model.null) %&gt;%
  group_by(scenario_completed, .prediction) %&gt;%
  count()
p2 =
  preds_p_df %&gt;%
    ggplot(aes(x = scenario_completed, y = .prediction, fill = n / sum(preds_p_df$n))) +
      geom_tile() +
      geom_text(aes(label = sprintf(&quot;%1.2f&quot;, n / sum(preds_p_df$n))), size = 10, vjust = 1) +
      scale_fill_gradient(limits = c(0, 1))

preds_p_df = scenario_completed.model.t %&gt;% predict() %&gt;% as_tibble()
p3 =
  preds_p_df %&gt;%
    add_column(scenario_completed = scenario_completed.model.t$data$scenario_completed) %&gt;%
    ggplot(aes(x = Estimate, y = scenario_completed, color = scenario_completed)) +
      geom_point() +
      labs(x = &quot;Predicted values&quot;) +
      ylim(0, 1) +
      xlim(0, 1) +
      ggtitle(&quot;Trends Model&quot;)

preds_p_df = plot_df %&gt;%
  add_predicted_draws(scenario_completed.model.t) %&gt;%
  group_by(scenario_completed, .prediction) %&gt;%
  count()
p4 =
  preds_p_df %&gt;%
    ggplot(aes(x = scenario_completed, y = .prediction, fill = n / sum(preds_p_df$n))) +
      geom_tile() +
      geom_text(aes(label = sprintf(&quot;%1.2f&quot;, n / sum(preds_p_df$n))), size = 10, vjust = 1) +
      scale_fill_gradient(limits = c(0, 1))

# Plot predictions vs original
preds_p_df = scenario_completed.model.v %&gt;% predict() %&gt;% as_tibble()
p5 =
  preds_p_df %&gt;%
    add_column(scenario_completed = scenario_completed.model.v$data$scenario_completed) %&gt;%
    ggplot(aes(x = Estimate, y = scenario_completed, color = scenario_completed)) +
      geom_point() +
      labs(x = &quot;Predicted values&quot;) +
      ylim(0, 1) +
      xlim(0, 1) +
      ggtitle(&quot;Values Model&quot;)

preds_p_df = plot_df %&gt;%
  add_predicted_draws(scenario_completed.model.v) %&gt;%
  group_by(scenario_completed, .prediction) %&gt;%
  count()
p6 =
  preds_p_df %&gt;%
    ggplot(aes(x = scenario_completed, y = .prediction, fill = n / sum(preds_p_df$n))) +
      geom_tile() +
      geom_text(aes(label = sprintf(&quot;%1.2f&quot;, n / sum(preds_p_df$n))), size = 10, vjust = 1) +
      scale_fill_gradient(limits = c(0, 1))

grid.arrange(p1, p3, p5, p2, p4, p6, nrow = 2, ncol = 3)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-10-7.png' %}" width="1440" /></p>
<p>We are able to recreate the data, and there is a mild improvement in the recreation / prediction as a result of our model. So we are fine, I think.</p>
</div>
<div id="inference" class="section level2">
<h2>Inference</h2>
<pre class="r"><code># Compute the bayes factor for each parameter value. Note: we compare
# here against the ROPE, and not the degenerate null hypothesis of 0
# Bayes-Factor values would be more significant in the latter case
# According to the Raftery, 1995 rules
# bf = 1 - 3: Weak
# bf = 3 - 20: Positive
# bf = 20 - 150: Strong
# bf &gt; 150: Very strong
print(bayesfactor_rope(scenario_completed.model.null, null = c(-0.055, 0.055)), n = 30)
print(bayesfactor_rope(scenario_completed.model.t, null = c(-0.055, 0.055)), n = 30)
print(bayesfactor_rope(scenario_completed.model.v, null = c(-0.055, 0.055)), n = 30)</code></pre>
<pre class="r"><code># Compare the models. Note: we cannot do this because we don&#39;t have enough samples.
# The default is 4000; apparently these functions are only meaningful with 40000
# comparison = bayesfactor_models(scenario_completed.model.t, scenario_completed.model.v,
#                                 denominator = scenario_completed.model.null)
# print(comparison)
# print(bayesfactor_inclusion(comparison))

# # If we&#39;re using the brms functions, then use the following (make sure to update!)
# common_hyp_to_test = c(&quot;-2 * has_ax1 = 0&quot;, &quot;-2 * has_dx1 = 0&quot;)
# noise_levels_hyp_to_test = c(
#   &quot;Intercept-noise_level_f1 = 0&quot;, &quot;Intercept-noise_level_f2 = 0&quot;, &quot;Intercept-noise_level_f1-noise_level_f2 = 0&quot;,
#   &quot;Intercept-has_ax1:noise_level_f1 = 0&quot;, &quot;Intercept-has_dx1:noise_level_f1 = 0&quot;,
#   &quot;Intercept-has_ax1:noise_level_f2 = 0&quot;, &quot;Intercept-has_dx1:noise_level_f2 = 0&quot;,
#   &quot;Intercept-has_ax1:noise_level_f1-has_ax1:noise_level_f2 = 0&quot;,
#   &quot;Intercept-has_dx1:noise_level_f1-has_dx1:noise_level_f2 = 0&quot;
# )
# hyp_results = test_hypotheses(hypotheses_list = c(common_hyp_to_test, noise_levels_hyp_to_test), model = scenario_completed.model.v)
# print(hyp_results$hypothesis)
# plot(hyp_results, ask=F)

options(digits = 3)
# Check the significance of each hypothesis
h_df = as_tibble(insight::get_parameters(scenario_completed.model.t)) %&gt;%
  transmute(
    Intercept = b_Intercept,
    has_ax_test = -2 * b_has_ax1,
    has_dx_test = -2 * b_has_dx1,
    noise_levelL_test = b_noise_level.L,
    noise_levelQ_test = b_noise_level.Q
  )

hyp_results = test_hypotheses(h_df, rope_values = c(-0.055, 0.055))
print(hyp_results %&gt;% select(Parameter, pd, HDI_low, HDI_high, ROPE_Percentage, ROPE_Equivalence) %&gt;% as.matrix())</code></pre>
<pre><code>##      Parameter           pd      HDI_low  HDI_high ROPE_Percentage
## [1,] &quot;Intercept&quot;         &quot;0.996&quot; &quot;-0.839&quot; &quot;5.74&quot;   &quot;0.00075&quot;
## [2,] &quot;has_ax_test&quot;       &quot;1.000&quot; &quot; 0.246&quot; &quot;4.83&quot;   &quot;0.00000&quot;
## [3,] &quot;has_dx_test&quot;       &quot;0.798&quot; &quot;-3.279&quot; &quot;1.82&quot;   &quot;0.05050&quot;
## [4,] &quot;noise_levelL_test&quot; &quot;0.728&quot; &quot;-2.412&quot; &quot;3.28&quot;   &quot;0.05550&quot;
## [5,] &quot;noise_levelQ_test&quot; &quot;0.998&quot; &quot;-0.204&quot; &quot;3.53&quot;   &quot;0.00100&quot;
##      ROPE_Equivalence
## [1,] &quot;Rejected&quot;
## [2,] &quot;Rejected&quot;
## [3,] &quot;Undecided&quot;
## [4,] &quot;Undecided&quot;
## [5,] &quot;Rejected&quot;</code></pre>
<pre class="r"><code>h_df = as_tibble(insight::get_parameters(scenario_completed.model.v)) %&gt;%
  transmute(
    Intercept = b_Intercept,
    has_ax_test = -2 * b_has_ax1,
    has_dx_test = -2 * b_has_dx1,
    noise_level1_test = b_noise_level_f1,
    noise_level2_test = b_noise_level_f2,
    noise_level3_test = - b_noise_level_f1 - b_noise_level_f2
  )

hyp_results = test_hypotheses(h_df, rope_values = c(-0.055, 0.055))
print(hyp_results %&gt;% select(Parameter, pd, HDI_low, HDI_high, ROPE_Percentage, ROPE_Equivalence) %&gt;% as.matrix())</code></pre>
<pre><code>##      Parameter           pd      HDI_low  HDI_high ROPE_Percentage
## [1,] &quot;Intercept&quot;         &quot;0.996&quot; &quot;-0.752&quot; &quot;5.260&quot;  &quot;0.0020&quot;
## [2,] &quot;has_ax_test&quot;       &quot;1.000&quot; &quot; 0.105&quot; &quot;4.729&quot;  &quot;0.0000&quot;
## [3,] &quot;has_dx_test&quot;       &quot;0.792&quot; &quot;-3.663&quot; &quot;1.696&quot;  &quot;0.0460&quot;
## [4,] &quot;noise_level1_test&quot; &quot;0.962&quot; &quot;-0.775&quot; &quot;2.622&quot;  &quot;0.0175&quot;
## [5,] &quot;noise_level2_test&quot; &quot;0.999&quot; &quot;-2.665&quot; &quot;0.243&quot;  &quot;0.0015&quot;
## [6,] &quot;noise_level3_test&quot; &quot;0.726&quot; &quot;-1.429&quot; &quot;3.128&quot;  &quot;0.0775&quot;
##      ROPE_Equivalence
## [1,] &quot;Rejected&quot;
## [2,] &quot;Rejected&quot;
## [3,] &quot;Undecided&quot;
## [4,] &quot;Rejected&quot;
## [5,] &quot;Rejected&quot;
## [6,] &quot;Undecided&quot;</code></pre>
<pre class="r"><code>options(digits = 7)</code></pre>
<p><strong>Results</strong>:</p>
<ol style="list-style-type: decimal">
<li>AX suggestions
<ul>
<li>Positive effect pd = 100%</li>
<li>Median = 2.36, 89% CI [1.48, 3.38]</li>
<li>Medium, Std.Median = 0.58</li>
<li>0.0% in ROPE</li>
</ul></li>
<li>Noise Level Quadratic
<ul>
<li>Positive (convex-shape) pd = 99.8%</li>
<li>Median = 1.37, 89% CI [0.63, 2.24]</li>
<li>Medium, Std.Median = 0.51</li>
<li>0.1% in ROPE</li>
</ul></li>
</ol>
<p>(The ROPE is defined as [-0.055, 0.055]. It is the range is suggested in the literature for logistic models and it corresponds to a probability range of 0.11. Therefore, a probability change of less than 0.055 is considered no different from a probability change of 0)</p>
</div>
<div id="posterior-plots" class="section level2">
<h2>Posterior Plots</h2>
<pre class="r"><code># A visualization data frame
gg_df = plot_df %&gt;%
  mutate(scenario_completed = factor(scenario_completed)) %&gt;%
  mutate(scenario_completed = fct_recode(scenario_completed, incomplete=&quot;0&quot;, complete = &quot;1&quot;)) %&gt;%
  mutate(noise_level = fct_rev(noise_level)) %&gt;%
  count(has_ax, has_dx, noise_level, scenario_completed)

# Simulate predictions on new data
p_df =
  plot_df %&gt;%
  data_grid(scenario_completed,
            has_ax,
            has_dx,
            noise_level_f,
            gender,
            age_group,
            robot_experience,
            num_optimal,
            .model = scenario_completed.model.v)

fits_p_df = p_df %&gt;% add_fitted_draws(scenario_completed.model.v)
pars_p_df = scenario_completed.model.t %&gt;% extract_draws(newdata = p_df %&gt;% rename(noise_level = noise_level_f))
# We don&#39;t want predicted draws because that&#39;s a raw count of ones and zeros; not the
# probability distribution
# preds_p_df = p_df %&gt;% add_predicted_draws(scenario_completed.model.v)

# Plot the posterior distributions
# rope_value = inv_logit_scaled(fixef(scenario_completed.model.null)[&quot;Intercept&quot;,&quot;Estimate&quot;])
rope_value = gg_df %&gt;%
  filter(scenario_completed == &#39;complete&#39;, has_ax == F) %&gt;%
  summarise(.value = median(n) / 20) %&gt;%
  pull(.value)
gg_df %&gt;%
  filter(scenario_completed == &#39;complete&#39;) %&gt;%
  ggplot(aes(x = has_ax, y = n / 20)) +
    geom_rect(ymin = rope_value-0.055, ymax = rope_value+0.055,
              xmin = -Inf, xmax = Inf,
              color = &quot;transparent&quot;, fill = &quot;white&quot;) +
    annotate(&quot;text&quot;, x = 0.5, y = rope_value, label = &quot;ROPE&quot;, angle = 90) +
    stat_halfeye(aes(y = .value), data = fits_p_df, position = position_nudge(x = 0.1)) +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep = &quot;:&quot;)), size = 2) +
    stat_summary(fun.y = median, geom = &quot;point&quot;, shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(y = &quot;scenario_completed&quot;, color = &quot;AX:DX:Noise&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-13-1.png' %}" width="672" /></p>
<pre class="r"><code>rope_value = gg_df %&gt;%
  filter(scenario_completed == &#39;complete&#39;, has_dx == F) %&gt;%
  summarise(.value = median(n) / 20) %&gt;%
  pull(.value)
gg_df %&gt;%
  filter(scenario_completed == &#39;complete&#39;) %&gt;%
  ggplot(aes(x = has_dx, y = n / 20)) +
    geom_rect(ymin = rope_value-0.055, ymax = rope_value+0.055,
              xmin = -Inf, xmax = Inf,
              color = &quot;transparent&quot;, fill = &quot;white&quot;) +
    annotate(&quot;text&quot;, x = 0.5, y = rope_value, label = &quot;ROPE&quot;, angle = 90) +
    stat_halfeye(aes(y = .value), data = fits_p_df, position = position_nudge(x = 0.1)) +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep = &quot;:&quot;)), size = 2) +
    stat_summary(fun.y = median, geom = &quot;point&quot;, shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(y = &quot;scenario_completed&quot;, color = &quot;AX:DX:Noise&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-13-2.png' %}" width="672" /></p>
<pre class="r"><code>rope_value = gg_df %&gt;%
  filter(scenario_completed == &#39;complete&#39;, noise_level == &quot;0.0&quot;) %&gt;%
  summarise(.value = median(n) / 20) %&gt;%
  pull(.value)
gg_df %&gt;%
  filter(scenario_completed == &#39;complete&#39;) %&gt;%
  mutate(noise_level = fct_rev(noise_level)) %&gt;%
  ggplot(aes(x = noise_level, y = n / 20)) +
    geom_rect(ymin = rope_value-0.055, ymax = rope_value+0.055,
              xmin = -Inf, xmax = Inf,
              color = &quot;transparent&quot;, fill = &quot;white&quot;) +
    annotate(&quot;text&quot;, x = 0.5, y = rope_value, label = &quot;ROPE&quot;, angle = 90) +
    stat_halfeye(aes(y = .value, x = noise_level_f), data = fits_p_df, position = position_nudge(x = 0.1)) +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep = &quot;:&quot;)), size = 2) +
    stat_summary(fun.y = median, geom = &quot;point&quot;, shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(y = &quot;scenario_completed&quot;, color = &quot;AX:DX:Noise&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-13-3.png' %}" width="672" /></p>
<pre class="r"><code># Create a plot to show the trend
pars_p_df_X = pars_p_df$dpars$mu$fe$X[,c(&quot;Intercept&quot;, &quot;noise_level.Q&quot;)]
pars_p_df_b = pars_p_df$dpars$mu$fe$b[,c(&quot;b_Intercept&quot;, &quot;b_noise_level.Q&quot;)]
pars_p_df_y = pars_p_df_X %*% t(pars_p_df_b)[,as.integer(seq(from = 1, to = 4000, length.out = 100))]
pars_p_df_y = inv_logit_scaled(pars_p_df_y)
p_df =
  p_df %&gt;%
  bind_cols(as_tibble(pars_p_df_y)) %&gt;%
  gather(key = &quot;.sample&quot;, value = &quot;.value&quot;, V1:V100) %&gt;%
  rename(noise_level = noise_level_f)</code></pre>
<pre><code>## Warning: `as_tibble.matrix()` requires a matrix with column names or a `.name_repair` argument. Using compatibility `.name_repair`.
## This warning is displayed once per session.</code></pre>
<pre class="r"><code>rope_value = gg_df %&gt;%
  filter(scenario_completed == &#39;complete&#39;, noise_level == &quot;0.0&quot;) %&gt;%
  summarise(.value = median(n) / 20) %&gt;%
  pull(.value)
gg_df %&gt;%
  filter(scenario_completed == &#39;complete&#39;) %&gt;%
  mutate(noise_level = fct_rev(noise_level)) %&gt;%
  ggplot(aes(x = noise_level, y = n / 20)) +
    geom_rect(ymin = rope_value-0.055, ymax = rope_value+0.055,
              xmin = -Inf, xmax = Inf,
              color = &quot;transparent&quot;, fill = &quot;white&quot;) +
    annotate(&quot;text&quot;, x = 0.5, y = rope_value, label = &quot;ROPE&quot;, angle = 90) +
    stat_halfeye(aes(y = .value, x = noise_level_f), data = fits_p_df, position = position_nudge(x = 0.1)) +
    geom_line(aes(y = .value, group = .sample, alpha = 0.3), data = p_df) +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep = &quot;:&quot;)), size = 2) +
    stat_summary(fun.y = median, geom = &quot;point&quot;, shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(y = &quot;scenario_completed&quot;, color = &quot;AX:DX:Noise&quot;) +
    guides(alpha = F)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-13-4.png' %}" width="672" /></p>
<pre class="r"><code># Remove the giant fits data frames
rm(fits_p_df, pars_p_df, pars_p_df_b, pars_p_df_X, pars_p_df_y, p_df, gg_df)</code></pre>
<pre class="r"><code>eff = fixef(scenario_completed.model.v)

gg_df = plot_df %&gt;%
  mutate(scenario_completed = factor(scenario_completed)) %&gt;%
  mutate(scenario_completed = fct_recode(scenario_completed, Unresolved=&quot;0&quot;, Resolved = &quot;1&quot;)) %&gt;%
  count(has_ax, has_dx, noise_level, scenario_completed) %&gt;%
  mutate(has_ax = fct_recode(has_ax, &quot;No AX&quot;=&quot;FALSE&quot;, &quot;AX&quot;=&quot;TRUE&quot;),
         has_dx = fct_recode(has_dx, &quot;No DX&quot;=&quot;FALSE&quot;, &quot;DX&quot;=&quot;TRUE&quot;),
         noise_level = fct_recode(noise_level, &quot;Acc: 100%&quot;=&quot;0.0&quot;, &quot;Acc: 90%&quot;=&quot;1.0&quot;, &quot;Acc: 80%&quot;=&quot;2.0&quot;),
         estimate = -1) %&gt;%
  mutate(estimate = if_else(has_ax == &quot;No AX&quot;, inv_logit_scaled(eff[&quot;Intercept&quot;, &quot;Estimate&quot;] + eff[&quot;has_ax1&quot;,&quot;Estimate&quot;]), estimate)) %&gt;%
  mutate(estimate = if_else(has_ax == &quot;AX&quot;, inv_logit_scaled(eff[&quot;Intercept&quot;, &quot;Estimate&quot;] - eff[&quot;has_ax1&quot;, &quot;Estimate&quot;]), estimate))

gg_df %&gt;%
  filter(scenario_completed == &#39;Resolved&#39;) %&gt;%
  ggplot(aes(has_ax, n/20, group = has_ax, colour = has_ax)) +
    geom_count() +
    geom_boxplot(aes(y = estimate)) +
    geom_signif(y_position = 1.05, xmin = 1, xmax = 2, annotation = &quot;**&quot;, textsize = 8, color = &quot;black&quot;) +
    scale_y_continuous(limits = c(0.5, 1.1)) +
    labs(y = &quot;Fraction of participants that resolved fault&quot;, x = NULL) +
    scale_colour_manual(values = c(&quot;#d62728&quot;, &quot;#2ca02c&quot;, &quot;#ff7f0e&quot;)) +
    theme_economist_white(base_size = 13) +
    theme(plot.background = element_rect(fill = &quot;white&quot;),
          axis.title.y = element_blank()) +
    legend_none()</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-14-1.png' %}" width="672" /></p>
<pre class="r"><code>gg_df = plot_df %&gt;%
  mutate(scenario_completed = factor(scenario_completed)) %&gt;%
  mutate(scenario_completed = fct_recode(scenario_completed, Unresolved=&quot;0&quot;, Resolved = &quot;1&quot;)) %&gt;%
  count(has_ax, has_dx, noise_level, scenario_completed) %&gt;%
  mutate(has_ax = fct_recode(has_ax, &quot;No AX&quot;=&quot;FALSE&quot;, &quot;AX&quot;=&quot;TRUE&quot;),
         has_dx = fct_recode(has_dx, &quot;No DX&quot;=&quot;FALSE&quot;, &quot;DX&quot;=&quot;TRUE&quot;),
         noise_level = fct_recode(noise_level, &quot;Acc: 100%&quot;=&quot;0.0&quot;, &quot;Acc: 90%&quot;=&quot;1.0&quot;, &quot;Acc: 80%&quot;=&quot;2.0&quot;),
         estimate = -1) %&gt;%
  mutate(estimate = if_else(noise_level == &quot;Acc: 100%&quot;, inv_logit_scaled(eff[&quot;noise_level_f1&quot;,&quot;Estimate&quot;] + eff[&quot;Intercept&quot;, &quot;Estimate&quot;]), estimate)) %&gt;%
  mutate(estimate = if_else(noise_level == &quot;Acc: 90%&quot;, inv_logit_scaled(eff[&quot;noise_level_f2&quot;,&quot;Estimate&quot;] + eff[&quot;Intercept&quot;, &quot;Estimate&quot;]), estimate)) %&gt;%
  mutate(estimate = if_else(noise_level == &quot;Acc: 80%&quot;, inv_logit_scaled(eff[&quot;Intercept&quot;, &quot;Estimate&quot;] - eff[&quot;noise_level_f1&quot;,&quot;Estimate&quot;] - eff[&quot;noise_level_f2&quot;,&quot;Estimate&quot;]), estimate)) %&gt;%
  mutate(noise_level = fct_rev(noise_level))

gg_df %&gt;%
  filter(scenario_completed == &#39;Resolved&#39;) %&gt;%
  ggplot(aes(noise_level, n/20, group = noise_level, colour = noise_level)) +
    geom_hline(yintercept = inv_logit_scaled(eff[&quot;Intercept&quot;, &quot;Estimate&quot;]),
               size = .7, linetype = &quot;dashed&quot;, color = &quot;grey&quot;) +
    geom_count() +
    geom_boxplot(aes(y = estimate)) +
    annotate(&quot;segment&quot;,
             x = 3,
             y = inv_logit_scaled(eff[&quot;noise_level_f1&quot;,&quot;Estimate&quot;] + eff[&quot;Intercept&quot;, &quot;Estimate&quot;]),
             xend = 2,
             yend = inv_logit_scaled(eff[&quot;noise_level_f2&quot;,&quot;Estimate&quot;] + eff[&quot;Intercept&quot;, &quot;Estimate&quot;]),
             colour = &quot;black&quot;) +
    annotate(&quot;segment&quot;,
             x = 1,
             y = inv_logit_scaled(eff[&quot;Intercept&quot;, &quot;Estimate&quot;] - eff[&quot;noise_level_f1&quot;,&quot;Estimate&quot;] - eff[&quot;noise_level_f2&quot;,&quot;Estimate&quot;]),
             xend = 2,
             yend = inv_logit_scaled(eff[&quot;noise_level_f2&quot;,&quot;Estimate&quot;] + eff[&quot;Intercept&quot;, &quot;Estimate&quot;]),
             color = &quot;black&quot;) +
    annotate(&quot;text&quot;,
             x = 3.5,
             y = inv_logit_scaled(eff[&quot;Intercept&quot;, &quot;Estimate&quot;] - 0.85),
             label = &quot;Mean of all levels&quot;,
             angle = 90,
             group = NA,
             colour = &quot;black&quot;) +
    scale_y_continuous(limits = c(0.5, 1.1), breaks = c(0.5, 1.0)) +
    labs(y = &quot;Fraction of participants that resolved fault&quot;, x = NULL) +
    scale_colour_manual(values = c(&quot;#d62728&quot;, &quot;#2ca02c&quot;, &quot;#ff7f0e&quot;)) +
    scale_size_area(max_size = 1) +
    theme_economist_white(base_size = 13) +
    theme(plot.background = element_rect(fill = &quot;white&quot;),
          axis.title.y = element_blank()) +
    legend_none()</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-14-2.png' %}" width="672" /></p>
</div>
</div>
<div id="rax-rate-of-optimal-action-selection" class="section level1">
<h1>RAX: Rate of Optimal Action Selection</h1>
<p><strong>Did the user take an optimal action given the state that they were in?</strong></p>
<p>This is also a measure of Reliance on Suggestions. In the code, the variable might be referred to as <code>optimal_ax</code>, <code>correct_ax</code>, etc. depending on the version of the codebase</p>
<pre class="r"><code>plot_df = actions %&gt;%
  select(X1, id, user_id, study_condition, start_condition, num_optimal,
         state_idx, state_idx_rescaled,
         age_group, gender, robot_experience,
         noise_level, noise_level_f, has_noise, has_dx, has_ax,
         scenario_completed, num_actions, optimal_ax)

text_short(report(plot_df))</code></pre>
<pre><code>## Warning in report.numeric(model[[col]], median = median, centrality =
## centrality, : Variable `model[[col]]` contains only two different values.
## Consider converting it to a factor.

## Warning in report.numeric(model[[col]], median = median, centrality =
## centrality, : Variable `model[[col]]` contains only two different values.
## Consider converting it to a factor.</code></pre>
<pre><code>## The data contains 2115 observations of the following variables:
##   - X1: Mean = 1057.00, SD = 610.69, range = [0, 2114], 0 missing
##   - id: Mean = 1058.00, SD = 610.69, range = [1, 2115], 0 missing
##   - user_id: Mean = 102.24, SD = 56.97, range = [1, 200], 0 missing
##   - study_condition: 10 levels: DX_100, n = 247; AX_100, n = 152; DXAX_100, n = 193; DX_90, n = 243; AX_90, n = 217; DXAX_90, n = 183; DX_80, n = 256; AX_80, n = 204; DXAX_80, n = 189 and BASELINE, n = 231
##   - start_condition: 4 entries: kc.dt.occluding.above_mug.default.empty.dt, n = 672; kc.kc.occluding.default.default.empty.dt, n = 595; dt.kc.default.default.default.empty.kc, n = 487 and 1 other
##   - num_optimal: Mean = 4.78, SD = 1.56, range = [3, 7], 0 missing
##   - state_idx: Mean = 88.30, SD = 60.07, range = [1, 173], 0 missing
##   - state_idx_rescaled: Mean = 0.51, SD = 0.35, range = [0, 1], 0 missing
##   - age_group: 7 levels: 2, n = 226; 3, n = 506; 4, n = 473; 5, n = 283; 6, n = 222; 7, n = 170 and 8, n = 235
##   - gender: 2 levels: F, n = 834 and M, n = 1281
##   - robot_experience: 5 levels: 0, n = 1337; 1, n = 336; 2, n = 161; 3, n = 169 and 4, n = 112
##   - noise_level: 3 levels: 0.0, n = 823; 1.0, n = 643 and 2.0, n = 649
##   - noise_level_f: 3 levels: 0.0, n = 823; 1.0, n = 643 and 2.0, n = 649
##   - has_noise: 2 levels: FALSE, n = 823 and TRUE, n = 1292
##   - has_dx: 2 levels: FALSE, n = 804 and TRUE, n = 1311
##   - has_ax: 2 levels: FALSE, n = 977 and TRUE, n = 1138
##   - scenario_completed: Mean = 0.64, SD = 0.48, range = [0, 1], 0 missing
##   - num_actions: Mean = 13.92, SD = 5.86, range = [3, 20], 0 missing
##   - optimal_ax: Mean = 0.39, SD = 0.49, range = [0, 1], 0 missing</code></pre>
<div id="data-1" class="section level2">
<h2>Data</h2>
<pre class="r"><code>gg_df =
  plot_df %&gt;%
    mutate(noise_level = fct_rev(noise_level)) %&gt;%
    mutate(optimal_ax = factor(optimal_ax)) %&gt;%
    mutate(user_id = factor(user_id)) %&gt;%
    mutate(optimal_ax = fct_recode(optimal_ax, incorrect=&quot;0&quot;, correct = &quot;1&quot;)) %&gt;%
    mutate(.fill_column = paste(noise_level, !(study_condition == &#39;BASELINE&#39;), sep = &quot;:&quot;)) %&gt;%
    count(user_id, study_condition, .fill_column, optimal_ax, num_actions, has_ax, has_dx, noise_level,
          name = &quot;num_optimal_ax&quot;)

# Plot by the study condition
gg_df %&gt;%
  ggplot(aes(user_id, num_optimal_ax / num_actions, fill=optimal_ax)) +
    geom_bar(stat=&quot;identity&quot;) +
    geom_hline(yintercept = 0.5) +
    facet_wrap(vars(study_condition), nrow = 1, scales = &quot;free_y&quot;) +
    labs(y = &quot;Fraction correct actions&quot;) +
    coord_flip() +
    scale_fill_economist()</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-17-1.png' %}" width="1440" /></p>
<pre class="r"><code>gg_df %&gt;%
  filter(optimal_ax == &quot;correct&quot;) %&gt;%
  ggplot(aes(study_condition, num_optimal_ax / num_actions, fill=.fill_column)) +
    geom_violin() +
    geom_boxplot(width = 0.1) +
    labs(y = &quot;Fraction correct actions&quot;, fill = &quot;noise:has_suggestions&quot;) +
    scale_fill_economist()</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-17-2.png' %}" width="1440" /></p>
<pre class="r"><code># Visualize the data facetted by the variables that we care about
p1 = gg_df %&gt;%
  filter(optimal_ax == &quot;correct&quot;) %&gt;%
  ggplot(aes(has_ax, num_optimal_ax / num_actions, fill=.fill_column)) +
    geom_boxplot() +
    geom_count() +
    facet_grid(rows = vars(noise_level), cols = vars(has_dx), labeller = label_both) +
    labs(y = &quot;Fraction correct actions&quot;) +
    scale_fill_economist() +
    theme(legend.position = &quot;left&quot;)

p2 = gg_df %&gt;%
  filter(optimal_ax == &quot;correct&quot;) %&gt;%
  ggplot(aes(has_dx, num_optimal_ax / num_actions, fill=.fill_column)) +
    geom_boxplot() +
    geom_count() +
    facet_grid(rows = vars(noise_level), cols = vars(has_ax), labeller = label_both) +
    labs(y = &quot;Fraction correct actions&quot;) +
    scale_fill_economist() +
    legend_none()

p3 = gg_df %&gt;%
  filter(optimal_ax == &quot;correct&quot;) %&gt;%
  mutate(noise_level = fct_rev(noise_level)) %&gt;%
  mutate(has_ax = factor(has_ax)) %&gt;%
  mutate(has_ax = fct_rev(has_ax)) %&gt;%
  ggplot(aes(noise_level, num_optimal_ax / num_actions, fill=.fill_column)) +
    geom_boxplot() +
    geom_count() +
    facet_grid(rows = vars(has_ax), cols = vars(has_dx), labeller = label_both) +
    labs(y = &quot;Fraction correct actions&quot;) +
    scale_fill_economist() +
    legend_none()

grid.arrange(p1, p2, p3, ncol = 3)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-17-3.png' %}" width="1440" /></p>
<pre class="r"><code># Visualize also the variation in each of the three levels that we care about
p1 = gg_df %&gt;%
  filter(optimal_ax == &#39;correct&#39;) %&gt;%
  ggplot(aes(has_ax, num_optimal_ax/num_actions)) +
    geom_violin() +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep=&quot;:&quot;), size = 2)) +
    stat_summary(fun.y = median, geom=&quot;point&quot;, shape=23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(color = &quot;AX:DX:Noise&quot;) +
    theme(legend.position = &quot;left&quot;) +
    guides(size = F)

p2 = gg_df %&gt;%
  filter(optimal_ax == &#39;correct&#39;) %&gt;%
  ggplot(aes(has_dx, num_optimal_ax/num_actions)) +
    geom_violin() +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep=&quot;:&quot;), size = 2)) +
    stat_summary(fun.y = median, geom=&quot;point&quot;, shape=23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    legend_none()

p3 = gg_df %&gt;%
  filter(optimal_ax == &#39;correct&#39;) %&gt;%
  ggplot(aes(noise_level, num_optimal_ax/num_actions)) +
    geom_violin() +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep=&quot;:&quot;), size = 2)) +
    stat_summary(fun.y = median, geom=&quot;point&quot;, shape=23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    scale_x_discrete(limits = rev(levels(gg_df$noise_level))) +
    legend_none()

grid.arrange(p1, p2, p3, ncol = 3)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-17-4.png' %}" width="1440" /></p>
</div>
<div id="model-1" class="section level2">
<h2>Model</h2>
<p>Based on the data, we assume the following structural model:</p>
<p><span class="math display">\[rax_{ij} = Bernoulli(p_{ij})\]</span> <span class="math display">\[\begin{aligned}
logit^{-1}(p_{ij}) &amp;= \beta_0 + \beta_{0i} + \beta_{ax}\text{ax}_i + \beta_{dx}\text{dx}_i + \beta_{noise}\text{noise}_i +\\
&amp;\beta_{ax:noise}\text{ax}_i\text{noise}_i + \beta_{dx:noise}\text{dx}_i\text{noise}_i + \\
&amp;\beta_{no}\text{no}_i + \mathbf{\beta_{demo}X_{demo,i}} + \beta_{state} \text{state}_{ij}
\end{aligned}\]</span> <span class="math display">\[\begin{aligned}
\beta_{.} &amp;\sim Normal(0, 10) \\
\beta_{0i} &amp;\sim Normal(0, \sigma_i) \\
\sigma_i &amp;\sim HalfStudent(3, 0, 10)
\end{aligned}\]</span></p>
<p>The prior for the <span class="math inline">\(\sigma_i\)</span> parameter is the default used in <code>brms</code>; I see no need to change it.</p>
<p>Model fitting:</p>
<pre class="r"><code># The null model
optimal_ax.model.null = brm(
  optimal_ax ~ 0 + Intercept + (1 | user_id),
  family = &quot;bernoulli&quot;,
  prior = c(set_prior(&quot;normal(0, 10)&quot;, class = &quot;b&quot;),
            set_prior(&quot;student_t(3, 0, 10)&quot;, class = &quot;sd&quot;)),
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
optimal_ax.model.null = add_criterion(optimal_ax.model.null, &quot;waic&quot;)
optimal_ax.model.null = add_criterion(optimal_ax.model.null, &quot;loo&quot;, reloo = T)
saveModel(optimal_ax.model.null)

# The trend model to see if there is a trend in the noise level variable
optimal_ax.model.t = brm(
  optimal_ax ~ 0 + Intercept + ((has_dx + has_ax) * noise_level) + (age_group + robot_experience + gender) + num_optimal + state_idx_rescaled + (1 | user_id),
  family = bernoulli,
  prior = c(set_prior(&quot;normal(0, 10)&quot;, class = &quot;b&quot;),
            set_prior(&quot;student_t(3, 0, 10)&quot;, class = &quot;sd&quot;)),
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
optimal_ax.model.t = add_criterion(optimal_ax.model.t, &quot;waic&quot;)
optimal_ax.model.t = add_criterion(optimal_ax.model.t, &quot;loo&quot;, reloo = T)
saveModel(optimal_ax.model.t)

# The values model, to see if specific values of the noise level variable are significant
optimal_ax.model.v = brm(
  optimal_ax ~ 0 + Intercept + ((has_dx + has_ax) * noise_level_f) + (age_group + robot_experience + gender) + num_optimal + state_idx_rescaled + (1 | user_id),
  family = bernoulli,
  prior = c(set_prior(&quot;normal(0, 10)&quot;, class = &quot;b&quot;),
            set_prior(&quot;student_t(3, 0, 10)&quot;, class = &quot;sd&quot;)),
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
optimal_ax.model.v = add_criterion(optimal_ax.model.v, &quot;waic&quot;)
optimal_ax.model.v = add_criterion(optimal_ax.model.v, &quot;loo&quot;, reloo = T)
saveModel(optimal_ax.model.v)</code></pre>
<pre class="r"><code># Load the models
optimal_ax.model.null = loadModel(&quot;optimal_ax.model.null&quot;)
optimal_ax.model.t = loadModel(&quot;optimal_ax.model.t&quot;)
optimal_ax.model.v = loadModel(&quot;optimal_ax.model.v&quot;)</code></pre>
<p>Model fitting results:</p>
<pre class="r"><code># print(tidy_stan(optimal_ax.model.null))
print(performance::r2(optimal_ax.model.null))</code></pre>
<pre><code>## # Bayesian R2 with Standard Error
##
##   Conditional R2: 0.281 [0.014]
##      Marginal R2: 0.000 [0.000]</code></pre>
<pre class="r"><code># Print the parameters, and some initial diagnostics
print(tidy_stan(optimal_ax.model.t, effects = &quot;fixed&quot;))</code></pre>
<pre><code>## Warning: &#39;tidy_stan&#39; is deprecated.
## Use &#39;parameters::model_parameters()&#39; instead.
## See help(&quot;Deprecated&quot;)</code></pre>
<pre><code>##
## Summary Statistics of Stan-Model
##
## # Fixed effects
##
##              Parameter Estimate Std.Error       HDI(89%)     pd  ESS Rhat MCSE
##            (Intercept)    -0.13      0.48 [-0.96,  0.63]  61.2% 2008 1.00 0.01
##                has_dx1    -0.09      0.17 [-0.35,  0.18]  70.9% 1822 1.00 0.00
##                has_ax1    -0.61      0.17 [-0.87, -0.34] 100.0% 1825 1.00 0.00
##          noise_level.L    -0.28      0.28 [-0.71,  0.13]  84.1% 1723 1.00 0.01
##          noise_level.Q     0.37      0.29 [-0.08,  0.88]  89.3% 1974 1.00 0.01
##             age_group1     0.08      0.39 [-0.56,  0.68]  58.9% 1957 1.00 0.01
##             age_group2    -0.22      0.27 [-0.71,  0.20]  79.4% 2196 1.00 0.01
##             age_group3    -0.01      0.29 [-0.44,  0.49]  50.8% 1995 1.00 0.01
##             age_group4     0.70      0.33 [ 0.17,  1.24]  98.3% 1980 1.00 0.01
##             age_group5    -0.59      0.41 [-1.27,  0.07]  92.0% 1985 1.00 0.01
##             age_group6     0.58      0.45 [-0.20,  1.25]  89.7% 2148 1.00 0.01
##      robot_experience1     0.98      0.27 [ 0.57,  1.42] 100.0% 2040 1.00 0.01
##      robot_experience2     0.86      0.36 [ 0.30,  1.46]  99.0% 1989 1.00 0.01
##      robot_experience3    -0.32      0.49 [-1.17,  0.41]  74.8% 2238 1.00 0.01
##      robot_experience4    -0.93      0.48 [-1.72, -0.18]  97.7% 2499 1.00 0.01
##                gender1    -0.25      0.15 [-0.48, -0.02]  96.4% 1980 1.00 0.00
##            num_optimal     0.06      0.09 [-0.08,  0.21]  76.6% 1919 1.00 0.00
##     state_idx_rescaled    -2.23      0.21 [-2.57, -1.90] 100.0% 6987 1.00 0.00
##  has_dx1.noise_level.L    -0.32      0.27 [-0.75,  0.11]  88.8% 1895 1.00 0.01
##  has_dx1.noise_level.Q     0.39      0.29 [-0.06,  0.87]  91.5% 1834 1.00 0.01
##  has_ax1.noise_level.L     0.22      0.27 [-0.24,  0.61]  78.2% 1781 1.00 0.01
##  has_ax1.noise_level.Q     0.03      0.30 [-0.47,  0.49]  54.8% 1917 1.00 0.01</code></pre>
<pre class="r"><code>print(performance::r2(optimal_ax.model.t))</code></pre>
<pre><code>## # Bayesian R2 with Standard Error
##
##   Conditional R2: 0.360 [0.013]
##      Marginal R2: 0.213 [0.018]</code></pre>
<pre class="r"><code>print(tidy_stan(optimal_ax.model.v, effects = &quot;fixed&quot;))</code></pre>
<pre><code>## Warning: &#39;tidy_stan&#39; is deprecated.
## Use &#39;parameters::model_parameters()&#39; instead.
## See help(&quot;Deprecated&quot;)</code></pre>
<pre><code>##
## Summary Statistics of Stan-Model
##
## # Fixed effects
##
##               Parameter Estimate Std.Error       HDI(89%)     pd  ESS Rhat MCSE
##             (Intercept)    -0.13      0.49 [-0.93,  0.68]  61.3% 1651 1.00 0.01
##                 has_dx1    -0.09      0.15 [-0.34,  0.15]  71.8% 1931 1.00 0.00
##                 has_ax1    -0.60      0.16 [-0.85, -0.33] 100.0% 1936 1.00 0.00
##          noise_level_f1     0.34      0.20 [ 0.01,  0.67]  95.0% 1894 1.00 0.00
##          noise_level_f2    -0.30      0.24 [-0.67,  0.10]  88.2% 1811 1.00 0.01
##              age_group1     0.09      0.40 [-0.54,  0.74]  59.0% 2069 1.00 0.01
##              age_group2    -0.22      0.29 [-0.67,  0.25]  77.6% 1910 1.00 0.01
##              age_group3    -0.01      0.29 [-0.48,  0.45]  51.5% 1930 1.00 0.01
##              age_group4     0.71      0.33 [ 0.17,  1.22]  98.4% 2070 1.00 0.01
##              age_group5    -0.59      0.42 [-1.24,  0.08]  92.2% 2087 1.00 0.01
##              age_group6     0.56      0.45 [-0.17,  1.29]  88.5% 2050 1.00 0.01
##       robot_experience1     0.99      0.26 [ 0.57,  1.40] 100.0% 2092 1.00 0.01
##       robot_experience2     0.84      0.37 [ 0.29,  1.43]  98.9% 2082 1.00 0.01
##       robot_experience3    -0.32      0.48 [-1.08,  0.49]  75.8% 2370 1.00 0.01
##       robot_experience4    -0.93      0.47 [-1.68, -0.18]  97.1% 2348 1.00 0.01
##                 gender1    -0.26      0.14 [-0.49, -0.03]  96.2% 1897 1.00 0.00
##             num_optimal     0.06      0.09 [-0.09,  0.20]  75.2% 1727 1.00 0.00
##      state_idx_rescaled    -2.23      0.21 [-2.52, -1.89] 100.0% 6377 1.00 0.00
##  has_dx1:noise_level_f1     0.39      0.21 [ 0.06,  0.73]  97.1% 1963 1.00 0.00
##  has_dx1:noise_level_f2    -0.32      0.24 [-0.69,  0.10]  90.4% 1725 1.00 0.01
##  has_ax1:noise_level_f1    -0.15      0.21 [-0.48,  0.20]  75.0% 1999 1.00 0.00
##  has_ax1:noise_level_f2    -0.03      0.24 [-0.46,  0.35]  55.3% 1520 1.00 0.01</code></pre>
<pre class="r"><code>print(performance::r2(optimal_ax.model.v))</code></pre>
<pre><code>## # Bayesian R2 with Standard Error
##
##   Conditional R2: 0.360 [0.013]
##      Marginal R2: 0.212 [0.018]</code></pre>
</div>
<div id="diagnostic-1" class="section level2">
<h2>Diagnostic</h2>
<pre class="r"><code># Alternative diagnostics: diagnostic_posterior(scenario_completed.model.v)

# LOO cross-validation results
print(optimal_ax.model.t$criteria$loo$estimates)</code></pre>
<pre><code>##            Estimate        SE
## elpd_loo -1080.8157 24.155576
## p_loo      152.9746  4.566275
## looic     2161.6314 48.311153</code></pre>
<pre class="r"><code>print(optimal_ax.model.v$criteria$loo$estimates)</code></pre>
<pre><code>##            Estimate        SE
## elpd_loo -1081.5580 24.132215
## p_loo      153.4446  4.581049
## looic     2163.1161 48.264430</code></pre>
<pre class="r"><code>print(loo_compare(optimal_ax.model.null, optimal_ax.model.t, optimal_ax.model.v))</code></pre>
<pre><code>##                       elpd_diff se_diff
## optimal_ax.model.t      0.0       0.0
## optimal_ax.model.v     -0.7       0.4
## optimal_ax.model.null -81.3      12.7</code></pre>
<pre class="r"><code># Diagnostics of the sampling process
par(mfrow=c(1,2))
plot(optimal_ax.model.t$criteria$loo, main = &quot;Trend Model&quot;)
plot(optimal_ax.model.v$criteria$loo, main = &quot;Values Model&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-21-1.png' %}" width="1440" /></p>
<pre class="r"><code># Include posterior predictive checks of the sampling (are sampling according
# to the data that we actually have?)
p1 = pp_check(optimal_ax.model.t, type = &quot;bars_grouped&quot;, group = &quot;has_ax&quot;, stat = &quot;median&quot;) + ggtitle(&quot;t:has_ax&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars_grouped&#39; by default.</code></pre>
<pre><code>## Warning: The following arguments were unrecognized and ignored: stat</code></pre>
<pre class="r"><code>p2 = pp_check(optimal_ax.model.t, type = &quot;bars_grouped&quot;, group = &quot;has_dx&quot;) + ggtitle(&quot;t:has_dx&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars_grouped&#39; by default.</code></pre>
<pre class="r"><code>p3 = pp_check(optimal_ax.model.t, type = &quot;bars_grouped&quot;, group = &quot;noise_level&quot;) + ggtitle(&quot;t:noise_level&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars_grouped&#39; by default.</code></pre>
<pre class="r"><code>grid.arrange(p1, p2, p3, nrow = 3)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-21-2.png' %}" width="1440" /></p>
<pre class="r"><code>mcmc_intervals(as.matrix(optimal_ax.model.t), regex_pars = &quot;b_|^shape&quot;, prob = 0.90, prob_outer = 0.95) +
  ggtitle(&quot;Trends Model&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-21-3.png' %}" width="1440" /></p>
<pre class="r"><code>p1 = pp_check(optimal_ax.model.v, type = &quot;bars_grouped&quot;, group = &quot;has_ax&quot;) + ggtitle(&quot;v:has_ax&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars_grouped&#39; by default.</code></pre>
<pre class="r"><code>p2 = pp_check(optimal_ax.model.v, type = &quot;bars_grouped&quot;, group = &quot;has_dx&quot;) + ggtitle(&quot;v:has_dx&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars_grouped&#39; by default.</code></pre>
<pre class="r"><code>p3 = pp_check(optimal_ax.model.v, type = &quot;bars_grouped&quot;, group = &quot;noise_level_f&quot;) + ggtitle(&quot;v:noise_level&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars_grouped&#39; by default.</code></pre>
<pre class="r"><code>grid.arrange(p1, p2, p3, nrow = 3)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-21-4.png' %}" width="1440" /></p>
<pre class="r"><code>mcmc_intervals(as.matrix(optimal_ax.model.v), regex_pars = &quot;b_|^shape&quot;, prob = 0.90, prob_outer = 0.95) +
  ggtitle(&quot;Values Model&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-21-5.png' %}" width="1440" /></p>
<pre class="r"><code># Plot predictions vs original
preds_p_df = optimal_ax.model.null %&gt;% predict() %&gt;% as_tibble()
p1 =
  preds_p_df %&gt;%
    add_column(optimal_ax = optimal_ax.model.null$data$optimal_ax) %&gt;%
    ggplot(aes(x = Estimate, y = optimal_ax, color = optimal_ax)) +
      geom_point() +
      labs(x = &quot;Predicted values&quot;) +
      ylim(0, 1) +
      xlim(0, 1) +
      ggtitle(&quot;Null Model&quot;)

preds_p_df = plot_df %&gt;%
  add_predicted_draws(optimal_ax.model.null) %&gt;%
  group_by(optimal_ax, .prediction) %&gt;%
  count()
p2 =
  preds_p_df %&gt;%
    ggplot(aes(x = optimal_ax, y = .prediction, fill = n / sum(preds_p_df$n))) +
      geom_tile() +
      geom_text(aes(label = sprintf(&quot;%1.2f&quot;, n / sum(preds_p_df$n))), size = 10, vjust = 1) +
      scale_fill_gradient(limits = c(0, 1))

preds_p_df = optimal_ax.model.t %&gt;% predict() %&gt;% as_tibble()
p3 =
  preds_p_df %&gt;%
    add_column(optimal_ax = optimal_ax.model.t$data$optimal_ax) %&gt;%
    ggplot(aes(x = Estimate, y = optimal_ax, color = optimal_ax)) +
      geom_point() +
      labs(x = &quot;Predicted values&quot;) +
      ylim(0, 1) +
      xlim(0, 1) +
      ggtitle(&quot;Trends Model&quot;)

preds_p_df = plot_df %&gt;%
  add_predicted_draws(optimal_ax.model.t) %&gt;%
  group_by(optimal_ax, .prediction) %&gt;%
  count()
p4 =
  preds_p_df %&gt;%
    ggplot(aes(x = optimal_ax, y = .prediction, fill = n / sum(preds_p_df$n))) +
      geom_tile() +
      geom_text(aes(label = sprintf(&quot;%1.2f&quot;, n / sum(preds_p_df$n))), size = 10, vjust = 1) +
      scale_fill_gradient(limits = c(0, 1))

# Plot predictions vs original
preds_p_df = optimal_ax.model.v %&gt;% predict() %&gt;% as_tibble()
p5 =
  preds_p_df %&gt;%
    add_column(optimal_ax = optimal_ax.model.v$data$optimal_ax) %&gt;%
    ggplot(aes(x = Estimate, y = optimal_ax, color = optimal_ax)) +
      geom_point() +
      labs(x = &quot;Predicted values&quot;) +
      ylim(0, 1) +
      xlim(0, 1) +
      ggtitle(&quot;Values Model&quot;)

preds_p_df = plot_df %&gt;%
  add_predicted_draws(optimal_ax.model.v) %&gt;%
  group_by(optimal_ax, .prediction) %&gt;%
  count()
p6 =
  preds_p_df %&gt;%
    ggplot(aes(x = optimal_ax, y = .prediction, fill = n / sum(preds_p_df$n))) +
      geom_tile() +
      geom_text(aes(label = sprintf(&quot;%1.2f&quot;, n / sum(preds_p_df$n))), size = 10, vjust = 1) +
      scale_fill_gradient(limits = c(0, 1))

grid.arrange(p1, p3, p5, p2, p4, p6, nrow = 2, ncol = 3)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-21-6.png' %}" width="1440" /></p>
</div>
<div id="inference-1" class="section level2">
<h2>Inference</h2>
<pre class="r"><code># Compute the bayes factor for each parameter value. Note: we compare
# here against the ROPE, and not the degenerate null hypothesis of 0
# Bayes-Factor values would be more significant in the latter case
# According to the Raftery, 1995 rules
# bf = 1 - 3: Weak
# bf = 3 - 20: Positive
# bf = 20 - 150: Strong
# bf &gt; 150: Very strong
print(bayesfactor_rope(optimal_ax.model.null, null = c(-0.055, 0.055)), n = 30)
print(bayesfactor_rope(optimal_ax.model.t, null = c(-0.055, 0.055)), n = 30)
print(bayesfactor_rope(optimal_ax.model.v, null = c(-0.055, 0.055)), n = 30)</code></pre>
<pre class="r"><code>options(digits = 3)
# Check the significance of each hypothesis
h_df = as_tibble(insight::get_parameters(optimal_ax.model.t)) %&gt;%
  transmute(
    Intercept = b_Intercept,
    has_ax_test = -2 * b_has_ax1,
    has_dx_test = -2 * b_has_dx1,
    noise_levelL_test = b_noise_level.L,
    noise_levelQ_test = b_noise_level.Q
  )

hyp_results = test_hypotheses(h_df, rope_values = c(-0.055, 0.055))
print(hyp_results %&gt;% select(Parameter, pd, HDI_low, HDI_high, ROPE_Percentage, ROPE_Equivalence) %&gt;% as.matrix())</code></pre>
<pre><code>##      Parameter           pd      HDI_low   HDI_high ROPE_Percentage
## [1,] &quot;Intercept&quot;         &quot;0.612&quot; &quot;-1.7733&quot; &quot;2.081&quot;  &quot;0.0882&quot;
## [2,] &quot;has_ax_test&quot;       &quot;1.000&quot; &quot; 0.0272&quot; &quot;2.398&quot;  &quot;0.0005&quot;
## [3,] &quot;has_dx_test&quot;       &quot;0.709&quot; &quot;-0.8912&quot; &quot;1.285&quot;  &quot;0.1095&quot;
## [4,] &quot;noise_levelL_test&quot; &quot;0.841&quot; &quot;-1.3635&quot; &quot;0.533&quot;  &quot;0.1010&quot;
## [5,] &quot;noise_levelQ_test&quot; &quot;0.893&quot; &quot;-0.6355&quot; &quot;1.528&quot;  &quot;0.0653&quot;
##      ROPE_Equivalence
## [1,] &quot;Undecided&quot;
## [2,] &quot;Rejected&quot;
## [3,] &quot;Undecided&quot;
## [4,] &quot;Undecided&quot;
## [5,] &quot;Undecided&quot;</code></pre>
<pre class="r"><code>h_df = as_tibble(insight::get_parameters(optimal_ax.model.v)) %&gt;%
  transmute(
    Intercept = b_Intercept,
    has_ax_test = -2 * b_has_ax1,
    has_dx_test = -2 * b_has_dx1,
    noise_level1_test = b_noise_level_f1,
    noise_level2_test = b_noise_level_f2,
    noise_level3_test = - b_noise_level_f1 - b_noise_level_f2
  )

hyp_results = test_hypotheses(h_df, rope_values = c(-0.055, 0.055))
print(hyp_results %&gt;% select(Parameter, pd, HDI_low, HDI_high, ROPE_Percentage, ROPE_Equivalence) %&gt;% as.matrix())</code></pre>
<pre><code>##      Parameter           pd      HDI_low  HDI_high ROPE_Percentage
## [1,] &quot;Intercept&quot;         &quot;0.613&quot; &quot;-1.800&quot; &quot;1.753&quot;  &quot;0.0815&quot;
## [2,] &quot;has_ax_test&quot;       &quot;1.000&quot; &quot;-0.194&quot; &quot;2.331&quot;  &quot;0.0000&quot;
## [3,] &quot;has_dx_test&quot;       &quot;0.718&quot; &quot;-0.977&quot; &quot;1.276&quot;  &quot;0.1163&quot;
## [4,] &quot;noise_level1_test&quot; &quot;0.950&quot; &quot;-0.382&quot; &quot;1.035&quot;  &quot;0.0545&quot;
## [5,] &quot;noise_level2_test&quot; &quot;0.882&quot; &quot;-1.154&quot; &quot;0.479&quot;  &quot;0.0843&quot;
## [6,] &quot;noise_level3_test&quot; &quot;0.572&quot; &quot;-0.907&quot; &quot;0.731&quot;  &quot;0.1725&quot;
##      ROPE_Equivalence
## [1,] &quot;Undecided&quot;
## [2,] &quot;Rejected&quot;
## [3,] &quot;Undecided&quot;
## [4,] &quot;Undecided&quot;
## [5,] &quot;Undecided&quot;
## [6,] &quot;Undecided&quot;</code></pre>
<pre class="r"><code>options(digits = 7)</code></pre>
<p><strong>Results</strong>:</p>
<ol style="list-style-type: decimal">
<li>AX suggestions
<ul>
<li>Positive effect pd = 100%</li>
<li>Median = 1.20, 89% CI [0.66, 1.70]</li>
<li>Small, Std.Median = 0.32</li>
<li>0.0% in ROPE</li>
</ul></li>
</ol>
<p>(The ROPE is defined as [-0.055, 0.055]. It is the range is suggested in the literature for logistic models and it corresponds to a probability range of 0.11. Therefore, a probability change of less than 0.055 is considered no different from a probability change of 0)</p>
</div>
<div id="posterior-plots-1" class="section level2">
<h2>Posterior Plots</h2>
<pre class="r"><code># A visualization data frame
gg_df = plot_df %&gt;%
    mutate(noise_level = fct_rev(noise_level)) %&gt;%
    mutate(optimal_ax = factor(optimal_ax)) %&gt;%
    mutate(user_id = factor(user_id)) %&gt;%
    mutate(optimal_ax = fct_recode(optimal_ax, incorrect=&quot;0&quot;, correct = &quot;1&quot;)) %&gt;%
    mutate(.fill_column = paste(noise_level, !(study_condition == &#39;BASELINE&#39;), sep = &quot;:&quot;)) %&gt;%
    count(user_id, study_condition, .fill_column, optimal_ax, num_actions, has_ax, has_dx, noise_level,
          name = &quot;num_optimal_ax&quot;)

# Simulate predictions on new data
p_df =
  plot_df %&gt;%
  data_grid(optimal_ax,
            has_ax,
            has_dx,
            noise_level_f,
            gender,
            age_group,
            robot_experience,
            num_optimal,
            state_idx_rescaled = seq_range(state_idx_rescaled, n = 4),
            .model = optimal_ax.model.v)

fits_p_df = p_df %&gt;% add_fitted_draws(optimal_ax.model.v,
                                      re_formula = NA,
                                      n = 20,
                                      seed = default_seed)
gc()</code></pre>
<pre><code>##            used  (Mb) gc trigger   (Mb)   max used    (Mb)
## Ncells  4310177 230.2   13084560  698.8   13084560   698.8
## Vcells 33831804 258.2 1197862071 9139.0 1458980758 11131.2</code></pre>
<pre class="r"><code># We don&#39;t want predicted draws because that&#39;s a raw count of ones and zeros; not the
# probability distribution
# preds_p_df = p_df %&gt;% add_predicted_draws(scenario_completed.model.v)

# Plot the posterior distributions
rope_value = gg_df %&gt;%
  filter(optimal_ax == &#39;correct&#39;, has_ax == F) %&gt;%
  summarise(.value = median(num_optimal_ax / num_actions)) %&gt;%
  pull(.value)
gg_df %&gt;%
  filter(optimal_ax == &#39;correct&#39;) %&gt;%
  ggplot(aes(x = has_ax, y = num_optimal_ax / num_actions)) +
    geom_rect(ymin = rope_value-0.055, ymax = rope_value+0.055,
              xmin = -Inf, xmax = Inf,
              color = &quot;transparent&quot;, fill = &quot;white&quot;) +
    annotate(&quot;text&quot;, x = 0.5, y = rope_value, label = &quot;ROPE&quot;, angle = 90) +
    stat_halfeye(aes(y = .value), data = fits_p_df, position = position_nudge(x = 0.1)) +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep = &quot;:&quot;)), size = 2) +
    stat_summary(fun.y = median, geom = &quot;point&quot;, shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(y = &quot;correct_ax&quot;, color = &quot;AX:DX:Noise&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-24-1.png' %}" width="672" /></p>
<pre class="r"><code>rope_value = gg_df %&gt;%
  filter(optimal_ax == &#39;correct&#39;, has_dx == F) %&gt;%
  summarise(.value = median(num_optimal_ax / num_actions)) %&gt;%
  pull(.value)
gg_df %&gt;%
  filter(optimal_ax == &#39;correct&#39;) %&gt;%
  ggplot(aes(x = has_dx, y = num_optimal_ax / num_actions)) +
    geom_rect(ymin = rope_value-0.055, ymax = rope_value+0.055,
              xmin = -Inf, xmax = Inf,
              color = &quot;transparent&quot;, fill = &quot;white&quot;) +
    annotate(&quot;text&quot;, x = 0.5, y = rope_value, label = &quot;ROPE&quot;, angle = 90) +
    stat_halfeye(aes(y = .value), data = fits_p_df, position = position_nudge(x = 0.1)) +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep = &quot;:&quot;)), size = 2) +
    stat_summary(fun.y = median, geom = &quot;point&quot;, shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(y = &quot;correct_ax&quot;, color = &quot;AX:DX:Noise&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-24-2.png' %}" width="672" /></p>
<pre class="r"><code>rope_value = gg_df %&gt;%
  filter(optimal_ax == &#39;correct&#39;, noise_level == &quot;0.0&quot;) %&gt;%
  summarise(.value = median(num_optimal_ax / num_actions)) %&gt;%
  pull(.value)
gg_df %&gt;%
  filter(optimal_ax == &#39;correct&#39;) %&gt;%
  mutate(noise_level = fct_rev(noise_level)) %&gt;%
  ggplot(aes(x = noise_level, y = num_optimal_ax / num_actions)) +
    geom_rect(ymin = rope_value-0.055, ymax = rope_value+0.055,
              xmin = -Inf, xmax = Inf,
              color = &quot;transparent&quot;, fill = &quot;white&quot;) +
    annotate(&quot;text&quot;, x = 0.5, y = rope_value, label = &quot;ROPE&quot;, angle = 90) +
    stat_halfeye(aes(y = .value, x = noise_level_f), data = fits_p_df, position = position_nudge(x = 0.1)) +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep = &quot;:&quot;)), size = 2) +
    stat_summary(fun.y = median, geom = &quot;point&quot;, shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(y = &quot;correct_ax&quot;, color = &quot;AX:DX:Noise&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-24-3.png' %}" width="672" /></p>
<pre class="r"><code># Remove the giant fits data frames
rm(fits_p_df, p_df, gg_df)</code></pre>
<pre class="r"><code>plot_df = actions %&gt;%
  select(X1, id, user_id, study_condition, start_condition, num_optimal,
         state_idx, state_idx_rescaled,
         age_group, gender, robot_experience,
         noise_level, noise_level_f, has_noise, has_dx, has_ax,
         scenario_completed, num_actions, optimal_ax) %&gt;%
  mutate(scenario_completed = factor(scenario_completed)) %&gt;%
  mutate(scenario_completed = fct_recode(scenario_completed, Unresolved=&quot;0&quot;, Resolved = &quot;1&quot;),
         has_ax = fct_recode(has_ax, &quot;No AX&quot;=&quot;FALSE&quot;, &quot;AX&quot;=&quot;TRUE&quot;),
         has_dx = fct_recode(has_dx, &quot;No DX&quot;=&quot;FALSE&quot;, &quot;DX&quot;=&quot;TRUE&quot;),
         noise_level = fct_recode(noise_level, &quot;Acc: 100%&quot;=&quot;0.0&quot;, &quot;Acc: 90%&quot;=&quot;1.0&quot;, &quot;Acc: 80%&quot;=&quot;2.0&quot;))

eff = fixef(optimal_ax.model.v)

gg_df = plot_df %&gt;%
  mutate(.fill_column = paste(noise_level, !(study_condition == &#39;BASELINE&#39;), sep = &quot;:&quot;)) %&gt;%
  count(user_id, study_condition, .fill_column, optimal_ax, num_actions, has_ax, has_dx, noise_level,
        name = &quot;num_optimal_ax&quot;) %&gt;%
  mutate(estimate = -1) %&gt;%
  mutate(estimate = if_else(has_ax == &quot;No AX&quot;, inv_logit_scaled(eff[&quot;Intercept&quot;, &quot;Estimate&quot;] + eff[&quot;has_ax1&quot;,&quot;Estimate&quot;]), estimate)) %&gt;%
  mutate(estimate = if_else(has_ax == &quot;AX&quot;, inv_logit_scaled(eff[&quot;Intercept&quot;, &quot;Estimate&quot;] - eff[&quot;has_ax1&quot;, &quot;Estimate&quot;]), estimate))

gg_df %&gt;%
  filter(optimal_ax == &quot;1&quot;) %&gt;%
  ggplot(aes(has_ax, num_optimal_ax / num_actions, group = has_ax, colour=has_ax)) +
    geom_count(aes(colour = NULL)) +
    geom_boxplot(aes(y = estimate)) +
    geom_signif(y_position = 1.07, xmin = 1, xmax = 2, annotation = &quot;*&quot;, textsize = 8, color = &quot;black&quot;) +
    scale_y_continuous(limits = c(0.0, 1.1), breaks = c(0.0, 0.5, 1.0)) +
    labs(y = NULL, x = NULL) +
    scale_colour_manual(values = c(&quot;#d62728&quot;, &quot;#2ca02c&quot;, &quot;#ff7f0e&quot;)) +
    theme_economist_white(base_size = 13) +
    theme(plot.background = element_rect(fill = &quot;white&quot;),
          axis.title.y = element_blank()) +
    legend_none()</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-25-1.png' %}" width="672" /></p>
</div>
</div>
<div id="rdx-rate-of-correct-diagnosis-selection" class="section level1">
<h1>RDX: Rate of Correct Diagnosis Selection</h1>
<p><strong>Did the user figure out the correct diagnoses for their situation?</strong></p>
<p>This is also a measure of Reliance on Suggestions. In the code, the variable might be referred to as <code>optimal_dx</code>, <code>correct_dx</code>, etc. depending on the version of the codebase</p>
<pre class="r"><code>plot_df = actions %&gt;%
  select(X1, id, user_id, study_condition, start_condition, num_optimal,
         state_idx, state_idx_rescaled,
         age_group, gender, robot_experience,
         noise_level, noise_level_f, has_noise, has_dx, has_ax,
         scenario_completed, num_actions, optimal_dx)

text_short(report(plot_df))</code></pre>
<pre><code>## Warning in report.numeric(model[[col]], median = median, centrality =
## centrality, : Variable `model[[col]]` contains only two different values.
## Consider converting it to a factor.

## Warning in report.numeric(model[[col]], median = median, centrality =
## centrality, : Variable `model[[col]]` contains only two different values.
## Consider converting it to a factor.</code></pre>
<pre><code>## The data contains 2115 observations of the following variables:
##   - X1: Mean = 1057.00, SD = 610.69, range = [0, 2114], 0 missing
##   - id: Mean = 1058.00, SD = 610.69, range = [1, 2115], 0 missing
##   - user_id: Mean = 102.24, SD = 56.97, range = [1, 200], 0 missing
##   - study_condition: 10 levels: DX_100, n = 247; AX_100, n = 152; DXAX_100, n = 193; DX_90, n = 243; AX_90, n = 217; DXAX_90, n = 183; DX_80, n = 256; AX_80, n = 204; DXAX_80, n = 189 and BASELINE, n = 231
##   - start_condition: 4 entries: kc.dt.occluding.above_mug.default.empty.dt, n = 672; kc.kc.occluding.default.default.empty.dt, n = 595; dt.kc.default.default.default.empty.kc, n = 487 and 1 other
##   - num_optimal: Mean = 4.78, SD = 1.56, range = [3, 7], 0 missing
##   - state_idx: Mean = 88.30, SD = 60.07, range = [1, 173], 0 missing
##   - state_idx_rescaled: Mean = 0.51, SD = 0.35, range = [0, 1], 0 missing
##   - age_group: 7 levels: 2, n = 226; 3, n = 506; 4, n = 473; 5, n = 283; 6, n = 222; 7, n = 170 and 8, n = 235
##   - gender: 2 levels: F, n = 834 and M, n = 1281
##   - robot_experience: 5 levels: 0, n = 1337; 1, n = 336; 2, n = 161; 3, n = 169 and 4, n = 112
##   - noise_level: 3 levels: 0.0, n = 823; 1.0, n = 643 and 2.0, n = 649
##   - noise_level_f: 3 levels: 0.0, n = 823; 1.0, n = 643 and 2.0, n = 649
##   - has_noise: 2 levels: FALSE, n = 823 and TRUE, n = 1292
##   - has_dx: 2 levels: FALSE, n = 804 and TRUE, n = 1311
##   - has_ax: 2 levels: FALSE, n = 977 and TRUE, n = 1138
##   - scenario_completed: Mean = 0.64, SD = 0.48, range = [0, 1], 0 missing
##   - num_actions: Mean = 13.92, SD = 5.86, range = [3, 20], 0 missing
##   - optimal_dx: Mean = 0.63, SD = 0.48, range = [0, 1], 0 missing</code></pre>
<div id="data-2" class="section level2">
<h2>Data</h2>
<pre class="r"><code>gg_df =
  plot_df %&gt;%
    mutate(noise_level = fct_rev(noise_level)) %&gt;%
    mutate(optimal_dx = factor(optimal_dx)) %&gt;%
    mutate(user_id = factor(user_id)) %&gt;%
    mutate(optimal_dx = fct_recode(optimal_dx, incorrect=&quot;0&quot;, correct = &quot;1&quot;)) %&gt;%
    mutate(.fill_column = paste(noise_level, !(study_condition == &#39;BASELINE&#39;), sep = &quot;:&quot;)) %&gt;%
    count(user_id, study_condition, .fill_column, optimal_dx, num_actions, has_ax, has_dx, noise_level,
          name = &quot;num_optimal_dx&quot;)

# Plot by the study condition
gg_df %&gt;%
  ggplot(aes(user_id, num_optimal_dx / num_actions, fill=optimal_dx)) +
    geom_bar(stat=&quot;identity&quot;) +
    geom_hline(yintercept = 0.5) +
    facet_wrap(vars(study_condition), nrow = 1, scales = &quot;free_y&quot;) +
    labs(y = &quot;Fraction correct diagnoses&quot;) +
    coord_flip() +
    scale_fill_economist()</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-28-1.png' %}" width="1440" /></p>
<pre class="r"><code>gg_df %&gt;%
  filter(optimal_dx == &quot;correct&quot;) %&gt;%
  ggplot(aes(study_condition, num_optimal_dx / num_actions, fill=.fill_column)) +
    geom_violin() +
    geom_boxplot(width = 0.1) +
    labs(y = &quot;Fraction correct diagnoses&quot;, fill = &quot;noise:has_suggestions&quot;) +
    scale_fill_economist()</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-28-2.png' %}" width="1440" /></p>
<pre class="r"><code># Visualize the data facetted by the variables that we care about
p1 = gg_df %&gt;%
  filter(optimal_dx == &quot;correct&quot;) %&gt;%
  ggplot(aes(has_ax, num_optimal_dx / num_actions, fill=.fill_column)) +
    geom_boxplot() +
    geom_count() +
    facet_grid(rows = vars(noise_level), cols = vars(has_dx), labeller = label_both) +
    labs(y = &quot;Fraction correct diagnoses&quot;) +
    scale_fill_economist() +
    theme(legend.position = &quot;left&quot;)

p2 = gg_df %&gt;%
  filter(optimal_dx == &quot;correct&quot;) %&gt;%
  ggplot(aes(has_dx, num_optimal_dx / num_actions, fill=.fill_column)) +
    geom_boxplot() +
    geom_count() +
    facet_grid(rows = vars(noise_level), cols = vars(has_ax), labeller = label_both) +
    labs(y = &quot;Fraction correct diagnoses&quot;) +
    scale_fill_economist() +
    legend_none()

p3 = gg_df %&gt;%
  filter(optimal_dx == &quot;correct&quot;) %&gt;%
  mutate(noise_level = fct_rev(noise_level)) %&gt;%
  mutate(has_ax = factor(has_ax)) %&gt;%
  mutate(has_ax = fct_rev(has_ax)) %&gt;%
  ggplot(aes(noise_level, num_optimal_dx / num_actions, fill=.fill_column)) +
    geom_boxplot() +
    geom_count() +
    facet_grid(rows = vars(has_ax), cols = vars(has_dx), labeller = label_both) +
    labs(y = &quot;Fraction correct diagnoses&quot;) +
    scale_fill_economist() +
    legend_none()

grid.arrange(p1, p2, p3, ncol = 3)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-28-3.png' %}" width="1440" /></p>
<pre class="r"><code># Visualize also the variation in each of the three levels that we care about
p1 = gg_df %&gt;%
  filter(optimal_dx == &#39;correct&#39;) %&gt;%
  ggplot(aes(has_ax, num_optimal_dx/num_actions)) +
    geom_violin() +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep=&quot;:&quot;), size = 2)) +
    stat_summary(fun.y = median, geom=&quot;point&quot;, shape=23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(color = &quot;AX:DX:Noise&quot;) +
    theme(legend.position = &quot;left&quot;) +
    guides(size = F)

p2 = gg_df %&gt;%
  filter(optimal_dx == &#39;correct&#39;) %&gt;%
  ggplot(aes(has_dx, num_optimal_dx/num_actions)) +
    geom_violin() +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep=&quot;:&quot;), size = 2)) +
    stat_summary(fun.y = median, geom=&quot;point&quot;, shape=23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    legend_none()

p3 = gg_df %&gt;%
  filter(optimal_dx == &#39;correct&#39;) %&gt;%
  ggplot(aes(noise_level, num_optimal_dx/num_actions)) +
    geom_violin() +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep=&quot;:&quot;), size = 2)) +
    stat_summary(fun.y = median, geom=&quot;point&quot;, shape=23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    scale_x_discrete(limits = rev(levels(gg_df$noise_level))) +
    legend_none()

grid.arrange(p1, p2, p3, ncol = 3)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-28-4.png' %}" width="1440" /></p>
</div>
<div id="model-2" class="section level2">
<h2>Model</h2>
<p>Based on the data, we assume the following structural model:</p>
<p><span class="math display">\[rdx_{ij} = Bernoulli(p_{ij})\]</span> <span class="math display">\[\begin{aligned}
logit^{-1}(p_{ij}) &amp;= \beta_0 + \beta_{0i} + \beta_{ax}\text{ax}_i + \beta_{dx}\text{dx}_i + \beta_{noise}\text{noise}_i +\\
&amp;\beta_{ax:noise}\text{ax}_i\text{noise}_i + \beta_{dx:noise}\text{dx}_i\text{noise}_i + \\
&amp;\beta_{no}\text{no}_i + \mathbf{\beta_{demo}X_{demo,i}} + \beta_{state} \text{state}_{ij}
\end{aligned}\]</span> <span class="math display">\[\begin{aligned}
\beta_{.} &amp;\sim Normal(0, 10) \\
\beta_{0i} &amp;\sim Normal(0, \sigma_i) \\
\sigma_i &amp;\sim HalfStudent(3, 0, 10)
\end{aligned}\]</span></p>
<p>The prior for the <span class="math inline">\(\sigma_i\)</span> parameter is the default used in <code>brms</code>; I see no need to change it.</p>
<p>Model fitting:</p>
<pre class="r"><code># The null model
optimal_dx.model.null = brm(
  optimal_dx ~ 0 + Intercept + (1 | user_id),
  family = &quot;bernoulli&quot;,
  prior = c(set_prior(&quot;normal(0, 10)&quot;, class = &quot;b&quot;),
            set_prior(&quot;student_t(3, 0, 10)&quot;, class = &quot;sd&quot;)),
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
optimal_dx.model.null = add_criterion(optimal_dx.model.null, &quot;waic&quot;)
optimal_dx.model.null = add_criterion(optimal_dx.model.null, &quot;loo&quot;, reloo = T)
saveModel(optimal_dx.model.null)

# The trend model to see if there is a trend in the noise level variable
optimal_dx.model.t = brm(
  optimal_dx ~ 0 + Intercept + ((has_dx + has_ax) * noise_level) + (age_group + robot_experience + gender) + num_optimal + state_idx_rescaled + (1 | user_id),
  family = bernoulli,
  prior = c(set_prior(&quot;normal(0, 10)&quot;, class = &quot;b&quot;),
            set_prior(&quot;student_t(3, 0, 10)&quot;, class = &quot;sd&quot;)),
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
optimal_dx.model.t = add_criterion(optimal_dx.model.t, &quot;waic&quot;)
optimal_dx.model.t = add_criterion(optimal_dx.model.t, &quot;loo&quot;, reloo = T)
saveModel(optimal_dx.model.t)

# The values model, to see if specific values of the noise level variable are significant
optimal_dx.model.v = brm(
  optimal_dx ~ 0 + Intercept + ((has_dx + has_ax) * noise_level_f) + (age_group + robot_experience + gender) + num_optimal + state_idx_rescaled + (1 | user_id),
  family = bernoulli,
  prior = c(set_prior(&quot;normal(0, 10)&quot;, class = &quot;b&quot;),
            set_prior(&quot;student_t(3, 0, 10)&quot;, class = &quot;sd&quot;)),
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
optimal_dx.model.v = add_criterion(optimal_dx.model.v, &quot;waic&quot;)
optimal_dx.model.v = add_criterion(optimal_dx.model.v, &quot;loo&quot;, reloo = T)
saveModel(optimal_dx.model.v)</code></pre>
<pre class="r"><code># Load the models
optimal_dx.model.null = loadModel(&quot;optimal_dx.model.null&quot;)
optimal_dx.model.t = loadModel(&quot;optimal_dx.model.t&quot;)
optimal_dx.model.v = loadModel(&quot;optimal_dx.model.v&quot;)</code></pre>
<p>Model fitting results:</p>
<pre class="r"><code># print(tidy_stan(optimal_dx.model.null))
print(performance::r2(optimal_dx.model.null))</code></pre>
<pre><code>## # Bayesian R2 with Standard Error
##
##   Conditional R2: 0.228 [0.015]
##      Marginal R2: 0.000 [0.000]</code></pre>
<pre class="r"><code># Print the parameters, and some initial diagnostics
print(tidy_stan(optimal_dx.model.t, effects = &quot;fixed&quot;))</code></pre>
<pre><code>## Warning: &#39;tidy_stan&#39; is deprecated.
## Use &#39;parameters::model_parameters()&#39; instead.
## See help(&quot;Deprecated&quot;)</code></pre>
<pre><code>##
## Summary Statistics of Stan-Model
##
## # Fixed effects
##
##              Parameter Estimate Std.Error       HDI(89%)    pd  ESS Rhat MCSE
##            (Intercept)     0.24      0.41 [-0.36,  0.93] 72.2% 1203 1.01 0.01
##                has_dx1    -0.28      0.12 [-0.50, -0.10] 98.6% 1423 1.00 0.00
##                has_ax1    -0.10      0.13 [-0.32,  0.10] 78.1% 1408 1.00 0.00
##          noise_level.L    -0.42      0.21 [-0.76, -0.09] 97.8% 1471 1.00 0.01
##          noise_level.Q    -0.02      0.23 [-0.36,  0.37] 52.7% 1475 1.00 0.01
##             age_group1     0.47      0.31 [-0.04,  0.99] 93.0% 1294 1.01 0.01
##             age_group2    -0.21      0.23 [-0.57,  0.15] 82.8% 1504 1.00 0.01
##             age_group3    -0.21      0.23 [-0.58,  0.14] 82.9% 1372 1.00 0.01
##             age_group4     0.17      0.27 [-0.26,  0.59] 72.8% 1478 1.00 0.01
##             age_group5    -0.35      0.32 [-0.83,  0.18] 86.6% 1478 1.00 0.01
##             age_group6     0.32      0.34 [-0.24,  0.89] 80.6% 1378 1.00 0.01
##      robot_experience1     0.53      0.20 [ 0.19,  0.84] 99.5% 1327 1.00 0.01
##      robot_experience2     0.33      0.29 [-0.13,  0.76] 87.8% 1325 1.00 0.01
##      robot_experience3    -0.11      0.37 [-0.70,  0.49] 62.2% 1290 1.00 0.01
##      robot_experience4    -0.28      0.37 [-0.89,  0.31] 78.3% 1331 1.00 0.01
##                gender1    -0.16      0.11 [-0.33,  0.04] 91.2% 1496 1.00 0.00
##            num_optimal     0.01      0.07 [-0.10,  0.13] 58.0% 1244 1.01 0.00
##     state_idx_rescaled     0.42      0.17 [ 0.14,  0.68] 99.3% 3411 1.00 0.00
##  has_dx1.noise_level.L     0.13      0.20 [-0.18,  0.48] 74.7% 1580 1.00 0.01
##  has_dx1.noise_level.Q    -0.01      0.23 [-0.37,  0.34] 51.7% 1594 1.00 0.01
##  has_ax1.noise_level.L    -0.11      0.21 [-0.43,  0.22] 70.0% 1515 1.00 0.01
##  has_ax1.noise_level.Q    -0.00      0.23 [-0.36,  0.38] 50.2% 1408 1.00 0.01</code></pre>
<pre class="r"><code>print(performance::r2(optimal_dx.model.t))</code></pre>
<pre><code>## # Bayesian R2 with Standard Error
##
##   Conditional R2: 0.241 [0.014]
##      Marginal R2: 0.085 [0.019]</code></pre>
<pre class="r"><code>print(tidy_stan(optimal_dx.model.v, effects = &quot;fixed&quot;))</code></pre>
<pre><code>## Warning: &#39;tidy_stan&#39; is deprecated.
## Use &#39;parameters::model_parameters()&#39; instead.
## See help(&quot;Deprecated&quot;)</code></pre>
<pre><code>##
## Summary Statistics of Stan-Model
##
## # Fixed effects
##
##               Parameter Estimate Std.Error       HDI(89%)    pd  ESS Rhat MCSE
##             (Intercept)     0.24      0.41 [-0.41,  0.88] 72.7% 1171 1.00 0.01
##                 has_dx1    -0.28      0.12 [-0.46, -0.05] 98.5% 1413 1.00 0.00
##                 has_ax1    -0.09      0.13 [-0.30,  0.10] 76.8% 1377 1.00 0.00
##          noise_level_f1     0.30      0.16 [ 0.05,  0.57] 96.2% 1623 1.00 0.00
##          noise_level_f2     0.01      0.19 [-0.28,  0.31] 52.4% 1459 1.00 0.00
##              age_group1     0.48      0.31 [ 0.00,  1.02] 93.7% 1534 1.00 0.01
##              age_group2    -0.22      0.23 [-0.58,  0.14] 83.7% 1473 1.00 0.01
##              age_group3    -0.21      0.23 [-0.57,  0.17] 81.7% 1500 1.00 0.01
##              age_group4     0.16      0.27 [-0.28,  0.57] 72.0% 1420 1.00 0.01
##              age_group5    -0.34      0.32 [-0.87,  0.15] 85.5% 1565 1.00 0.01
##              age_group6     0.32      0.36 [-0.28,  0.85] 80.3% 1531 1.00 0.01
##       robot_experience1     0.53      0.20 [ 0.23,  0.88] 99.4% 1510 1.00 0.01
##       robot_experience2     0.32      0.28 [-0.10,  0.78] 88.5% 1495 1.00 0.01
##       robot_experience3    -0.14      0.38 [-0.75,  0.45] 64.5% 1432 1.00 0.01
##       robot_experience4    -0.30      0.37 [-0.93,  0.24] 79.9% 1596 1.00 0.01
##                 gender1    -0.16      0.11 [-0.35,  0.02] 92.4% 1629 1.00 0.00
##             num_optimal     0.01      0.07 [-0.11,  0.13] 57.5% 1242 1.00 0.00
##      state_idx_rescaled     0.42      0.17 [ 0.17,  0.72] 99.1% 3327 1.00 0.00
##  has_dx1:noise_level_f1    -0.11      0.17 [-0.39,  0.14] 74.5% 1329 1.00 0.00
##  has_dx1:noise_level_f2     0.03      0.19 [-0.28,  0.34] 56.0% 1310 1.01 0.01
##  has_ax1:noise_level_f1     0.08      0.16 [-0.17,  0.35] 69.7% 1312 1.00 0.00
##  has_ax1:noise_level_f2     0.01      0.19 [-0.28,  0.33] 53.0% 1369 1.00 0.01</code></pre>
<pre class="r"><code>print(performance::r2(optimal_dx.model.v))</code></pre>
<pre><code>## # Bayesian R2 with Standard Error
##
##   Conditional R2: 0.241 [0.014]
##      Marginal R2: 0.086 [0.019]</code></pre>
</div>
<div id="diagnostic-2" class="section level2">
<h2>Diagnostic</h2>
<pre class="r"><code># Alternative diagnostics: diagnostic_posterior(scenario_completed.model.v)

# LOO cross-validation results
print(optimal_dx.model.t$criteria$loo$estimates)</code></pre>
<pre><code>##            Estimate        SE
## elpd_loo -1206.0267 21.549109
## p_loo      131.6111  3.815714
## looic     2412.0533 43.098217</code></pre>
<pre class="r"><code>print(optimal_dx.model.v$criteria$loo$estimates)</code></pre>
<pre><code>##            Estimate        SE
## elpd_loo -1206.8597 21.606338
## p_loo      132.6614  3.843095
## looic     2413.7193 43.212675</code></pre>
<pre class="r"><code>print(loo_compare(optimal_dx.model.null, optimal_dx.model.t, optimal_dx.model.v))</code></pre>
<pre><code>##                       elpd_diff se_diff
## optimal_dx.model.null  0.0       0.0
## optimal_dx.model.t    -3.1       3.8
## optimal_dx.model.v    -3.9       3.8</code></pre>
<pre class="r"><code># Diagnostics of the sampling process
par(mfrow=c(1,2))
plot(optimal_dx.model.t$criteria$loo, main = &quot;Trend Model&quot;)
plot(optimal_dx.model.v$criteria$loo, main = &quot;Values Model&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-32-1.png' %}" width="1440" /></p>
<pre class="r"><code># Include posterior predictive checks of the sampling (are sampling according
# to the data that we actually have?)
p1 = pp_check(optimal_dx.model.t, type = &quot;bars_grouped&quot;, group = &quot;has_ax&quot;, stat = &quot;median&quot;) + ggtitle(&quot;t:has_ax&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars_grouped&#39; by default.</code></pre>
<pre><code>## Warning: The following arguments were unrecognized and ignored: stat</code></pre>
<pre class="r"><code>p2 = pp_check(optimal_dx.model.t, type = &quot;bars_grouped&quot;, group = &quot;has_dx&quot;) + ggtitle(&quot;t:has_dx&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars_grouped&#39; by default.</code></pre>
<pre class="r"><code>p3 = pp_check(optimal_dx.model.t, type = &quot;bars_grouped&quot;, group = &quot;noise_level&quot;) + ggtitle(&quot;t:noise_level&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars_grouped&#39; by default.</code></pre>
<pre class="r"><code>grid.arrange(p1, p2, p3, nrow = 3)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-32-2.png' %}" width="1440" /></p>
<pre class="r"><code>mcmc_intervals(as.matrix(optimal_dx.model.t), regex_pars = &quot;b_|^shape&quot;, prob = 0.90, prob_outer = 0.95) +
  ggtitle(&quot;Trends Model&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-32-3.png' %}" width="1440" /></p>
<pre class="r"><code>p1 = pp_check(optimal_dx.model.v, type = &quot;bars_grouped&quot;, group = &quot;has_ax&quot;) + ggtitle(&quot;v:has_ax&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars_grouped&#39; by default.</code></pre>
<pre class="r"><code>p2 = pp_check(optimal_dx.model.v, type = &quot;bars_grouped&quot;, group = &quot;has_dx&quot;) + ggtitle(&quot;v:has_dx&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars_grouped&#39; by default.</code></pre>
<pre class="r"><code>p3 = pp_check(optimal_dx.model.v, type = &quot;bars_grouped&quot;, group = &quot;noise_level_f&quot;) + ggtitle(&quot;v:noise_level&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars_grouped&#39; by default.</code></pre>
<pre class="r"><code>grid.arrange(p1, p2, p3, nrow = 3)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-32-4.png' %}" width="1440" /></p>
<pre class="r"><code>mcmc_intervals(as.matrix(optimal_dx.model.v), regex_pars = &quot;b_|^shape&quot;, prob = 0.90, prob_outer = 0.95) +
  ggtitle(&quot;Values Model&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-32-5.png' %}" width="1440" /></p>
<pre class="r"><code># Plot predictions vs original
preds_p_df = optimal_dx.model.null %&gt;% predict() %&gt;% as_tibble()
p1 =
  preds_p_df %&gt;%
    add_column(optimal_dx = optimal_dx.model.null$data$optimal_dx) %&gt;%
    ggplot(aes(x = Estimate, y = optimal_dx, color = optimal_dx)) +
      geom_point() +
      labs(x = &quot;Predicted values&quot;) +
      ylim(0, 1) +
      xlim(0, 1) +
      ggtitle(&quot;Null Model&quot;)

preds_p_df = plot_df %&gt;%
  add_predicted_draws(optimal_dx.model.null) %&gt;%
  group_by(optimal_dx, .prediction) %&gt;%
  count()
p2 =
  preds_p_df %&gt;%
    ggplot(aes(x = optimal_dx, y = .prediction, fill = n / sum(preds_p_df$n))) +
      geom_tile() +
      geom_text(aes(label = sprintf(&quot;%1.2f&quot;, n / sum(preds_p_df$n))), size = 10, vjust = 1) +
      scale_fill_gradient(limits = c(0, 1))

preds_p_df = optimal_dx.model.t %&gt;% predict() %&gt;% as_tibble()
p3 =
  preds_p_df %&gt;%
    add_column(optimal_dx = optimal_dx.model.t$data$optimal_dx) %&gt;%
    ggplot(aes(x = Estimate, y = optimal_dx, color = optimal_dx)) +
      geom_point() +
      labs(x = &quot;Predicted values&quot;) +
      ylim(0, 1) +
      xlim(0, 1) +
      ggtitle(&quot;Trends Model&quot;)

preds_p_df = plot_df %&gt;%
  add_predicted_draws(optimal_dx.model.t) %&gt;%
  group_by(optimal_dx, .prediction) %&gt;%
  count()
p4 =
  preds_p_df %&gt;%
    ggplot(aes(x = optimal_dx, y = .prediction, fill = n / sum(preds_p_df$n))) +
      geom_tile() +
      geom_text(aes(label = sprintf(&quot;%1.2f&quot;, n / sum(preds_p_df$n))), size = 10, vjust = 1) +
      scale_fill_gradient(limits = c(0, 1))

# Plot predictions vs original
preds_p_df = optimal_dx.model.v %&gt;% predict() %&gt;% as_tibble()
p5 =
  preds_p_df %&gt;%
    add_column(optimal_dx = optimal_dx.model.v$data$optimal_dx) %&gt;%
    ggplot(aes(x = Estimate, y = optimal_dx, color = optimal_dx)) +
      geom_point() +
      labs(x = &quot;Predicted values&quot;) +
      ylim(0, 1) +
      xlim(0, 1) +
      ggtitle(&quot;Values Model&quot;)

preds_p_df = plot_df %&gt;%
  add_predicted_draws(optimal_dx.model.v) %&gt;%
  group_by(optimal_dx, .prediction) %&gt;%
  count()
p6 =
  preds_p_df %&gt;%
    ggplot(aes(x = optimal_dx, y = .prediction, fill = n / sum(preds_p_df$n))) +
      geom_tile() +
      geom_text(aes(label = sprintf(&quot;%1.2f&quot;, n / sum(preds_p_df$n))), size = 10, vjust = 1) +
      scale_fill_gradient(limits = c(0, 1))

grid.arrange(p1, p3, p5, p2, p4, p6, nrow = 2, ncol = 3)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-32-6.png' %}" width="1440" /></p>
</div>
<div id="inference-2" class="section level2">
<h2>Inference</h2>
<pre class="r"><code># Compute the bayes factor for each parameter value. Note: we compare
# here against the ROPE, and not the degenerate null hypothesis of 0
# Bayes-Factor values would be more significant in the latter case
# According to the Raftery, 1995 rules
# bf = 1 - 3: Weak
# bf = 3 - 20: Positive
# bf = 20 - 150: Strong
# bf &gt; 150: Very strong
print(bayesfactor_rope(optimal_dx.model.null, null = c(-0.055, 0.055)), n = 30)
print(bayesfactor_rope(optimal_dx.model.t, null = c(-0.055, 0.055)), n = 30)
print(bayesfactor_rope(optimal_dx.model.v, null = c(-0.055, 0.055)), n = 30)</code></pre>
<pre class="r"><code>options(digits = 3)
# Check the significance of each hypothesis
h_df = as_tibble(insight::get_parameters(optimal_dx.model.t)) %&gt;%
  transmute(
    Intercept = b_Intercept,
    has_ax_test = -2 * b_has_ax1,
    has_dx_test = -2 * b_has_dx1,
    noise_levelL_test = b_noise_level.L,
    noise_levelQ_test = b_noise_level.Q
  )

hyp_results = test_hypotheses(h_df, rope_values = c(-0.055, 0.055))
print(hyp_results %&gt;% select(Parameter, pd, HDI_low, HDI_high, ROPE_Percentage, ROPE_Equivalence) %&gt;% as.matrix())</code></pre>
<pre><code>##      Parameter           pd      HDI_low  HDI_high ROPE_Percentage
## [1,] &quot;Intercept&quot;         &quot;0.722&quot; &quot;-1.102&quot; &quot;1.631&quot;  &quot;0.0853&quot;
## [2,] &quot;has_ax_test&quot;       &quot;0.781&quot; &quot;-0.915&quot; &quot;1.157&quot;  &quot;0.1285&quot;
## [3,] &quot;has_dx_test&quot;       &quot;0.986&quot; &quot;-0.381&quot; &quot;1.407&quot;  &quot;0.0155&quot;
## [4,] &quot;noise_levelL_test&quot; &quot;0.978&quot; &quot;-1.085&quot; &quot;0.327&quot;  &quot;0.0260&quot;
## [5,] &quot;noise_levelQ_test&quot; &quot;0.527&quot; &quot;-0.948&quot; &quot;0.691&quot;  &quot;0.1920&quot;
##      ROPE_Equivalence
## [1,] &quot;Undecided&quot;
## [2,] &quot;Undecided&quot;
## [3,] &quot;Rejected&quot;
## [4,] &quot;Undecided&quot;
## [5,] &quot;Undecided&quot;</code></pre>
<pre class="r"><code>h_df = as_tibble(insight::get_parameters(optimal_dx.model.v)) %&gt;%
  transmute(
    Intercept = b_Intercept,
    has_ax_test = -2 * b_has_ax1,
    has_dx_test = -2 * b_has_dx1,
    noise_level1_test = b_noise_level_f1,
    noise_level2_test = b_noise_level_f2,
    noise_level3_test = - b_noise_level_f1 - b_noise_level_f2
  )

hyp_results = test_hypotheses(h_df, rope_values = c(-0.055, 0.055))
print(hyp_results %&gt;% select(Parameter, pd, HDI_low, HDI_high, ROPE_Percentage, ROPE_Equivalence) %&gt;% as.matrix())</code></pre>
<pre><code>##      Parameter           pd      HDI_low  HDI_high ROPE_Percentage
## [1,] &quot;Intercept&quot;         &quot;0.727&quot; &quot;-1.103&quot; &quot;1.636&quot;  &quot;0.0912&quot;
## [2,] &quot;has_ax_test&quot;       &quot;0.768&quot; &quot;-0.897&quot; &quot;1.189&quot;  &quot;0.1315&quot;
## [3,] &quot;has_dx_test&quot;       &quot;0.985&quot; &quot;-0.310&quot; &quot;1.597&quot;  &quot;0.0160&quot;
## [4,] &quot;noise_level1_test&quot; &quot;0.962&quot; &quot;-0.354&quot; &quot;0.949&quot;  &quot;0.0517&quot;
## [5,] &quot;noise_level2_test&quot; &quot;0.524&quot; &quot;-0.754&quot; &quot;0.848&quot;  &quot;0.2335&quot;
## [6,] &quot;noise_level3_test&quot; &quot;0.954&quot; &quot;-0.989&quot; &quot;0.295&quot;  &quot;0.0587&quot;
##      ROPE_Equivalence
## [1,] &quot;Undecided&quot;
## [2,] &quot;Undecided&quot;
## [3,] &quot;Rejected&quot;
## [4,] &quot;Undecided&quot;
## [5,] &quot;Undecided&quot;
## [6,] &quot;Undecided&quot;</code></pre>
<pre class="r"><code>options(digits = 7)</code></pre>
<p><strong>Results</strong>:</p>
<ol style="list-style-type: decimal">
<li>DX suggestions
<ul>
<li>Positive effect pd = 98.5%</li>
<li>Median = 0.56, 89% CI [0.10, 0.92]</li>
<li>Small, Std.Median = 0.24</li>
<li>1.6% in ROPE</li>
</ul></li>
<li>Noise Level Linear
<ul>
<li>Negative (negative slope) pd = 97.8%</li>
<li>Median = -0.42, 89% CI [-0.76, -0.09]</li>
<li>Small, Std.Median = 0.21</li>
<li>2.6% in ROPE (n.s.)</li>
</ul></li>
</ol>
<p>(The ROPE is defined as [-0.055, 0.055]. It is the range is suggested in the literature for logistic models and it corresponds to a probability range of 0.11. Therefore, a probability change of less than 0.055 is considered no different from a probability change of 0)</p>
</div>
<div id="posterior-plots-2" class="section level2">
<h2>Posterior Plots</h2>
<pre class="r"><code># A visualization data frame
gg_df = plot_df %&gt;%
    mutate(noise_level = fct_rev(noise_level)) %&gt;%
    mutate(optimal_dx = factor(optimal_dx)) %&gt;%
    mutate(user_id = factor(user_id)) %&gt;%
    mutate(optimal_dx = fct_recode(optimal_dx, incorrect=&quot;0&quot;, correct = &quot;1&quot;)) %&gt;%
    mutate(.fill_column = paste(noise_level, !(study_condition == &#39;BASELINE&#39;), sep = &quot;:&quot;)) %&gt;%
    count(user_id, study_condition, .fill_column, optimal_dx, num_actions, has_ax, has_dx, noise_level,
          name = &quot;num_optimal_dx&quot;)

# Simulate predictions on new data
p_df =
  plot_df %&gt;%
  data_grid(optimal_dx,
            has_ax,
            has_dx,
            noise_level_f,
            gender,
            age_group,
            robot_experience,
            num_optimal,
            state_idx_rescaled = seq_range(state_idx_rescaled, n = 4),
            .model = optimal_dx.model.v)

fits_p_df = p_df %&gt;% add_fitted_draws(optimal_dx.model.v,
                                      re_formula = NA,
                                      n = 20,
                                      seed = default_seed)
gc()</code></pre>
<pre><code>##            used  (Mb) gc trigger   (Mb)   max used    (Mb)
## Ncells  4075246 217.7   10467648  559.1   13084560   698.8
## Vcells 33556313 256.1 1151096409 8782.2 1458980758 11131.2</code></pre>
<pre class="r"><code>pars_p_df = optimal_dx.model.t %&gt;%
  extract_draws(newdata = p_df %&gt;% rename(noise_level = noise_level_f),
                re_formula = NA,
                nsamples = 100)
gc()</code></pre>
<pre><code>##            used  (Mb) gc trigger   (Mb)   max used    (Mb)
## Ncells  4075335 217.7   10467648  559.1   13084560   698.8
## Vcells 34032496 259.7  920877128 7025.8 1458980758 11131.2</code></pre>
<pre class="r"><code># We don&#39;t want predicted draws because that&#39;s a raw count of ones and zeros; not the
# probability distribution
# preds_p_df = p_df %&gt;% add_predicted_draws(scenario_completed.model.v)

# Plot the posterior distributions
rope_value = gg_df %&gt;%
  filter(optimal_dx == &#39;correct&#39;, has_ax == F) %&gt;%
  summarise(.value = median(num_optimal_dx / num_actions)) %&gt;%
  pull(.value)
gg_df %&gt;%
  filter(optimal_dx == &#39;correct&#39;) %&gt;%
  ggplot(aes(x = has_ax, y = num_optimal_dx / num_actions)) +
    geom_rect(ymin = rope_value-0.055, ymax = rope_value+0.055,
              xmin = -Inf, xmax = Inf,
              color = &quot;transparent&quot;, fill = &quot;white&quot;) +
    annotate(&quot;text&quot;, x = 0.5, y = rope_value, label = &quot;ROPE&quot;, angle = 90) +
    stat_halfeye(aes(y = .value), data = fits_p_df, position = position_nudge(x = 0.1)) +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep = &quot;:&quot;)), size = 2) +
    stat_summary(fun.y = median, geom = &quot;point&quot;, shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(y = &quot;correct_ax&quot;, color = &quot;AX:DX:Noise&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-35-1.png' %}" width="672" /></p>
<pre class="r"><code>rope_value = gg_df %&gt;%
  filter(optimal_dx == &#39;correct&#39;, has_dx == F) %&gt;%
  summarise(.value = median(num_optimal_dx / num_actions)) %&gt;%
  pull(.value)
gg_df %&gt;%
  filter(optimal_dx == &#39;correct&#39;) %&gt;%
  ggplot(aes(x = has_dx, y = num_optimal_dx / num_actions)) +
    geom_rect(ymin = rope_value-0.055, ymax = rope_value+0.055,
              xmin = -Inf, xmax = Inf,
              color = &quot;transparent&quot;, fill = &quot;white&quot;) +
    annotate(&quot;text&quot;, x = 0.5, y = rope_value, label = &quot;ROPE&quot;, angle = 90) +
    stat_halfeye(aes(y = .value), data = fits_p_df, position = position_nudge(x = 0.1)) +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep = &quot;:&quot;)), size = 2) +
    stat_summary(fun.y = median, geom = &quot;point&quot;, shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(y = &quot;correct_ax&quot;, color = &quot;AX:DX:Noise&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-35-2.png' %}" width="672" /></p>
<pre class="r"><code>rope_value = gg_df %&gt;%
  filter(optimal_dx == &#39;correct&#39;, noise_level == &quot;0.0&quot;) %&gt;%
  summarise(.value = median(num_optimal_dx / num_actions)) %&gt;%
  pull(.value)
gg_df %&gt;%
  filter(optimal_dx == &#39;correct&#39;) %&gt;%
  mutate(noise_level = fct_rev(noise_level)) %&gt;%
  ggplot(aes(x = noise_level, y = num_optimal_dx / num_actions)) +
    geom_rect(ymin = rope_value-0.055, ymax = rope_value+0.055,
              xmin = -Inf, xmax = Inf,
              color = &quot;transparent&quot;, fill = &quot;white&quot;) +
    annotate(&quot;text&quot;, x = 0.5, y = rope_value, label = &quot;ROPE&quot;, angle = 90) +
    stat_halfeye(aes(y = .value, x = noise_level_f), data = fits_p_df, position = position_nudge(x = 0.1)) +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep = &quot;:&quot;)), size = 2) +
    stat_summary(fun.y = median, geom = &quot;point&quot;, shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(y = &quot;correct_ax&quot;, color = &quot;AX:DX:Noise&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-35-3.png' %}" width="672" /></p>
<pre class="r"><code># Create a plot to show the trend
pars_p_df_X = pars_p_df$dpars$mu$fe$X[,c(&quot;Intercept&quot;, &quot;noise_level.L&quot;)]
pars_p_df_b = pars_p_df$dpars$mu$fe$b[,c(&quot;b_Intercept&quot;, &quot;b_noise_level.L&quot;)]
pars_p_df_y = pars_p_df_X %*% t(pars_p_df_b)
pars_p_df_y = inv_logit_scaled(pars_p_df_y)
p_df =
  p_df %&gt;%
  bind_cols(as_tibble(pars_p_df_y)) %&gt;%
  gather(key = &quot;.sample&quot;, value = &quot;.value&quot;, V1:V100) %&gt;%
  rename(noise_level = noise_level_f)

rope_value = gg_df %&gt;%
  filter(optimal_dx == &#39;correct&#39;, noise_level == &quot;0.0&quot;) %&gt;%
  summarise(.value = median(num_optimal_dx / num_actions)) %&gt;%
  pull(.value)
gg_df %&gt;%
  filter(optimal_dx == &#39;correct&#39;) %&gt;%
  mutate(noise_level = fct_rev(noise_level)) %&gt;%
  ggplot(aes(x = noise_level, y = num_optimal_dx / num_actions)) +
    geom_rect(ymin = rope_value-0.055, ymax = rope_value+0.055,
              xmin = -Inf, xmax = Inf,
              color = &quot;transparent&quot;, fill = &quot;white&quot;) +
    annotate(&quot;text&quot;, x = 0.5, y = rope_value, label = &quot;ROPE&quot;, angle = 90) +
    stat_halfeye(aes(y = .value, x = noise_level_f), data = fits_p_df, position = position_nudge(x = 0.1)) +
    geom_line(aes(y = .value, group = .sample, alpha = 0.3), data = p_df) +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep = &quot;:&quot;)), size = 2) +
    stat_summary(fun.y = median, geom = &quot;point&quot;, shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(y = &quot;scenario_completed&quot;, color = &quot;AX:DX:Noise&quot;) +
    guides(alpha = F)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-35-4.png' %}" width="672" /></p>
<pre class="r"><code># Remove the giant fits data frames
rm(fits_p_df, pars_p_df, pars_p_df_b, pars_p_df_X, pars_p_df_y, p_df, gg_df)</code></pre>
<pre class="r"><code>plot_df = actions %&gt;%
  select(X1, id, user_id, study_condition, start_condition, num_optimal,
         state_idx, state_idx_rescaled,
         age_group, gender, robot_experience,
         noise_level, noise_level_f, has_noise, has_dx, has_ax,
         scenario_completed, num_actions, optimal_dx) %&gt;%
  mutate(scenario_completed = factor(scenario_completed)) %&gt;%
  mutate(scenario_completed = fct_recode(scenario_completed, Unresolved=&quot;0&quot;, Resolved = &quot;1&quot;),
         has_ax = fct_recode(has_ax, &quot;No AX&quot;=&quot;FALSE&quot;, &quot;AX&quot;=&quot;TRUE&quot;),
         has_dx = fct_recode(has_dx, &quot;No DX&quot;=&quot;FALSE&quot;, &quot;DX&quot;=&quot;TRUE&quot;),
         noise_level = fct_recode(noise_level, &quot;Acc: 100%&quot;=&quot;0.0&quot;, &quot;Acc: 90%&quot;=&quot;1.0&quot;, &quot;Acc: 80%&quot;=&quot;2.0&quot;))

eff = fixef(optimal_dx.model.v)

gg_df = plot_df %&gt;%
  mutate(.fill_column = paste(noise_level, !(study_condition == &#39;BASELINE&#39;), sep = &quot;:&quot;)) %&gt;%
  count(user_id, study_condition, .fill_column, optimal_dx, num_actions, has_ax, has_dx, noise_level,
        name = &quot;num_correct_dx&quot;) %&gt;%
  mutate(estimate = -1) %&gt;%
  mutate(estimate = if_else(has_dx == &quot;No DX&quot;, inv_logit_scaled(eff[&quot;Intercept&quot;, &quot;Estimate&quot;] + eff[&quot;has_dx1&quot;,&quot;Estimate&quot;]), estimate)) %&gt;%
  mutate(estimate = if_else(has_dx == &quot;DX&quot;, inv_logit_scaled(eff[&quot;Intercept&quot;, &quot;Estimate&quot;] - eff[&quot;has_dx1&quot;, &quot;Estimate&quot;]), estimate))

gg_df %&gt;%
  filter(optimal_dx == &quot;1&quot;) %&gt;%
  ggplot(aes(has_dx, num_correct_dx / num_actions, group = has_dx, colour=has_dx)) +
    geom_count(aes(colour = NULL)) +
    geom_boxplot(aes(y = estimate)) +
    geom_signif(y_position = 1.15, xmin = 1, xmax = 2, annotation = &quot;*&quot;, textsize = 8, color = &quot;black&quot;) +
    scale_y_continuous(limits = c(0.0, 1.25), breaks = c(0.0, 0.5, 1.0)) +
    labs(y = NULL, x = NULL) +
    scale_colour_manual(values = c(&quot;#d62728&quot;, &quot;#2ca02c&quot;, &quot;#ff7f0e&quot;)) +
    theme_economist_white(base_size = 13) +
    theme(plot.background = element_rect(fill = &quot;white&quot;),
          axis.title.y = element_blank()) +
    legend_none()</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-36-1.png' %}" width="672" /></p>
<pre class="r"><code>gg_df = plot_df %&gt;%
  mutate(.fill_column = paste(noise_level, !(study_condition == &#39;BASELINE&#39;), sep = &quot;:&quot;)) %&gt;%
  count(user_id, study_condition, .fill_column, optimal_dx, num_actions, has_ax, has_dx, noise_level,
        name = &quot;num_correct_dx&quot;) %&gt;%
  mutate(estimate = -1) %&gt;%
  mutate(estimate = if_else(noise_level == &quot;Acc: 100%&quot;, inv_logit_scaled(eff[&quot;noise_level_f1&quot;,&quot;Estimate&quot;] + eff[&quot;Intercept&quot;, &quot;Estimate&quot;]), estimate)) %&gt;%
  mutate(estimate = if_else(noise_level == &quot;Acc: 90%&quot;, inv_logit_scaled(eff[&quot;noise_level_f2&quot;,&quot;Estimate&quot;] + eff[&quot;Intercept&quot;, &quot;Estimate&quot;]), estimate)) %&gt;%
  mutate(estimate = if_else(noise_level == &quot;Acc: 80%&quot;, inv_logit_scaled(eff[&quot;Intercept&quot;, &quot;Estimate&quot;] - eff[&quot;noise_level_f1&quot;,&quot;Estimate&quot;] - eff[&quot;noise_level_f2&quot;,&quot;Estimate&quot;]), estimate)) %&gt;%
  mutate(noise_level = fct_rev(noise_level))

gg_df %&gt;%
  filter(optimal_dx == &quot;1&quot;) %&gt;%
  ggplot(aes(noise_level, num_correct_dx / num_actions, group = noise_level, colour = noise_level)) +
    geom_hline(yintercept = inv_logit_scaled(eff[&quot;Intercept&quot;, &quot;Estimate&quot;]),
               size = .7, linetype = &quot;dashed&quot;, color = &quot;grey&quot;) +
    geom_count() +
    geom_boxplot(aes(y = estimate)) +
    annotate(&quot;segment&quot;,
             x = 3,
             y = inv_logit_scaled(eff[&quot;noise_level_f1&quot;,&quot;Estimate&quot;] + eff[&quot;Intercept&quot;, &quot;Estimate&quot;]),
             xend = 2,
             yend = inv_logit_scaled(eff[&quot;noise_level_f2&quot;,&quot;Estimate&quot;] + eff[&quot;Intercept&quot;, &quot;Estimate&quot;]),
             colour = &quot;black&quot;) +
    annotate(&quot;segment&quot;,
             x = 1,
             y = inv_logit_scaled(eff[&quot;Intercept&quot;, &quot;Estimate&quot;] - eff[&quot;noise_level_f1&quot;,&quot;Estimate&quot;] - eff[&quot;noise_level_f2&quot;,&quot;Estimate&quot;]),
             xend = 2,
             yend = inv_logit_scaled(eff[&quot;noise_level_f2&quot;,&quot;Estimate&quot;] + eff[&quot;Intercept&quot;, &quot;Estimate&quot;]),
             color = &quot;black&quot;) +
    annotate(&quot;text&quot;,
             x = 3.5,
             y = inv_logit_scaled(eff[&quot;Intercept&quot;, &quot;Estimate&quot;] - 0.85),
             label = &quot;Mean of all levels&quot;,
             angle = 90,
             group = NA,
             colour = &quot;black&quot;) +
    scale_y_continuous(limits = c(0.0, 1.0), breaks = c(0.0, 0.5, 1.0)) +
    labs(y = &quot;Fraction of participants that resolved fault&quot;, x = NULL) +
    scale_colour_manual(values = c(&quot;#d62728&quot;, &quot;#2ca02c&quot;, &quot;#ff7f0e&quot;)) +
    scale_size_area(max_size = 1) +
    theme_economist_white(base_size = 13) +
    theme(plot.background = element_rect(fill = &quot;white&quot;),
          axis.title.y = element_blank()) +
    legend_none()</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-36-2.png' %}" width="672" /></p>
</div>
</div>
<div id="cax-compliance-with-ax-suggestions" class="section level1">
<h1>CAX: Compliance with AX Suggestions</h1>
<p><strong>Did the user follow the AX suggestions that were provided to them?</strong></p>
<p>In the code, the variable might be referred to as <code>chose_ax</code>, <code>follow_ax</code>, etc. depending on the version of the codebase</p>
<pre class="r"><code>plot_df = actions %&gt;%
  filter(has_ax == T) %&gt;%
  select(X1, id, user_id, study_condition, start_condition, num_optimal,
         state_idx, state_idx_rescaled,
         age_group, gender, robot_experience,
         noise_level, noise_level_f, has_noise, has_dx, has_ax,
         scenario_completed, num_actions, chose_ax)

text_short(report(plot_df))</code></pre>
<pre><code>## Warning in report.numeric(model[[col]], median = median, centrality =
## centrality, : Variable `model[[col]]` contains only two different values.
## Consider converting it to a factor.

## Warning in report.numeric(model[[col]], median = median, centrality =
## centrality, : Variable `model[[col]]` contains only two different values.
## Consider converting it to a factor.</code></pre>
<pre><code>## The data contains 1138 observations of the following variables:
##   - X1: Mean = 1086.81, SD = 590.95, range = [0, 2114], 0 missing
##   - id: Mean = 1094.46, SD = 586.19, range = [1, 2115], 0 missing
##   - user_id: Mean = 105.04, SD = 55.05, range = [1, 200], 0 missing
##   - study_condition: 10 levels: DX_100, n = 0; AX_100, n = 152; DXAX_100, n = 193; DX_90, n = 0; AX_90, n = 217; DXAX_90, n = 183; DX_80, n = 0; AX_80, n = 204; DXAX_80, n = 189 and BASELINE, n = 0
##   - start_condition: 4 entries: kc.dt.occluding.above_mug.default.empty.dt, n = 366; kc.kc.occluding.default.default.empty.dt, n = 342; dt.kc.default.default.default.empty.kc, n = 247 and 1 other
##   - num_optimal: Mean = 4.80, SD = 1.55, range = [3, 7], 0 missing
##   - state_idx: Mean = 83.45, SD = 58.84, range = [1, 173], 0 missing
##   - state_idx_rescaled: Mean = 0.48, SD = 0.34, range = [0, 1], 0 missing
##   - age_group: 7 levels: 2, n = 160; 3, n = 327; 4, n = 220; 5, n = 140; 6, n = 127; 7, n = 49 and 8, n = 115
##   - gender: 2 levels: F, n = 453 and M, n = 685
##   - robot_experience: 5 levels: 0, n = 665; 1, n = 260; 2, n = 55; 3, n = 89 and 4, n = 69
##   - noise_level: 3 levels: 0.0, n = 345; 1.0, n = 400 and 2.0, n = 393
##   - noise_level_f: 3 levels: 0.0, n = 345; 1.0, n = 400 and 2.0, n = 393
##   - has_noise: 2 levels: FALSE, n = 345 and TRUE, n = 793
##   - has_dx: 2 levels: FALSE, n = 573 and TRUE, n = 565
##   - has_ax: 2 levels: FALSE, n = 0 and TRUE, n = 1138
##   - scenario_completed: Mean = 0.79, SD = 0.41, range = [0, 1], 0 missing
##   - num_actions: Mean = 12.51, SD = 5.71, range = [3, 20], 0 missing
##   - chose_ax: Mean = 0.54, SD = 0.50, range = [0, 1], 0 missing</code></pre>
<div id="data-3" class="section level2">
<h2>Data</h2>
<pre class="r"><code>gg_df =
  plot_df %&gt;%
    mutate(noise_level = fct_rev(noise_level)) %&gt;%
    mutate(chose_ax = factor(chose_ax)) %&gt;%
    mutate(user_id = factor(user_id)) %&gt;%
    mutate(chose_ax = fct_recode(chose_ax, no_follow=&quot;0&quot;, follow = &quot;1&quot;)) %&gt;%
    mutate(.fill_column = paste(noise_level, !(study_condition == &#39;BASELINE&#39;), sep = &quot;:&quot;)) %&gt;%
    count(user_id, study_condition, .fill_column, chose_ax, num_actions, has_ax, has_dx, noise_level,
          name = &quot;num_follow_ax&quot;)

# Plot by the study condition
gg_df %&gt;%
  ggplot(aes(user_id, num_follow_ax / num_actions, fill=chose_ax)) +
    geom_bar(stat=&quot;identity&quot;) +
    geom_hline(yintercept = 0.5) +
    facet_wrap(vars(study_condition), nrow = 1, scales = &quot;free_y&quot;) +
    labs(y = &quot;Fraction followed AX&quot;) +
    coord_flip() +
    scale_fill_economist()</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-39-1.png' %}" width="1440" /></p>
<pre class="r"><code>gg_df %&gt;%
  filter(chose_ax == &quot;follow&quot;) %&gt;%
  ggplot(aes(study_condition, num_follow_ax / num_actions, fill=.fill_column)) +
    geom_violin() +
    geom_boxplot(width = 0.1) +
    labs(y = &quot;Fraction followed AX&quot;, fill = &quot;noise:has_suggestions&quot;) +
    scale_fill_economist()</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-39-2.png' %}" width="1440" /></p>
<pre class="r"><code># Visualize the data facetted by the variables that we care about
p2 = gg_df %&gt;%
  filter(chose_ax == &quot;follow&quot;) %&gt;%
  ggplot(aes(has_dx, num_follow_ax / num_actions, fill=.fill_column)) +
    geom_boxplot() +
    geom_count() +
    facet_grid(rows = vars(noise_level), labeller = label_both) +
    labs(y = &quot;Fraction followed AX&quot;) +
    scale_fill_economist() +
    theme(legend.position = &quot;right&quot;)

p3 = gg_df %&gt;%
  filter(chose_ax == &quot;follow&quot;) %&gt;%
  mutate(noise_level = fct_rev(noise_level)) %&gt;%
  ggplot(aes(noise_level, num_follow_ax / num_actions, fill=.fill_column)) +
    geom_boxplot() +
    geom_count() +
    facet_grid(cols = vars(has_dx), labeller = label_both) +
    labs(y = &quot;Fraction followed AX&quot;) +
    scale_fill_economist() +
    legend_none()

grid.arrange(p2, p3, ncol = 2)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-39-3.png' %}" width="1440" /></p>
<pre class="r"><code># Visualize also the variation in each of the three levels that we care about
p2 = gg_df %&gt;%
  filter(chose_ax == &#39;follow&#39;) %&gt;%
  ggplot(aes(has_dx, num_follow_ax/num_actions)) +
    geom_violin() +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep=&quot;:&quot;), size = 2)) +
    stat_summary(fun.y = median, geom=&quot;point&quot;, shape=23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    theme(legend.position = &quot;right&quot;)

p3 = gg_df %&gt;%
  filter(chose_ax == &#39;follow&#39;) %&gt;%
  ggplot(aes(noise_level, num_follow_ax/num_actions)) +
    geom_violin() +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep=&quot;:&quot;), size = 2)) +
    stat_summary(fun.y = median, geom=&quot;point&quot;, shape=23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    scale_x_discrete(limits = rev(levels(gg_df$noise_level))) +
    legend_none()

grid.arrange(p2, p3, ncol = 2)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-39-4.png' %}" width="1440" /></p>
</div>
<div id="model-3" class="section level2">
<h2>Model</h2>
<p>Based on the data, we assume the following structural model:</p>
<p><span class="math display">\[cax_{ij} = Bernoulli(p_{ij})\]</span> <span class="math display">\[\begin{aligned}
logit^{-1}(p_{ij}) &amp;= \beta_0 + \beta_{0i} + \beta_{dx}\text{dx}_i + \beta_{noise}\text{noise}_i + \beta_{dx:noise}\text{dx}_i\text{noise}_i + \\
&amp;\beta_{no}\text{no}_i + \mathbf{\beta_{demo}X_{demo,i}} + \beta_{state} \text{state}_{ij}
\end{aligned}\]</span> <span class="math display">\[\begin{aligned}
\beta_{.} &amp;\sim Normal(0, 10) \\
\beta_{0i} &amp;\sim Normal(0, \sigma_i) \\
\sigma_i &amp;\sim HalfStudent(3, 0, 10)
\end{aligned}\]</span></p>
<p>The prior for the <span class="math inline">\(\sigma_i\)</span> parameter is the default used in <code>brms</code>; I see no need to change it.</p>
<p>Model fitting:</p>
<pre class="r"><code># The null model
chose_ax.model.null = brm(
  chose_ax ~ 0 + Intercept + (1 | user_id),
  family = &quot;bernoulli&quot;,
  prior = c(set_prior(&quot;normal(0, 10)&quot;, class = &quot;b&quot;),
            set_prior(&quot;student_t(3, 0, 10)&quot;, class = &quot;sd&quot;)),
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
chose_ax.model.null = add_criterion(chose_ax.model.null, &quot;waic&quot;)
chose_ax.model.null = add_criterion(chose_ax.model.null, &quot;loo&quot;, reloo = T)
saveModel(chose_ax.model.null)

# The trend model to see if there is a trend in the noise level variable
chose_ax.model.t = brm(
  chose_ax ~ 0 + Intercept + (has_dx * noise_level) + (age_group + robot_experience + gender) + num_optimal + state_idx_rescaled + (1 | user_id),
  family = bernoulli,
  prior = c(set_prior(&quot;normal(0, 10)&quot;, class = &quot;b&quot;),
            set_prior(&quot;student_t(3, 0, 10)&quot;, class = &quot;sd&quot;)),
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
chose_ax.model.t = add_criterion(chose_ax.model.t, &quot;waic&quot;)
chose_ax.model.t = add_criterion(chose_ax.model.t, &quot;loo&quot;, reloo = T)
saveModel(chose_ax.model.t)

# The values model, to see if specific values of the noise level variable are significant
chose_ax.model.v = brm(
  chose_ax ~ 0 + Intercept + (has_dx * noise_level_f) + (age_group + robot_experience + gender) + num_optimal + state_idx_rescaled + (1 | user_id),
  family = bernoulli,
  prior = c(set_prior(&quot;normal(0, 10)&quot;, class = &quot;b&quot;),
            set_prior(&quot;student_t(3, 0, 10)&quot;, class = &quot;sd&quot;)),
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
chose_ax.model.v = add_criterion(chose_ax.model.v, &quot;waic&quot;)
chose_ax.model.v = add_criterion(chose_ax.model.v, &quot;loo&quot;, reloo = T)
saveModel(chose_ax.model.v)</code></pre>
<pre class="r"><code># Load the models
chose_ax.model.null = loadModel(&quot;chose_ax.model.null&quot;)
chose_ax.model.t = loadModel(&quot;chose_ax.model.t&quot;)
chose_ax.model.v = loadModel(&quot;chose_ax.model.v&quot;)</code></pre>
<p>Model fitting results:</p>
<pre class="r"><code># print(tidy_stan(chose_ax.model.null))
print(performance::r2(chose_ax.model.null))</code></pre>
<pre><code>## # Bayesian R2 with Standard Error
##
##   Conditional R2: 0.154 [0.020]
##      Marginal R2: 0.000 [0.000]</code></pre>
<pre class="r"><code># Print the parameters, and some initial diagnostics
print(tidy_stan(chose_ax.model.t, effects = &quot;fixed&quot;))</code></pre>
<pre><code>## Warning: &#39;tidy_stan&#39; is deprecated.
## Use &#39;parameters::model_parameters()&#39; instead.
## See help(&quot;Deprecated&quot;)</code></pre>
<pre><code>##
## Summary Statistics of Stan-Model
##
## # Fixed effects
##
##              Parameter Estimate Std.Error       HDI(89%)     pd  ESS Rhat MCSE
##            (Intercept)     0.59      0.47 [-0.15,  1.32]  90.3% 1822 1.00 0.01
##                has_dx1    -0.13      0.13 [-0.35,  0.07]  84.3% 2062 1.00 0.00
##          noise_level.L    -0.40      0.23 [-0.77, -0.05]  96.3% 2052 1.00 0.00
##          noise_level.Q     0.18      0.23 [-0.15,  0.58]  79.3% 1905 1.00 0.01
##             age_group1     0.12      0.34 [-0.40,  0.69]  64.0% 1840 1.00 0.01
##             age_group2    -0.19      0.25 [-0.60,  0.21]  77.5% 1984 1.00 0.01
##             age_group3    -0.12      0.28 [-0.61,  0.29]  66.2% 2132 1.00 0.01
##             age_group4     0.53      0.33 [ 0.01,  1.03]  95.8% 2102 1.00 0.01
##             age_group5    -0.76      0.39 [-1.34, -0.12]  97.1% 1991 1.00 0.01
##             age_group6     0.37      0.59 [-0.56,  1.30]  73.9% 2062 1.00 0.01
##      robot_experience1     0.62      0.26 [ 0.21,  1.01]  99.1% 1960 1.00 0.01
##      robot_experience2     0.36      0.31 [-0.18,  0.83]  87.4% 1879 1.00 0.01
##      robot_experience3     0.04      0.54 [-0.83,  0.86]  52.9% 1931 1.00 0.01
##      robot_experience4    -0.24      0.44 [-0.99,  0.42]  70.7% 1678 1.00 0.01
##                gender1    -0.21      0.13 [-0.43, -0.00]  94.5% 2112 1.00 0.00
##            num_optimal     0.06      0.08 [-0.07,  0.19]  76.0% 1830 1.00 0.00
##     state_idx_rescaled    -1.54      0.23 [-1.90, -1.17] 100.0% 3916 1.00 0.00
##  has_dx1.noise_level.L    -0.24      0.23 [-0.61,  0.11]  85.9% 2266 1.00 0.00
##  has_dx1.noise_level.Q     0.22      0.22 [-0.13,  0.59]  84.5% 2237 1.00 0.00</code></pre>
<pre class="r"><code>print(performance::r2(chose_ax.model.t))</code></pre>
<pre><code>## # Bayesian R2 with Standard Error
##
##   Conditional R2: 0.225 [0.018]
##      Marginal R2: 0.129 [0.020]</code></pre>
<pre class="r"><code>print(tidy_stan(chose_ax.model.v, effects = &quot;fixed&quot;))</code></pre>
<pre><code>## Warning: &#39;tidy_stan&#39; is deprecated.
## Use &#39;parameters::model_parameters()&#39; instead.
## See help(&quot;Deprecated&quot;)</code></pre>
<pre><code>##
## Summary Statistics of Stan-Model
##
## # Fixed effects
##
##               Parameter Estimate Std.Error       HDI(89%)     pd  ESS Rhat MCSE
##             (Intercept)     0.58      0.46 [-0.15,  1.29]  90.4% 1161 1.00 0.01
##                 has_dx1    -0.13      0.13 [-0.36,  0.06]  83.5% 1987 1.00 0.00
##          noise_level_f1     0.37      0.19 [ 0.06,  0.68]  97.2% 1405 1.00 0.01
##          noise_level_f2    -0.15      0.19 [-0.46,  0.15]  78.8% 1249 1.00 0.01
##              age_group1     0.12      0.35 [-0.36,  0.70]  64.3% 1821 1.00 0.01
##              age_group2    -0.18      0.26 [-0.61,  0.22]  77.1% 1693 1.00 0.01
##              age_group3    -0.13      0.29 [-0.62,  0.34]  66.9% 1362 1.00 0.01
##              age_group4     0.55      0.33 [ 0.03,  1.09]  95.2% 1561 1.00 0.01
##              age_group5    -0.76      0.40 [-1.41, -0.11]  96.7% 1305 1.01 0.01
##              age_group6     0.41      0.57 [-0.48,  1.37]  75.8% 1674 1.00 0.01
##       robot_experience1     0.63      0.25 [ 0.23,  1.06]  99.3% 1317 1.00 0.01
##       robot_experience2     0.37      0.31 [-0.14,  0.86]  88.5% 1251 1.00 0.01
##       robot_experience3     0.02      0.54 [-0.79,  0.95]  51.2% 1487 1.00 0.01
##       robot_experience4    -0.23      0.44 [-0.97,  0.44]  70.2% 1477 1.00 0.01
##                 gender1    -0.21      0.14 [-0.43,  0.01]  94.3% 1785 1.00 0.00
##             num_optimal     0.06      0.08 [-0.07,  0.19]  78.0% 1227 1.00 0.00
##      state_idx_rescaled    -1.54      0.24 [-1.92, -1.19] 100.0% 3017 1.00 0.00
##  has_dx1:noise_level_f1     0.27      0.18 [-0.04,  0.55]  91.9% 1648 1.00 0.00
##  has_dx1:noise_level_f2    -0.18      0.18 [-0.47,  0.14]  83.1% 1590 1.00 0.00</code></pre>
<pre class="r"><code>print(performance::r2(chose_ax.model.v))</code></pre>
<pre><code>## # Bayesian R2 with Standard Error
##
##   Conditional R2: 0.226 [0.019]
##      Marginal R2: 0.131 [0.021]</code></pre>
</div>
<div id="diagnostic-3" class="section level2">
<h2>Diagnostic</h2>
<pre class="r"><code># Alternative diagnostics: diagnostic_posterior(scenario_completed.model.v)

# LOO cross-validation results
print(chose_ax.model.t$criteria$loo$estimates)</code></pre>
<pre><code>##            Estimate        SE
## elpd_loo -704.17042 14.192491
## p_loo      80.49373  2.424489
## looic    1408.34085 28.384982</code></pre>
<pre class="r"><code>print(chose_ax.model.v$criteria$loo$estimates)</code></pre>
<pre><code>##            Estimate        SE
## elpd_loo -705.05432 14.267188
## p_loo      81.57992  2.484395
## looic    1410.10864 28.534377</code></pre>
<pre class="r"><code>print(loo_compare(chose_ax.model.null, chose_ax.model.t, chose_ax.model.v))</code></pre>
<pre><code>##                     elpd_diff se_diff
## chose_ax.model.t      0.0       0.0
## chose_ax.model.v     -0.9       0.2
## chose_ax.model.null -24.8       7.7</code></pre>
<pre class="r"><code># Diagnostics of the sampling process
par(mfrow=c(1,2))
plot(chose_ax.model.t$criteria$loo, main = &quot;Trend Model&quot;)
plot(chose_ax.model.v$criteria$loo, main = &quot;Values Model&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-43-1.png' %}" width="1440" /></p>
<pre class="r"><code># Include posterior predictive checks of the sampling (are sampling according
# to the data that we actually have?)
p2 = pp_check(chose_ax.model.t, type = &quot;bars_grouped&quot;, group = &quot;has_dx&quot;) + ggtitle(&quot;t:has_dx&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars_grouped&#39; by default.</code></pre>
<pre class="r"><code>p3 = pp_check(chose_ax.model.t, type = &quot;bars_grouped&quot;, group = &quot;noise_level&quot;) + ggtitle(&quot;t:noise_level&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars_grouped&#39; by default.</code></pre>
<pre class="r"><code>grid.arrange(p2, p3, ncol = 2)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-43-2.png' %}" width="1440" /></p>
<pre class="r"><code>mcmc_intervals(as.matrix(chose_ax.model.t), regex_pars = &quot;b_|^shape&quot;, prob = 0.90, prob_outer = 0.95) +
  ggtitle(&quot;Trends Model&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-43-3.png' %}" width="1440" /></p>
<pre class="r"><code>p2 = pp_check(chose_ax.model.v, type = &quot;bars_grouped&quot;, group = &quot;has_dx&quot;) + ggtitle(&quot;v:has_dx&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars_grouped&#39; by default.</code></pre>
<pre class="r"><code>p3 = pp_check(chose_ax.model.v, type = &quot;bars_grouped&quot;, group = &quot;noise_level_f&quot;) + ggtitle(&quot;v:noise_level&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars_grouped&#39; by default.</code></pre>
<pre class="r"><code>grid.arrange(p2, p3, nrow = 2)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-43-4.png' %}" width="1440" /></p>
<pre class="r"><code>mcmc_intervals(as.matrix(chose_ax.model.v), regex_pars = &quot;b_|^shape&quot;, prob = 0.90, prob_outer = 0.95) +
  ggtitle(&quot;Values Model&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-43-5.png' %}" width="1440" /></p>
<pre class="r"><code># Plot predictions vs original
preds_p_df = chose_ax.model.null %&gt;% predict() %&gt;% as_tibble()
p1 =
  preds_p_df %&gt;%
    add_column(chose_ax = chose_ax.model.null$data$chose_ax) %&gt;%
    ggplot(aes(x = Estimate, y = chose_ax, color = chose_ax)) +
      geom_point() +
      labs(x = &quot;Predicted values&quot;) +
      ylim(0, 1) +
      xlim(0, 1) +
      ggtitle(&quot;Null Model&quot;)

preds_p_df = plot_df %&gt;%
  add_predicted_draws(chose_ax.model.null) %&gt;%
  group_by(chose_ax, .prediction) %&gt;%
  count()
p2 =
  preds_p_df %&gt;%
    ggplot(aes(x = chose_ax, y = .prediction, fill = n / sum(preds_p_df$n))) +
      geom_tile() +
      geom_text(aes(label = sprintf(&quot;%1.2f&quot;, n / sum(preds_p_df$n))), size = 10, vjust = 1) +
      scale_fill_gradient(limits = c(0, 1))

preds_p_df = chose_ax.model.t %&gt;% predict() %&gt;% as_tibble()
p3 =
  preds_p_df %&gt;%
    add_column(chose_ax = chose_ax.model.t$data$chose_ax) %&gt;%
    ggplot(aes(x = Estimate, y = chose_ax, color = chose_ax)) +
      geom_point() +
      labs(x = &quot;Predicted values&quot;) +
      ylim(0, 1) +
      xlim(0, 1) +
      ggtitle(&quot;Trends Model&quot;)

preds_p_df = plot_df %&gt;%
  add_predicted_draws(chose_ax.model.t) %&gt;%
  group_by(chose_ax, .prediction) %&gt;%
  count()
p4 =
  preds_p_df %&gt;%
    ggplot(aes(x = chose_ax, y = .prediction, fill = n / sum(preds_p_df$n))) +
      geom_tile() +
      geom_text(aes(label = sprintf(&quot;%1.2f&quot;, n / sum(preds_p_df$n))), size = 10, vjust = 1) +
      scale_fill_gradient(limits = c(0, 1))

# Plot predictions vs original
preds_p_df = chose_ax.model.v %&gt;% predict() %&gt;% as_tibble()
p5 =
  preds_p_df %&gt;%
    add_column(chose_ax = chose_ax.model.v$data$chose_ax) %&gt;%
    ggplot(aes(x = Estimate, y = chose_ax, color = chose_ax)) +
      geom_point() +
      labs(x = &quot;Predicted values&quot;) +
      ylim(0, 1) +
      xlim(0, 1) +
      ggtitle(&quot;Values Model&quot;)

preds_p_df = plot_df %&gt;%
  add_predicted_draws(chose_ax.model.v) %&gt;%
  group_by(chose_ax, .prediction) %&gt;%
  count()
p6 =
  preds_p_df %&gt;%
    ggplot(aes(x = chose_ax, y = .prediction, fill = n / sum(preds_p_df$n))) +
      geom_tile() +
      geom_text(aes(label = sprintf(&quot;%1.2f&quot;, n / sum(preds_p_df$n))), size = 10, vjust = 1) +
      scale_fill_gradient(limits = c(0, 1))

grid.arrange(p1, p3, p5, p2, p4, p6, nrow = 2, ncol = 3)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-43-6.png' %}" width="1440" /></p>
</div>
<div id="inference-3" class="section level2">
<h2>Inference</h2>
<pre class="r"><code># Compute the bayes factor for each parameter value. Note: we compare
# here against the ROPE, and not the degenerate null hypothesis of 0
# Bayes-Factor values would be more significant in the latter case
# According to the Raftery, 1995 rules
# bf = 1 - 3: Weak
# bf = 3 - 20: Positive
# bf = 20 - 150: Strong
# bf &gt; 150: Very strong
print(bayesfactor_rope(chose_ax.model.null, null = c(-0.055, 0.055)), n = 30)
print(bayesfactor_rope(chose_ax.model.t, null = c(-0.055, 0.055)), n = 30)
print(bayesfactor_rope(chose_ax.model.v, null = c(-0.055, 0.055)), n = 30)</code></pre>
<pre class="r"><code>options(digits = 3)
# Check the significance of each hypothesis
h_df = as_tibble(insight::get_parameters(chose_ax.model.t)) %&gt;%
  transmute(
    Intercept = b_Intercept,
    has_dx_test = -2 * b_has_dx1,
    noise_levelL_test = b_noise_level.L,
    noise_levelQ_test = b_noise_level.Q
  )

hyp_results = test_hypotheses(h_df, rope_values = c(-0.055, 0.055))
print(hyp_results %&gt;% select(Parameter, pd, HDI_low, HDI_high, ROPE_Percentage, ROPE_Equivalence) %&gt;% as.matrix())</code></pre>
<pre><code>##      Parameter           pd      HDI_low  HDI_high ROPE_Percentage
## [1,] &quot;Intercept&quot;         &quot;0.903&quot; &quot;-1.089&quot; &quot;2.295&quot;  &quot;0.0418&quot;
## [2,] &quot;has_dx_test&quot;       &quot;0.843&quot; &quot;-0.810&quot; &quot;1.178&quot;  &quot;0.0990&quot;
## [3,] &quot;noise_levelL_test&quot; &quot;0.963&quot; &quot;-1.226&quot; &quot;0.442&quot;  &quot;0.0425&quot;
## [4,] &quot;noise_levelQ_test&quot; &quot;0.793&quot; &quot;-0.689&quot; &quot;0.899&quot;  &quot;0.1395&quot;
##      ROPE_Equivalence
## [1,] &quot;Undecided&quot;
## [2,] &quot;Undecided&quot;
## [3,] &quot;Undecided&quot;
## [4,] &quot;Undecided&quot;</code></pre>
<pre class="r"><code>h_df = as_tibble(insight::get_parameters(chose_ax.model.v)) %&gt;%
  transmute(
    Intercept = b_Intercept,
    has_dx_test = -2 * b_has_dx1,
    noise_level1_test = b_noise_level_f1,
    noise_level2_test = b_noise_level_f2,
    noise_level3_test = - b_noise_level_f1 - b_noise_level_f2
  )

hyp_results = test_hypotheses(h_df, rope_values = c(-0.055, 0.055))
print(hyp_results %&gt;% select(Parameter, pd, HDI_low, HDI_high, ROPE_Percentage, ROPE_Equivalence) %&gt;% as.matrix())</code></pre>
<pre><code>##      Parameter           pd      HDI_low  HDI_high ROPE_Percentage
## [1,] &quot;Intercept&quot;         &quot;0.904&quot; &quot;-1.277&quot; &quot;2.924&quot;  &quot;0.0432&quot;
## [2,] &quot;has_dx_test&quot;       &quot;0.835&quot; &quot;-0.784&quot; &quot;1.318&quot;  &quot;0.1000&quot;
## [3,] &quot;noise_level1_test&quot; &quot;0.972&quot; &quot;-0.267&quot; &quot;1.076&quot;  &quot;0.0375&quot;
## [4,] &quot;noise_level2_test&quot; &quot;0.788&quot; &quot;-0.917&quot; &quot;0.698&quot;  &quot;0.1603&quot;
## [5,] &quot;noise_level3_test&quot; &quot;0.882&quot; &quot;-0.946&quot; &quot;0.493&quot;  &quot;0.1235&quot;
##      ROPE_Equivalence
## [1,] &quot;Undecided&quot;
## [2,] &quot;Undecided&quot;
## [3,] &quot;Undecided&quot;
## [4,] &quot;Undecided&quot;
## [5,] &quot;Undecided&quot;</code></pre>
<pre class="r"><code>options(digits = 7)</code></pre>
<p><strong>Results</strong>:</p>
<ol style="list-style-type: decimal">
<li>Noise Level Linear
<ul>
<li>Negative (negative slope) pd = 96.3%</li>
<li>Median = -0.40, 89% CI [-0.77, -0.05]</li>
<li>Small, Std.Median = 0.23</li>
<li>4.25% in ROPE (n.s.)</li>
</ul></li>
</ol>
<p>(The ROPE is defined as [-0.055, 0.055]. It is the range is suggested in the literature for logistic models and it corresponds to a probability range of 0.11. Therefore, a probability change of less than 0.055 is considered no different from a probability change of 0)</p>
</div>
<div id="posterior-plots-3" class="section level2">
<h2>Posterior Plots</h2>
<pre class="r"><code># A visualization data frame
gg_df = plot_df %&gt;%
    mutate(noise_level = fct_rev(noise_level)) %&gt;%
    mutate(chose_ax = factor(chose_ax)) %&gt;%
    mutate(user_id = factor(user_id)) %&gt;%
    mutate(chose_ax = fct_recode(chose_ax, no_follow=&quot;0&quot;, follow = &quot;1&quot;)) %&gt;%
    mutate(.fill_column = paste(noise_level, !(study_condition == &#39;BASELINE&#39;), sep = &quot;:&quot;)) %&gt;%
    count(user_id, study_condition, .fill_column, chose_ax, num_actions, has_ax, has_dx, noise_level,
          name = &quot;num_follow_ax&quot;)

# Simulate predictions on new data
p_df =
  plot_df %&gt;%
  data_grid(chose_ax,
            has_dx,
            noise_level_f,
            gender,
            age_group,
            robot_experience,
            num_optimal,
            state_idx_rescaled = seq_range(state_idx_rescaled, n = 4),
            .model = chose_ax.model.v)

fits_p_df = p_df %&gt;% add_fitted_draws(chose_ax.model.v,
                                      re_formula = NA,
                                      seed = default_seed)
gc()</code></pre>
<pre><code>##             used   (Mb) gc trigger   (Mb)   max used    (Mb)
## Ncells   3684703  196.8   10467648  559.1   13084560   698.8
## Vcells 367996917 2807.6  589361363 4496.5 1458980758 11131.2</code></pre>
<pre class="r"><code># We don&#39;t want predicted draws because that&#39;s a raw count of ones and zeros; not the
# probability distribution
# preds_p_df = p_df %&gt;% add_predicted_draws(scenario_completed.model.v)

# Plot the posterior distributions
rope_value = gg_df %&gt;%
  filter(chose_ax == &#39;follow&#39;, has_dx == F) %&gt;%
  summarise(.value = median(num_follow_ax / num_actions)) %&gt;%
  pull(.value)
gg_df %&gt;%
  filter(chose_ax == &#39;follow&#39;) %&gt;%
  ggplot(aes(x = has_dx, y = num_follow_ax / num_actions)) +
    geom_rect(ymin = rope_value-0.055, ymax = rope_value+0.055,
              xmin = -Inf, xmax = Inf,
              color = &quot;transparent&quot;, fill = &quot;white&quot;) +
    annotate(&quot;text&quot;, x = 0.5, y = rope_value, label = &quot;ROPE&quot;, angle = 90) +
    stat_halfeye(aes(y = .value), data = fits_p_df, position = position_nudge(x = 0.1)) +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep = &quot;:&quot;)), size = 2) +
    stat_summary(fun.y = median, geom = &quot;point&quot;, shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(y = &quot;follow_ax&quot;, color = &quot;AX:DX:Noise&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-46-1.png' %}" width="672" /></p>
<pre class="r"><code>rope_value = gg_df %&gt;%
  filter(chose_ax == &#39;follow&#39;, noise_level == &quot;0.0&quot;) %&gt;%
  summarise(.value = median(num_follow_ax / num_actions)) %&gt;%
  pull(.value)
gg_df %&gt;%
  filter(chose_ax == &#39;follow&#39;) %&gt;%
  mutate(noise_level = fct_rev(noise_level)) %&gt;%
  ggplot(aes(x = noise_level, y = num_follow_ax / num_actions)) +
    geom_rect(ymin = rope_value-0.055, ymax = rope_value+0.055,
              xmin = -Inf, xmax = Inf,
              color = &quot;transparent&quot;, fill = &quot;white&quot;) +
    annotate(&quot;text&quot;, x = 0.5, y = rope_value, label = &quot;ROPE&quot;, angle = 90) +
    stat_halfeye(aes(y = .value, x = noise_level_f), data = fits_p_df, position = position_nudge(x = 0.1)) +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep = &quot;:&quot;)), size = 2) +
    stat_summary(fun.y = median, geom = &quot;point&quot;, shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(y = &quot;follow_ax&quot;, color = &quot;AX:DX:Noise&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-46-2.png' %}" width="672" /></p>
<pre class="r"><code># Remove the giant fits data frames
rm(fits_p_df, p_df, gg_df)</code></pre>
<pre class="r"><code>plot_df = actions %&gt;%
  filter(has_ax == T) %&gt;%
  select(X1, id, user_id, study_condition, start_condition, num_optimal,
         state_idx, state_idx_rescaled,
         age_group, gender, robot_experience,
         noise_level, noise_level_f, has_noise, has_dx, has_ax,
         scenario_completed, num_actions, chose_ax) %&gt;%
  mutate(scenario_completed = factor(scenario_completed)) %&gt;%
  mutate(scenario_completed = fct_recode(scenario_completed, Unresolved=&quot;0&quot;, Resolved = &quot;1&quot;),
         has_dx = fct_recode(has_dx, &quot;No DX&quot;=&quot;FALSE&quot;, &quot;DX&quot;=&quot;TRUE&quot;),
         noise_level = fct_recode(noise_level, &quot;Acc: 100%&quot;=&quot;0.0&quot;, &quot;Acc: 90%&quot;=&quot;1.0&quot;, &quot;Acc: 80%&quot;=&quot;2.0&quot;))

eff = fixef(chose_ax.model.v)

gg_df = plot_df %&gt;%
  mutate(.fill_column = paste(noise_level, !(study_condition == &#39;BASELINE&#39;), sep = &quot;:&quot;)) %&gt;%
  count(user_id, study_condition, .fill_column, chose_ax, num_actions, has_dx, noise_level,
        name = &quot;num_chose_ax&quot;) %&gt;%
  mutate(estimate = -1) %&gt;%
  mutate(estimate = if_else(noise_level == &quot;Acc: 100%&quot;, inv_logit_scaled(eff[&quot;noise_level_f1&quot;,&quot;Estimate&quot;] + eff[&quot;Intercept&quot;, &quot;Estimate&quot;]), estimate)) %&gt;%
  mutate(estimate = if_else(noise_level == &quot;Acc: 90%&quot;, inv_logit_scaled(eff[&quot;noise_level_f2&quot;,&quot;Estimate&quot;] + eff[&quot;Intercept&quot;, &quot;Estimate&quot;]), estimate)) %&gt;%
  mutate(estimate = if_else(noise_level == &quot;Acc: 80%&quot;, inv_logit_scaled(eff[&quot;Intercept&quot;, &quot;Estimate&quot;] - eff[&quot;noise_level_f1&quot;,&quot;Estimate&quot;] - eff[&quot;noise_level_f2&quot;,&quot;Estimate&quot;]), estimate)) %&gt;%
  mutate(noise_level = fct_rev(noise_level))

gg_df %&gt;%
  filter(chose_ax == &quot;1&quot;) %&gt;%
  ggplot(aes(noise_level, num_chose_ax / num_actions, group = noise_level, colour = noise_level)) +
    geom_hline(yintercept = inv_logit_scaled(eff[&quot;Intercept&quot;, &quot;Estimate&quot;]),
               size = .7, linetype = &quot;dashed&quot;, color = &quot;grey&quot;) +
    geom_count() +
    geom_boxplot(aes(y = estimate)) +
    annotate(&quot;segment&quot;,
             x = 3,
             y = inv_logit_scaled(eff[&quot;noise_level_f1&quot;,&quot;Estimate&quot;] + eff[&quot;Intercept&quot;, &quot;Estimate&quot;]),
             xend = 2,
             yend = inv_logit_scaled(eff[&quot;noise_level_f2&quot;,&quot;Estimate&quot;] + eff[&quot;Intercept&quot;, &quot;Estimate&quot;]),
             colour = &quot;black&quot;) +
    annotate(&quot;segment&quot;,
             x = 1,
             y = inv_logit_scaled(eff[&quot;Intercept&quot;, &quot;Estimate&quot;] - eff[&quot;noise_level_f1&quot;,&quot;Estimate&quot;] - eff[&quot;noise_level_f2&quot;,&quot;Estimate&quot;]),
             xend = 2,
             yend = inv_logit_scaled(eff[&quot;noise_level_f2&quot;,&quot;Estimate&quot;] + eff[&quot;Intercept&quot;, &quot;Estimate&quot;]),
             color = &quot;black&quot;) +
    annotate(&quot;text&quot;,
             x = 3.5,
             y = inv_logit_scaled(eff[&quot;Intercept&quot;, &quot;Estimate&quot;] - 0.25),
             label = &quot;Mean of all levels&quot;,
             angle = 90,
             group = NA,
             colour = &quot;black&quot;) +
    scale_y_continuous(limits = c(0.0, 1.0), breaks = c(0.0, 0.5, 1.0)) +
    labs(y = &quot;Fraction of participants that resolved fault&quot;, x = NULL) +
    scale_colour_manual(values = c(&quot;#d62728&quot;, &quot;#2ca02c&quot;, &quot;#ff7f0e&quot;)) +
    scale_size_area(max_size = 1) +
    theme_economist_white(base_size = 13) +
    theme(plot.background = element_rect(fill = &quot;white&quot;),
          axis.title.y = element_blank()) +
    legend_none()</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-47-1.png' %}" width="672" /></p>
</div>
</div>
<div id="cdx-compliance-with-dx-suggestions" class="section level1">
<h1>CDX: Compliance with DX Suggestions</h1>
<p><strong>Did the user take an follow the DX suggestions that were provided to them?</strong></p>
<p>In the code, the variable might be referred to as <code>chose_dx</code>, <code>follow_dx</code>, etc. depending on the version of the codebase</p>
<pre class="r"><code>plot_df = actions %&gt;%
  filter(has_dx == T) %&gt;%
  select(X1, id, user_id, study_condition, start_condition, num_optimal,
         state_idx, state_idx_rescaled,
         age_group, gender, robot_experience,
         noise_level, noise_level_f, has_noise, has_dx, has_ax,
         scenario_completed, num_actions, chose_dx)

text_short(report(plot_df))</code></pre>
<pre><code>## Warning in report.numeric(model[[col]], median = median, centrality =
## centrality, : Variable `model[[col]]` contains only two different values.
## Consider converting it to a factor.

## Warning in report.numeric(model[[col]], median = median, centrality =
## centrality, : Variable `model[[col]]` contains only two different values.
## Consider converting it to a factor.</code></pre>
<pre><code>## The data contains 1311 observations of the following variables:
##   - X1: Mean = 1125.95, SD = 627.22, range = [0, 2114], 0 missing
##   - id: Mean = 1127.74, SD = 627.06, range = [1, 2115], 0 missing
##   - user_id: Mean = 108.55, SD = 58.57, range = [1, 200], 0 missing
##   - study_condition: 10 levels: DX_100, n = 247; AX_100, n = 0; DXAX_100, n = 193; DX_90, n = 243; AX_90, n = 0; DXAX_90, n = 183; DX_80, n = 256; AX_80, n = 0; DXAX_80, n = 189 and BASELINE, n = 0
##   - start_condition: 4 entries: kc.dt.occluding.above_mug.default.empty.dt, n = 402; kc.kc.occluding.default.default.empty.dt, n = 349; dt.kc.default.default.default.empty.kc, n = 306 and 1 other
##   - num_optimal: Mean = 4.73, SD = 1.56, range = [3, 7], 0 missing
##   - state_idx: Mean = 92.34, SD = 60.42, range = [1, 173], 0 missing
##   - state_idx_rescaled: Mean = 0.53, SD = 0.35, range = [0, 1], 0 missing
##   - age_group: 7 levels: 2, n = 112; 3, n = 331; 4, n = 297; 5, n = 169; 6, n = 137; 7, n = 87 and 8, n = 178
##   - gender: 2 levels: F, n = 534 and M, n = 777
##   - robot_experience: 5 levels: 0, n = 859; 1, n = 180; 2, n = 107; 3, n = 69 and 4, n = 96
##   - noise_level: 3 levels: 0.0, n = 440; 1.0, n = 426 and 2.0, n = 445
##   - noise_level_f: 3 levels: 0.0, n = 440; 1.0, n = 426 and 2.0, n = 445
##   - has_noise: 2 levels: FALSE, n = 440 and TRUE, n = 871
##   - has_dx: 2 levels: FALSE, n = 0 and TRUE, n = 1311
##   - has_ax: 2 levels: FALSE, n = 746 and TRUE, n = 565
##   - scenario_completed: Mean = 0.57, SD = 0.49, range = [0, 1], 0 missing
##   - num_actions: Mean = 14.23, SD = 5.88, range = [3, 20], 0 missing
##   - chose_dx: Mean = 0.65, SD = 0.48, range = [0, 1], 0 missing</code></pre>
<div id="data-4" class="section level2">
<h2>Data</h2>
<pre class="r"><code>gg_df =
  plot_df %&gt;%
    mutate(noise_level = fct_rev(noise_level)) %&gt;%
    mutate(chose_dx = factor(chose_dx)) %&gt;%
    mutate(user_id = factor(user_id)) %&gt;%
    mutate(chose_dx = fct_recode(chose_dx, no_follow=&quot;0&quot;, follow = &quot;1&quot;)) %&gt;%
    mutate(.fill_column = paste(noise_level, !(study_condition == &#39;BASELINE&#39;), sep = &quot;:&quot;)) %&gt;%
    count(user_id, study_condition, .fill_column, chose_dx, num_actions, has_ax, has_dx, noise_level,
          name = &quot;num_follow_dx&quot;)

# Plot by the study condition
gg_df %&gt;%
  ggplot(aes(user_id, num_follow_dx / num_actions, fill=chose_dx)) +
    geom_bar(stat=&quot;identity&quot;) +
    geom_hline(yintercept = 0.5) +
    facet_wrap(vars(study_condition), nrow = 1, scales = &quot;free_y&quot;) +
    labs(y = &quot;Fraction followed DX&quot;) +
    coord_flip() +
    scale_fill_economist()</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-50-1.png' %}" width="1440" /></p>
<pre class="r"><code>gg_df %&gt;%
  filter(chose_dx == &quot;follow&quot;) %&gt;%
  ggplot(aes(study_condition, num_follow_dx / num_actions, fill=.fill_column)) +
    geom_violin() +
    geom_boxplot(width = 0.1) +
    labs(y = &quot;Fraction followed DX&quot;, fill = &quot;noise:has_suggestions&quot;) +
    scale_fill_economist()</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-50-2.png' %}" width="1440" /></p>
<pre class="r"><code># Visualize the data facetted by the variables that we care about
p1 = gg_df %&gt;%
  filter(chose_dx == &quot;follow&quot;) %&gt;%
  ggplot(aes(has_ax, num_follow_dx / num_actions, fill=.fill_column)) +
    geom_boxplot() +
    geom_count() +
    facet_grid(rows = vars(noise_level), labeller = label_both) +
    labs(y = &quot;Fraction followed DX&quot;) +
    scale_fill_economist() +
    theme(legend.position = &quot;right&quot;)

p3 = gg_df %&gt;%
  filter(chose_dx == &quot;follow&quot;) %&gt;%
  mutate(noise_level = fct_rev(noise_level)) %&gt;%
  ggplot(aes(noise_level, num_follow_dx / num_actions, fill=.fill_column)) +
    geom_boxplot() +
    geom_count() +
    facet_grid(cols = vars(has_ax), labeller = label_both) +
    labs(y = &quot;Fraction followed DX&quot;) +
    scale_fill_economist() +
    legend_none()

grid.arrange(p1, p3, ncol = 2)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-50-3.png' %}" width="1440" /></p>
<pre class="r"><code># Visualize also the variation in each of the three levels that we care about
p1 = gg_df %&gt;%
  filter(chose_dx == &#39;follow&#39;) %&gt;%
  ggplot(aes(has_ax, num_follow_dx/num_actions)) +
    geom_violin() +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep=&quot;:&quot;), size = 2)) +
    stat_summary(fun.y = median, geom=&quot;point&quot;, shape=23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    theme(legend.position = &quot;right&quot;)

p3 = gg_df %&gt;%
  filter(chose_dx == &#39;follow&#39;) %&gt;%
  ggplot(aes(noise_level, num_follow_dx/num_actions)) +
    geom_violin() +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep=&quot;:&quot;), size = 2)) +
    stat_summary(fun.y = median, geom=&quot;point&quot;, shape=23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    scale_x_discrete(limits = rev(levels(gg_df$noise_level))) +
    legend_none()

grid.arrange(p1, p3, ncol = 2)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-50-4.png' %}" width="1440" /></p>
</div>
<div id="model-4" class="section level2">
<h2>Model</h2>
<p>Based on the data, we assume the following structural model:</p>
<p><span class="math display">\[cdx_{ij} = Bernoulli(p_{ij})\]</span> <span class="math display">\[\begin{aligned}
logit^{-1}(p_{ij}) &amp;= \beta_0 + \beta_{0i} + \beta_{ax}\text{ax}_i + \beta_{noise}\text{noise}_i + \beta_{ax:noise}\text{ax}_i\text{noise}_i + \\
&amp;\beta_{no}\text{no}_i + \mathbf{\beta_{demo}X_{demo,i}} + \beta_{state} \text{state}_{ij}
\end{aligned}\]</span> <span class="math display">\[\begin{aligned}
\beta_{.} &amp;\sim Normal(0, 10) \\
\beta_{0i} &amp;\sim Normal(0, \sigma_i) \\
\sigma_i &amp;\sim HalfStudent(3, 0, 10)
\end{aligned}\]</span></p>
<p>The prior for the <span class="math inline">\(\sigma_i\)</span> parameter is the default used in <code>brms</code>; I see no need to change it.</p>
<p>Model fitting:</p>
<pre class="r"><code># The null model
chose_dx.model.null = brm(
  chose_dx ~ 0 + Intercept + (1 | user_id),
  family = &quot;bernoulli&quot;,
  prior = set_prior(&quot;normal(0, 10)&quot;, class = &quot;b&quot;),
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
chose_dx.model.null = add_criterion(chose_dx.model.null, &quot;waic&quot;)
chose_dx.model.null = add_criterion(chose_dx.model.null, &quot;loo&quot;, reloo = T)
saveModel(chose_dx.model.null)

# The trend model to see if there is a trend in the noise level variable
chose_dx.model.t = brm(
  chose_dx ~ 0 + Intercept + (has_ax * noise_level) + (age_group + robot_experience + gender) + num_optimal + state_idx_rescaled + (1 | user_id),
  family = bernoulli,
  prior = c(set_prior(&quot;normal(0, 10)&quot;, class = &quot;b&quot;),
            set_prior(&quot;student_t(3, 0, 10)&quot;, class = &quot;sd&quot;)),
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
chose_dx.model.t = add_criterion(chose_dx.model.t, &quot;waic&quot;)
chose_dx.model.t = add_criterion(chose_dx.model.t, &quot;loo&quot;, reloo = T)
saveModel(chose_dx.model.t)

# The values model, to see if specific values of the noise level variable are significant
chose_dx.model.v = brm(
  chose_dx ~ 0 + Intercept + (has_ax * noise_level_f) + (age_group + robot_experience + gender) + num_optimal + state_idx_rescaled + (1 | user_id),
  family = bernoulli,
  prior = c(set_prior(&quot;normal(0, 10)&quot;, class = &quot;b&quot;),
            set_prior(&quot;student_t(3, 0, 10)&quot;, class = &quot;sd&quot;)),
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
chose_dx.model.v = add_criterion(chose_dx.model.v, &quot;waic&quot;)
chose_dx.model.v = add_criterion(chose_dx.model.v, &quot;loo&quot;, reloo = T)
saveModel(chose_dx.model.v)</code></pre>
<pre class="r"><code># Load the models
chose_dx.model.null = loadModel(&quot;chose_dx.model.null&quot;)
chose_dx.model.t = loadModel(&quot;chose_dx.model.t&quot;)
chose_dx.model.v = loadModel(&quot;chose_dx.model.v&quot;)</code></pre>
<p>Model fitting results:</p>
<pre class="r"><code># print(tidy_stan(chose_ax.model.null))
print(performance::r2(chose_dx.model.null))</code></pre>
<pre><code>## # Bayesian R2 with Standard Error
##
##   Conditional R2: 0.245 [0.019]
##      Marginal R2: 0.000 [0.000]</code></pre>
<pre class="r"><code># Print the parameters, and some initial diagnostics
print(tidy_stan(chose_dx.model.t, effects = &quot;fixed&quot;))</code></pre>
<pre><code>## Warning: &#39;tidy_stan&#39; is deprecated.
## Use &#39;parameters::model_parameters()&#39; instead.
## See help(&quot;Deprecated&quot;)</code></pre>
<pre><code>##
## Summary Statistics of Stan-Model
##
## # Fixed effects
##
##              Parameter Estimate Std.Error       HDI(89%)    pd  ESS Rhat MCSE
##            (Intercept)     0.89      0.57 [-0.04,  1.79] 94.6% 1776 1.00 0.01
##                has_ax1     0.00      0.16 [-0.27,  0.25] 50.3% 2111 1.00 0.00
##          noise_level.L    -0.67      0.27 [-1.13, -0.22] 99.4% 2026 1.00 0.01
##          noise_level.Q     0.04      0.28 [-0.39,  0.49] 56.0% 2435 1.00 0.01
##             age_group1     0.01      0.53 [-0.84,  0.79] 50.7% 2072 1.00 0.01
##             age_group2    -0.17      0.31 [-0.65,  0.37] 70.2% 2178 1.00 0.01
##             age_group3    -0.38      0.34 [-0.92,  0.16] 87.2% 2054 1.00 0.01
##             age_group4     0.18      0.38 [-0.42,  0.80] 68.4% 2202 1.00 0.01
##             age_group5    -0.25      0.45 [-0.99,  0.43] 71.1% 1894 1.00 0.01
##             age_group6     1.04      0.59 [ 0.09,  1.99] 96.1% 2338 1.00 0.01
##      robot_experience1     0.37      0.30 [-0.11,  0.87] 89.0% 2159 1.00 0.01
##      robot_experience2    -0.03      0.40 [-0.65,  0.64] 53.4% 2250 1.00 0.01
##      robot_experience3    -0.71      0.57 [-1.57,  0.20] 90.1% 2272 1.00 0.01
##      robot_experience4     0.78      0.59 [-0.11,  1.79] 91.0% 2358 1.00 0.01
##                gender1     0.01      0.17 [-0.28,  0.27] 52.2% 2230 1.00 0.00
##            num_optimal     0.01      0.10 [-0.16,  0.18] 52.8% 1570 1.00 0.00
##     state_idx_rescaled     0.30      0.23 [-0.07,  0.65] 91.1% 4123 1.00 0.00
##  has_ax1.noise_level.L    -0.33      0.28 [-0.81,  0.09] 88.1% 2306 1.00 0.01
##  has_ax1.noise_level.Q     0.01      0.28 [-0.41,  0.47] 51.0% 2183 1.00 0.01</code></pre>
<pre class="r"><code>print(performance::r2(chose_dx.model.t))</code></pre>
<pre><code>## # Bayesian R2 with Standard Error
##
##   Conditional R2: 0.263 [0.017]
##      Marginal R2: 0.104 [0.028]</code></pre>
<pre class="r"><code>print(tidy_stan(chose_dx.model.v, effects = &quot;fixed&quot;))</code></pre>
<pre><code>## Warning: &#39;tidy_stan&#39; is deprecated.
## Use &#39;parameters::model_parameters()&#39; instead.
## See help(&quot;Deprecated&quot;)</code></pre>
<pre><code>##
## Summary Statistics of Stan-Model
##
## # Fixed effects
##
##               Parameter Estimate Std.Error      HDI(89%)    pd  ESS Rhat MCSE
##             (Intercept)     0.89      0.55 [-0.00, 1.73] 95.1% 2171 1.00 0.01
##                 has_ax1    -0.00      0.16 [-0.24, 0.26] 50.7% 2268 1.00 0.00
##          noise_level_f1     0.50      0.22 [ 0.16, 0.86] 99.1% 2191 1.00 0.00
##          noise_level_f2    -0.04      0.22 [-0.40, 0.32] 56.7% 2232 1.00 0.00
##              age_group1    -0.02      0.52 [-0.82, 0.84] 51.0% 2107 1.00 0.01
##              age_group2    -0.16      0.32 [-0.66, 0.37] 69.7% 2234 1.00 0.01
##              age_group3    -0.36      0.33 [-0.86, 0.20] 85.9% 2437 1.00 0.01
##              age_group4     0.19      0.40 [-0.40, 0.84] 68.2% 2442 1.00 0.01
##              age_group5    -0.27      0.44 [-1.01, 0.42] 72.9% 2541 1.00 0.01
##              age_group6     1.03      0.60 [ 0.10, 1.96] 96.1% 2790 1.00 0.01
##       robot_experience1     0.37      0.29 [-0.10, 0.85] 88.9% 2377 1.00 0.01
##       robot_experience2    -0.04      0.40 [-0.69, 0.59] 54.5% 2571 1.00 0.01
##       robot_experience3    -0.68      0.57 [-1.68, 0.17] 89.8% 2260 1.00 0.01
##       robot_experience4     0.77      0.61 [-0.14, 1.84] 89.8% 2375 1.00 0.01
##                 gender1     0.02      0.17 [-0.27, 0.29] 54.0% 2323 1.00 0.00
##             num_optimal     0.00      0.10 [-0.16, 0.16] 50.6% 2313 1.00 0.00
##      state_idx_rescaled     0.30      0.22 [-0.07, 0.65] 91.1% 5864 1.00 0.00
##  has_ax1:noise_level_f1     0.23      0.22 [-0.13, 0.58] 85.2% 2532 1.00 0.00
##  has_ax1:noise_level_f2     0.00      0.23 [-0.36, 0.37] 50.4% 2405 1.00 0.00</code></pre>
<pre class="r"><code>print(performance::r2(chose_dx.model.v))</code></pre>
<pre><code>## # Bayesian R2 with Standard Error
##
##   Conditional R2: 0.263 [0.017]
##      Marginal R2: 0.104 [0.028]</code></pre>
</div>
<div id="diagnostic-4" class="section level2">
<h2>Diagnostic</h2>
<pre class="r"><code># Alternative diagnostics: diagnostic_posterior(scenario_completed.model.v)

# LOO cross-validation results
print(chose_dx.model.t$criteria$loo$estimates)</code></pre>
<pre><code>##            Estimate        SE
## elpd_loo -720.97577 18.451600
## p_loo      84.36675  3.680997
## looic    1441.95153 36.903200</code></pre>
<pre class="r"><code>print(chose_dx.model.v$criteria$loo$estimates)</code></pre>
<pre><code>##            Estimate        SE
## elpd_loo -720.47516 18.409545
## p_loo      83.91142  3.642657
## looic    1440.95032 36.819090</code></pre>
<pre class="r"><code>print(loo_compare(chose_dx.model.null, chose_dx.model.t, chose_dx.model.v))</code></pre>
<pre><code>##                     elpd_diff se_diff
## chose_dx.model.null  0.0       0.0
## chose_dx.model.v    -2.9       2.9
## chose_dx.model.t    -3.4       2.9</code></pre>
<pre class="r"><code># Diagnostics of the sampling process
par(mfrow=c(1,2))
plot(chose_dx.model.t$criteria$loo, main = &quot;Trend Model&quot;)
plot(chose_dx.model.v$criteria$loo, main = &quot;Values Model&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-54-1.png' %}" width="1440" /></p>
<pre class="r"><code># Include posterior predictive checks of the sampling (are sampling according
# to the data that we actually have?)
p1 = pp_check(chose_dx.model.t, type = &quot;bars_grouped&quot;, group = &quot;has_ax&quot;) + ggtitle(&quot;t:has_ax&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars_grouped&#39; by default.</code></pre>
<pre class="r"><code>p3 = pp_check(chose_dx.model.t, type = &quot;bars_grouped&quot;, group = &quot;noise_level&quot;) + ggtitle(&quot;t:noise_level&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars_grouped&#39; by default.</code></pre>
<pre class="r"><code>grid.arrange(p1, p3, ncol = 2)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-54-2.png' %}" width="1440" /></p>
<pre class="r"><code>mcmc_intervals(as.matrix(chose_dx.model.t), regex_pars = &quot;b_|^shape&quot;, prob = 0.90, prob_outer = 0.95) +
  ggtitle(&quot;Trends Model&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-54-3.png' %}" width="1440" /></p>
<pre class="r"><code>p1 = pp_check(chose_dx.model.v, type = &quot;bars_grouped&quot;, group = &quot;has_ax&quot;) + ggtitle(&quot;v:has_ax&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars_grouped&#39; by default.</code></pre>
<pre class="r"><code>p3 = pp_check(chose_dx.model.v, type = &quot;bars_grouped&quot;, group = &quot;noise_level_f&quot;) + ggtitle(&quot;v:noise_level&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars_grouped&#39; by default.</code></pre>
<pre class="r"><code>grid.arrange(p1, p3, nrow = 2)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-54-4.png' %}" width="1440" /></p>
<pre class="r"><code>mcmc_intervals(as.matrix(chose_dx.model.v), regex_pars = &quot;b_|^shape&quot;, prob = 0.90, prob_outer = 0.95) +
  ggtitle(&quot;Values Model&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-54-5.png' %}" width="1440" /></p>
<pre class="r"><code># Plot predictions vs original
preds_p_df = chose_dx.model.null %&gt;% predict() %&gt;% as_tibble()
p1 =
  preds_p_df %&gt;%
    add_column(chose_dx = chose_dx.model.null$data$chose_dx) %&gt;%
    ggplot(aes(x = Estimate, y = chose_dx, color = chose_dx)) +
      geom_point() +
      labs(x = &quot;Predicted values&quot;) +
      ylim(0, 1) +
      xlim(0, 1) +
      ggtitle(&quot;Null Model&quot;)

preds_p_df = plot_df %&gt;%
  add_predicted_draws(chose_dx.model.null) %&gt;%
  group_by(chose_dx, .prediction) %&gt;%
  count()
p2 =
  preds_p_df %&gt;%
    ggplot(aes(x = chose_dx, y = .prediction, fill = n / sum(preds_p_df$n))) +
      geom_tile() +
      geom_text(aes(label = sprintf(&quot;%1.2f&quot;, n / sum(preds_p_df$n))), size = 10, vjust = 1) +
      scale_fill_gradient(limits = c(0, 1))

preds_p_df = chose_dx.model.t %&gt;% predict() %&gt;% as_tibble()
p3 =
  preds_p_df %&gt;%
    add_column(chose_dx = chose_dx.model.t$data$chose_dx) %&gt;%
    ggplot(aes(x = Estimate, y = chose_dx, color = chose_dx)) +
      geom_point() +
      labs(x = &quot;Predicted values&quot;) +
      ylim(0, 1) +
      xlim(0, 1) +
      ggtitle(&quot;Trends Model&quot;)

preds_p_df = plot_df %&gt;%
  add_predicted_draws(chose_dx.model.t) %&gt;%
  group_by(chose_dx, .prediction) %&gt;%
  count()
p4 =
  preds_p_df %&gt;%
    ggplot(aes(x = chose_dx, y = .prediction, fill = n / sum(preds_p_df$n))) +
      geom_tile() +
      geom_text(aes(label = sprintf(&quot;%1.2f&quot;, n / sum(preds_p_df$n))), size = 10, vjust = 1) +
      scale_fill_gradient(limits = c(0, 1))

# Plot predictions vs original
preds_p_df = chose_dx.model.v %&gt;% predict() %&gt;% as_tibble()
p5 =
  preds_p_df %&gt;%
    add_column(chose_dx = chose_dx.model.v$data$chose_dx) %&gt;%
    ggplot(aes(x = Estimate, y = chose_dx, color = chose_dx)) +
      geom_point() +
      labs(x = &quot;Predicted values&quot;) +
      ylim(0, 1) +
      xlim(0, 1) +
      ggtitle(&quot;Values Model&quot;)

preds_p_df = plot_df %&gt;%
  add_predicted_draws(chose_dx.model.v) %&gt;%
  group_by(chose_dx, .prediction) %&gt;%
  count()
p6 =
  preds_p_df %&gt;%
    ggplot(aes(x = chose_dx, y = .prediction, fill = n / sum(preds_p_df$n))) +
      geom_tile() +
      geom_text(aes(label = sprintf(&quot;%1.2f&quot;, n / sum(preds_p_df$n))), size = 10, vjust = 1) +
      scale_fill_gradient(limits = c(0, 1))

grid.arrange(p1, p3, p5, p2, p4, p6, nrow = 2, ncol = 3)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-54-6.png' %}" width="1440" /></p>
</div>
<div id="inference-4" class="section level2">
<h2>Inference</h2>
<pre class="r"><code># Compute the bayes factor for each parameter value. Note: we compare
# here against the ROPE, and not the degenerate null hypothesis of 0
# Bayes-Factor values would be more significant in the latter case
# According to the Raftery, 1995 rules
# bf = 1 - 3: Weak
# bf = 3 - 20: Positive
# bf = 20 - 150: Strong
# bf &gt; 150: Very strong
print(bayesfactor_rope(chose_dx.model.null, null = c(-0.055, 0.055)), n = 30)
print(bayesfactor_rope(chose_dx.model.t, null = c(-0.055, 0.055)), n = 30)
print(bayesfactor_rope(chose_dx.model.v, null = c(-0.055, 0.055)), n = 30)</code></pre>
<pre class="r"><code>options(digits = 3)
# Check the significance of each hypothesis
h_df = as_tibble(insight::get_parameters(chose_dx.model.t)) %&gt;%
  transmute(
    Intercept = b_Intercept,
    has_ax_test = -2 * b_has_ax1,
    noise_levelL_test = b_noise_level.L,
    noise_levelQ_test = b_noise_level.Q
  )

hyp_results = test_hypotheses(h_df, rope_values = c(-0.055, 0.055))
print(hyp_results %&gt;% select(Parameter, pd, HDI_low, HDI_high, ROPE_Percentage, ROPE_Equivalence) %&gt;% as.matrix())</code></pre>
<pre><code>##      Parameter           pd      HDI_low HDI_high ROPE_Percentage
## [1,] &quot;Intercept&quot;         &quot;0.946&quot; &quot;-1.07&quot; &quot;2.918&quot;  &quot;0.02550&quot;
## [2,] &quot;has_ax_test&quot;       &quot;0.503&quot; &quot;-1.29&quot; &quot;1.068&quot;  &quot;0.14300&quot;
## [3,] &quot;noise_levelL_test&quot; &quot;0.994&quot; &quot;-1.83&quot; &quot;0.364&quot;  &quot;0.00675&quot;
## [4,] &quot;noise_levelQ_test&quot; &quot;0.560&quot; &quot;-1.07&quot; &quot;0.993&quot;  &quot;0.15575&quot;
##      ROPE_Equivalence
## [1,] &quot;Undecided&quot;
## [2,] &quot;Undecided&quot;
## [3,] &quot;Rejected&quot;
## [4,] &quot;Undecided&quot;</code></pre>
<pre class="r"><code>h_df = as_tibble(insight::get_parameters(chose_dx.model.v)) %&gt;%
  transmute(
    Intercept = b_Intercept,
    has_ax_test = -2 * b_has_ax1,
    noise_level1_test = b_noise_level_f1,
    noise_level2_test = b_noise_level_f2,
    noise_level3_test = - b_noise_level_f1 - b_noise_level_f2
  )

hyp_results = test_hypotheses(h_df, rope_values = c(-0.055, 0.055))
print(hyp_results %&gt;% select(Parameter, pd, HDI_low, HDI_high, ROPE_Percentage, ROPE_Equivalence) %&gt;% as.matrix())</code></pre>
<pre><code>##      Parameter           pd      HDI_low  HDI_high ROPE_Percentage
## [1,] &quot;Intercept&quot;         &quot;0.951&quot; &quot;-1.416&quot; &quot;2.798&quot;  &quot;0.0225&quot;
## [2,] &quot;has_ax_test&quot;       &quot;0.507&quot; &quot;-1.081&quot; &quot;1.326&quot;  &quot;0.1355&quot;
## [3,] &quot;noise_level1_test&quot; &quot;0.991&quot; &quot;-0.189&quot; &quot;1.435&quot;  &quot;0.0150&quot;
## [4,] &quot;noise_level2_test&quot; &quot;0.567&quot; &quot;-0.900&quot; &quot;0.780&quot;  &quot;0.1920&quot;
## [5,] &quot;noise_level3_test&quot; &quot;0.979&quot; &quot;-1.234&quot; &quot;0.461&quot;  &quot;0.0253&quot;
##      ROPE_Equivalence
## [1,] &quot;Rejected&quot;
## [2,] &quot;Undecided&quot;
## [3,] &quot;Rejected&quot;
## [4,] &quot;Undecided&quot;
## [5,] &quot;Undecided&quot;</code></pre>
<pre class="r"><code>options(digits = 7)</code></pre>
<p><strong>Results</strong>:</p>
<ol style="list-style-type: decimal">
<li>Noise Level Linear
<ul>
<li>Negative (negative slope) pd = 99.4%</li>
<li>Median = -0.67, 89% CI [-1.13, -0.22]</li>
<li>Small, Std.Median = 0.27</li>
<li>0.68% in ROPE</li>
</ul></li>
</ol>
<p>(The ROPE is defined as [-0.055, 0.055]. It is the range is suggested in the literature for logistic models and it corresponds to a probability range of 0.11. Therefore, a probability change of less than 0.055 is considered no different from a probability change of 0)</p>
</div>
<div id="posterior-plots-4" class="section level2">
<h2>Posterior Plots</h2>
<pre class="r"><code># A visualization data frame
gg_df = plot_df %&gt;%
    mutate(noise_level = fct_rev(noise_level)) %&gt;%
    mutate(chose_dx = factor(chose_dx)) %&gt;%
    mutate(user_id = factor(user_id)) %&gt;%
    mutate(chose_dx = fct_recode(chose_dx, no_follow=&quot;0&quot;, follow = &quot;1&quot;)) %&gt;%
    mutate(.fill_column = paste(noise_level, !(study_condition == &#39;BASELINE&#39;), sep = &quot;:&quot;)) %&gt;%
    count(user_id, study_condition, .fill_column, chose_dx, num_actions, has_ax, has_dx, noise_level,
          name = &quot;num_follow_dx&quot;)

# Simulate predictions on new data
p_df =
  plot_df %&gt;%
  data_grid(chose_dx,
            has_ax,
            noise_level_f,
            gender,
            age_group,
            robot_experience,
            num_optimal,
            state_idx_rescaled = seq_range(state_idx_rescaled, n = 4),
            .model = chose_dx.model.v)

fits_p_df = p_df %&gt;% add_fitted_draws(chose_dx.model.v,
                                      re_formula = NA,
                                      n = 100,
                                      seed = default_seed)
gc()</code></pre>
<pre><code>##            used  (Mb) gc trigger   (Mb)   max used    (Mb)
## Ncells  3924272 209.6   10467648  559.1   13084560   698.8
## Vcells 34141466 260.5  809208344 6173.8 1458980758 11131.2</code></pre>
<pre class="r"><code>pars_p_df = chose_dx.model.t %&gt;%
  extract_draws(newdata = p_df %&gt;% rename(noise_level = noise_level_f),
                re_formula = NA,
                nsamples = 100)
gc()</code></pre>
<pre><code>##            used  (Mb) gc trigger   (Mb)   max used    (Mb)
## Ncells  3924307 209.6   10467648  559.1   13084560   698.8
## Vcells 34350130 262.1  647366676 4939.1 1458980758 11131.2</code></pre>
<pre class="r"><code># We don&#39;t want predicted draws because that&#39;s a raw count of ones and zeros; not the
# probability distribution
# preds_p_df = p_df %&gt;% add_predicted_draws(scenario_completed.model.v)

# Plot the posterior distributions
rope_value = gg_df %&gt;%
  filter(chose_dx == &#39;follow&#39;, has_ax == F) %&gt;%
  summarise(.value = median(num_follow_dx / num_actions)) %&gt;%
  pull(.value)
gg_df %&gt;%
  filter(chose_dx == &#39;follow&#39;) %&gt;%
  ggplot(aes(x = has_ax, y = num_follow_dx / num_actions)) +
    geom_rect(ymin = rope_value-0.055, ymax = rope_value+0.055,
              xmin = -Inf, xmax = Inf,
              color = &quot;transparent&quot;, fill = &quot;white&quot;) +
    annotate(&quot;text&quot;, x = 0.5, y = rope_value, label = &quot;ROPE&quot;, angle = 90) +
    stat_halfeye(aes(y = .value), data = fits_p_df, position = position_nudge(x = 0.1)) +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep = &quot;:&quot;)), size = 2) +
    stat_summary(fun.y = median, geom = &quot;point&quot;, shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(y = &quot;follow_dx&quot;, color = &quot;AX:DX:Noise&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-57-1.png' %}" width="672" /></p>
<pre class="r"><code>rope_value = gg_df %&gt;%
  filter(chose_dx == &#39;follow&#39;, noise_level == &quot;0.0&quot;) %&gt;%
  summarise(.value = median(num_follow_dx / num_actions)) %&gt;%
  pull(.value)
gg_df %&gt;%
  filter(chose_dx == &#39;follow&#39;) %&gt;%
  mutate(noise_level = fct_rev(noise_level)) %&gt;%
  ggplot(aes(x = noise_level, y = num_follow_dx / num_actions)) +
    geom_rect(ymin = rope_value-0.055, ymax = rope_value+0.055,
              xmin = -Inf, xmax = Inf,
              color = &quot;transparent&quot;, fill = &quot;white&quot;) +
    annotate(&quot;text&quot;, x = 0.5, y = rope_value, label = &quot;ROPE&quot;, angle = 90) +
    stat_halfeye(aes(y = .value, x = noise_level_f), data = fits_p_df, position = position_nudge(x = 0.1)) +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep = &quot;:&quot;)), size = 2) +
    stat_summary(fun.y = median, geom = &quot;point&quot;, shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(y = &quot;follow_dx&quot;, color = &quot;AX:DX:Noise&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-57-2.png' %}" width="672" /></p>
<pre class="r"><code># Create a plot to show the trend
pars_p_df_X = pars_p_df$dpars$mu$fe$X[,c(&quot;Intercept&quot;, &quot;noise_level.L&quot;)]
pars_p_df_b = pars_p_df$dpars$mu$fe$b[,c(&quot;b_Intercept&quot;, &quot;b_noise_level.L&quot;)]
pars_p_df_y = pars_p_df_X %*% t(pars_p_df_b)
pars_p_df_y = inv_logit_scaled(pars_p_df_y)
p_df =
  p_df %&gt;%
  bind_cols(as_tibble(pars_p_df_y)) %&gt;%
  gather(key = &quot;.sample&quot;, value = &quot;.value&quot;, V1:V100) %&gt;%
  rename(noise_level = noise_level_f)

rope_value = gg_df %&gt;%
  filter(chose_dx == &#39;follow&#39;, noise_level == &quot;0.0&quot;) %&gt;%
  summarise(.value = median(num_follow_dx / num_actions)) %&gt;%
  pull(.value)
gg_df %&gt;%
  filter(chose_dx == &#39;follow&#39;) %&gt;%
  mutate(noise_level = fct_rev(noise_level)) %&gt;%
  ggplot(aes(x = noise_level, y = num_follow_dx / num_actions)) +
    geom_rect(ymin = rope_value-0.055, ymax = rope_value+0.055,
              xmin = -Inf, xmax = Inf,
              color = &quot;transparent&quot;, fill = &quot;white&quot;) +
    annotate(&quot;text&quot;, x = 0.5, y = rope_value, label = &quot;ROPE&quot;, angle = 90) +
    stat_halfeye(aes(y = .value, x = noise_level_f), data = fits_p_df, position = position_nudge(x = 0.1)) +
    geom_line(aes(y = .value, group = .sample, alpha = 0.3), data = p_df) +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep = &quot;:&quot;)), size = 2) +
    stat_summary(fun.y = median, geom = &quot;point&quot;, shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(y = &quot;scenario_completed&quot;, color = &quot;AX:DX:Noise&quot;) +
    guides(alpha = F)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-57-3.png' %}" width="672" /></p>
<pre class="r"><code># Remove the giant fits data frames
rm(fits_p_df, pars_p_df, pars_p_df_b, pars_p_df_X, pars_p_df_y, p_df, gg_df)</code></pre>
<pre class="r"><code>plot_df = actions %&gt;%
  filter(has_dx == T) %&gt;%
  select(X1, id, user_id, study_condition, start_condition, num_optimal,
         state_idx, state_idx_rescaled,
         age_group, gender, robot_experience,
         noise_level, noise_level_f, has_noise, has_dx, has_ax,
         scenario_completed, num_actions, chose_dx) %&gt;%
  mutate(scenario_completed = factor(scenario_completed)) %&gt;%
  mutate(scenario_completed = fct_recode(scenario_completed, Unresolved=&quot;0&quot;, Resolved = &quot;1&quot;),
         has_ax = fct_recode(has_ax, &quot;No AX&quot;=&quot;FALSE&quot;, &quot;AX&quot;=&quot;TRUE&quot;),
         noise_level = fct_recode(noise_level, &quot;Acc: 100%&quot;=&quot;0.0&quot;, &quot;Acc: 90%&quot;=&quot;1.0&quot;, &quot;Acc: 80%&quot;=&quot;2.0&quot;))

eff = fixef(chose_dx.model.v)

gg_df = plot_df %&gt;%
  mutate(.fill_column = paste(noise_level, !(study_condition == &#39;BASELINE&#39;), sep = &quot;:&quot;)) %&gt;%
  count(user_id, study_condition, .fill_column, chose_dx, num_actions, has_ax, noise_level,
        name = &quot;num_chose_dx&quot;) %&gt;%
  mutate(estimate = -1) %&gt;%
  mutate(estimate = if_else(noise_level == &quot;Acc: 100%&quot;, inv_logit_scaled(eff[&quot;noise_level_f1&quot;,&quot;Estimate&quot;] + eff[&quot;Intercept&quot;, &quot;Estimate&quot;]), estimate)) %&gt;%
  mutate(estimate = if_else(noise_level == &quot;Acc: 90%&quot;, inv_logit_scaled(eff[&quot;noise_level_f2&quot;,&quot;Estimate&quot;] + eff[&quot;Intercept&quot;, &quot;Estimate&quot;]), estimate)) %&gt;%
  mutate(estimate = if_else(noise_level == &quot;Acc: 80%&quot;, inv_logit_scaled(eff[&quot;Intercept&quot;, &quot;Estimate&quot;] - eff[&quot;noise_level_f1&quot;,&quot;Estimate&quot;] - eff[&quot;noise_level_f2&quot;,&quot;Estimate&quot;]), estimate)) %&gt;%
  mutate(noise_level = fct_rev(noise_level))

gg_df %&gt;%
  filter(chose_dx == &quot;1&quot;) %&gt;%
  ggplot(aes(noise_level, num_chose_dx / num_actions, group = noise_level, colour = noise_level)) +
    geom_hline(yintercept = inv_logit_scaled(eff[&quot;Intercept&quot;, &quot;Estimate&quot;]),
               size = .7, linetype = &quot;dashed&quot;, color = &quot;grey&quot;) +
    geom_count() +
    geom_boxplot(aes(y = estimate)) +
    annotate(&quot;segment&quot;,
             x = 3,
             y = inv_logit_scaled(eff[&quot;noise_level_f1&quot;,&quot;Estimate&quot;] + eff[&quot;Intercept&quot;, &quot;Estimate&quot;]),
             xend = 2,
             yend = inv_logit_scaled(eff[&quot;noise_level_f2&quot;,&quot;Estimate&quot;] + eff[&quot;Intercept&quot;, &quot;Estimate&quot;]),
             colour = &quot;black&quot;) +
    annotate(&quot;segment&quot;,
             x = 1,
             y = inv_logit_scaled(eff[&quot;Intercept&quot;, &quot;Estimate&quot;] - eff[&quot;noise_level_f1&quot;,&quot;Estimate&quot;] - eff[&quot;noise_level_f2&quot;,&quot;Estimate&quot;]),
             xend = 2,
             yend = inv_logit_scaled(eff[&quot;noise_level_f2&quot;,&quot;Estimate&quot;] + eff[&quot;Intercept&quot;, &quot;Estimate&quot;]),
             color = &quot;black&quot;) +
    annotate(&quot;text&quot;,
             x = 3.5,
             y = inv_logit_scaled(eff[&quot;Intercept&quot;, &quot;Estimate&quot;] - 0.25),
             label = &quot;Mean of all levels&quot;,
             angle = 90,
             group = NA,
             colour = &quot;black&quot;) +
    scale_y_continuous(limits = c(0.0, 1.0), breaks = c(0.0, 0.5, 1.0)) +
    labs(y = &quot;Fraction of participants that resolved fault&quot;, x = NULL) +
    scale_colour_manual(values = c(&quot;#d62728&quot;, &quot;#2ca02c&quot;, &quot;#ff7f0e&quot;)) +
    scale_size_area(max_size = 1) +
    theme_economist_white(base_size = 13) +
    theme(plot.background = element_rect(fill = &quot;white&quot;),
          axis.title.y = element_blank()) +
    legend_none()</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-58-1.png' %}" width="672" /></p>
</div>
</div>
<div id="sus-system-usability-scale" class="section level1">
<h1>SUS: System Usability Scale</h1>
<pre class="r"><code>plot_df = users %&gt;%
  select(X1, id, study_condition, start_condition, num_optimal,
         age_group, gender, robot_experience,
         noise_level, noise_level_f, has_noise, has_dx, has_ax,
         sus, scenario_completed) %&gt;%
  mutate(scenario_completed = factor(scenario_completed))

text_short(report(plot_df))</code></pre>
<pre><code>## The data contains 200 observations of the following variables:
##   - X1: Mean = 99.50, SD = 57.88, range = [0, 199], 0 missing
##   - id: Mean = 339.73, SD = 157.51, range = [75, 590], 0 missing
##   - study_condition: 10 levels: DX_100, n = 20; AX_100, n = 20; DXAX_100, n = 20; DX_90, n = 20; AX_90, n = 20; DXAX_90, n = 20; DX_80, n = 20; AX_80, n = 20; DXAX_80, n = 20 and BASELINE, n = 20
##   - start_condition: 4 entries: dt.kc.default.default.default.empty.kc, n = 50; kc.dt.default.default.default.empty.dt, n = 50; kc.dt.occluding.above_mug.default.empty.dt, n = 50 and 1 other
##   - num_optimal: Mean = 4.50, SD = 1.50, range = [3, 7], 0 missing
##   - age_group: 7 levels: 2, n = 21; 3, n = 49; 4, n = 42; 5, n = 33; 6, n = 19; 7, n = 16 and 8, n = 20
##   - gender: 2 levels: F, n = 73 and M, n = 127
##   - robot_experience: 5 levels: 0, n = 135; 1, n = 32; 2, n = 12; 3, n = 12 and 4, n = 9
##   - noise_level: 3 levels: 0.0, n = 80; 1.0, n = 60 and 2.0, n = 60
##   - noise_level_f: 3 levels: 0.0, n = 80; 1.0, n = 60 and 2.0, n = 60
##   - has_noise: 2 levels: FALSE, n = 80 and TRUE, n = 120
##   - has_dx: 2 levels: FALSE, n = 80 and TRUE, n = 120
##   - has_ax: 2 levels: FALSE, n = 80 and TRUE, n = 120
##   - sus: Mean = 67.09, SD = 22.99, range = [0, 100], 0 missing
##   - scenario_completed: 2 levels: 0, n = 39 and 1, n = 161</code></pre>
<div id="data-5" class="section level2">
<h2>Data</h2>
<pre class="r"><code># Plot by the study condition
plot_df %&gt;%
  mutate(.fill_column = paste(noise_level, !(study_condition == &#39;BASELINE&#39;), sep = &quot;:&quot;)) %&gt;%
  ggplot(aes(study_condition, sus, fill = .fill_column)) +
    geom_boxplot() +
    geom_count() +
    labs(y = &quot;Number unnecessary actions&quot;, fill = &quot;noise:has_suggestions&quot;) +
    scale_fill_economist()</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-61-1.png' %}" width="1440" /></p>
<pre class="r"><code># Visualize the data by the three variables that we care about
gg_df = plot_df %&gt;%
  mutate(.fill_column = paste(noise_level, !(study_condition == &#39;BASELINE&#39;), sep = &quot;:&quot;)) %&gt;%
  mutate(noise_level = fct_rev(noise_level))

p1 = gg_df %&gt;%
  ggplot(aes(has_ax, sus, fill=.fill_column)) +
    geom_boxplot() +
    geom_count() +
    facet_grid(rows = vars(noise_level), cols = vars(has_dx), labeller = label_both) +
    labs(y = &quot;Number unnecessary actions&quot;) +
    scale_fill_economist()

p2 = gg_df %&gt;%
  ggplot(aes(has_dx, sus, fill=.fill_column)) +
    geom_boxplot() +
    geom_count() +
    facet_grid(rows = vars(noise_level), cols = vars(has_ax), labeller = label_both) +
    labs(y = &quot;Number unnecessary actions&quot;) +
    scale_fill_economist()

p3 = gg_df %&gt;%
  mutate(noise_level = fct_rev(noise_level)) %&gt;%
  mutate(has_ax = factor(has_ax)) %&gt;%
  mutate(has_ax = fct_rev(has_ax)) %&gt;%
  ggplot(aes(noise_level, sus, fill=.fill_column)) +
    geom_boxplot() +
    geom_count() +
    facet_grid(rows = vars(has_ax), cols = vars(has_dx), labeller = label_both) +
    labs(y = &quot;Number unnecessary actions&quot;) +
    scale_fill_economist()

grid.arrange(p1, p2, p3, ncol = 3)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-61-2.png' %}" width="1440" /></p>
<pre class="r"><code># Visualize also the variation in each of the three levels that we care about
p1 = gg_df %&gt;%
  ggplot(aes(sus)) +
    geom_histogram(binwidth = 10) +
    facet_grid(rows = vars(has_ax)) +
    ylim(0, 30) +
    scale_fill_economist() +
    legend_none()

p2 = gg_df %&gt;%
  ggplot(aes(sus)) +
    geom_histogram(binwidth = 10) +
    facet_grid(rows = vars(has_dx)) +
    ylim(0, 30) +
    scale_fill_economist() +
    theme(legend.position = &quot;bottom&quot;)

p3 = gg_df %&gt;%
  ggplot(aes(sus)) +
    geom_histogram(binwidth = 10) +
    facet_grid(rows = vars(noise_level)) +
    ylim(0, 30) +
    scale_fill_economist() +
    legend_none()

grid.arrange(p1, p2, p3, ncol = 3)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-61-3.png' %}" width="1440" /></p>
</div>
<div id="model-5" class="section level2">
<h2>Model</h2>
<p>Based on the data, we assume a skew-normal linear model:</p>
<p><span class="math display">\[sus_i = SkewNormal(\mu_i, \sigma, \alpha)\]</span> <span class="math display">\[\begin{aligned}
\mu_i &amp;= \beta_0 + \beta_{ax}\text{ax}_i + \beta_{dx}\text{dx}_i + \beta_{noise}\text{noise}_i +\\ &amp;\beta_{ax:noise}\text{ax}_i\text{noise}_i + \beta_{dx:noise}\text{dx}_i\text{noise}_i + \\
&amp;\beta_{no}\text{no}_i + \mathbf{\beta_{demo}X_{demo,i}}
\end{aligned}\]</span> <span class="math display">\[\begin{aligned}
\beta_{.} &amp;\sim Normal(0, 10) \\
\sigma &amp;\sim HalfStudent(3, 0, 22) \\
\alpha &amp;\sim Normal(0, 4)
\end{aligned}\]</span></p>
<p>The prior for the <span class="math inline">\(\sigma, \alpha\)</span> parameters are the default used in <code>brms</code>; I see no need to change it.</p>
<p>Model fitting:</p>
<pre class="r"><code># A null model to compare against
sus.model.null = brm(
  sus ~ 0 + Intercept,
  family = &quot;skew_normal&quot;,
  prior = c(set_prior(&quot;normal(0, 10)&quot;, class = &quot;b&quot;),
            set_prior(&quot;student_t(3, 0, 22)&quot;, class = &quot;sigma&quot;),
            set_prior(&quot;normal(0, 4)&quot;, class = &quot;alpha&quot;)),
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
sus.model.null = add_criterion(sus.model.null, &quot;waic&quot;)
sus.model.null = add_criterion(sus.model.null, &quot;loo&quot;, reloo = T)
saveModel(sus.model.null)

# The trend model to see if there is a trend in the noise level variable
sus.model.t = brm(
  sus ~ 0 + Intercept + ((has_dx + has_ax) * noise_level) + (gender + age_group + robot_experience) + num_optimal,
  family = &quot;skew_normal&quot;,
  prior = c(set_prior(&quot;normal(0, 10)&quot;, class = &quot;b&quot;),
            set_prior(&quot;student_t(3, 0, 22)&quot;, class = &quot;sigma&quot;),
            set_prior(&quot;normal(0, 4)&quot;, class = &quot;alpha&quot;)),
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
sus.model.t = add_criterion(sus.model.t, &quot;waic&quot;)
sus.model.t = add_criterion(sus.model.t, &quot;loo&quot;, reloo = T)
saveModel(sus.model.t)

# The values model, to see if specific values of the noise level variable are significant
sus.model.v = brm(
  sus ~ 0 + Intercept + ((has_dx + has_ax) * noise_level_f) + (gender + age_group + robot_experience) + num_optimal,
  family = &quot;skew_normal&quot;,
  prior = c(set_prior(&quot;normal(0, 10)&quot;, class = &quot;b&quot;),
            set_prior(&quot;student_t(3, 0, 22)&quot;, class = &quot;sigma&quot;),
            set_prior(&quot;normal(0, 4)&quot;, class = &quot;alpha&quot;)),
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
sus.model.v = add_criterion(sus.model.v, &quot;waic&quot;)
sus.model.v = add_criterion(sus.model.v, &quot;loo&quot;, reloo = T)
saveModel(sus.model.v)
# More model plots are available at:
# https://bookdown.org/ajkurz/Statistical_Rethinking_recoded/counting-and-classification.html

# Attempted RStanarm models: The estimate contrasts doesn&#39;t seem to be as helpful?
# scenario_completed.model.v = stan_glm(
#   scenario_completed ~ ((has_dx + has_ax) * noise_level) + (gender + age_group + robot_experience) + num_optimal,
#   family = binomial(link = &quot;logit&quot;),
#   prior = normal(0, 10),
#   prior_intercept = normal(0, 10),
#   data = plot_df,
#   QR = T,
#   seed = default_seed
# )</code></pre>
<pre class="r"><code># Load the models
sus.model.null = loadModel(&quot;sus.model.null&quot;)
sus.model.t = loadModel(&quot;sus.model.t&quot;)
sus.model.v = loadModel(&quot;sus.model.v&quot;)</code></pre>
<p>Model fitting results:</p>
<pre class="r"><code># print(summary(sus.model.null))
print(performance::r2(sus.model.null))</code></pre>
<pre><code>## # Bayesian R2 with Standard Error
##
##   Conditional R2: 0.000 [0.000]</code></pre>
<pre class="r"><code># Print the parameters, and some initial diagnostics
print(tidy_stan(sus.model.t))</code></pre>
<pre><code>## Warning: &#39;tidy_stan&#39; is deprecated.
## Use &#39;parameters::model_parameters()&#39; instead.
## See help(&quot;Deprecated&quot;)</code></pre>
<pre><code>##
## Summary Statistics of Stan-Model
##
## # Fixed effects
##
##              Parameter Estimate Std.Error        HDI(89%)     pd  ESS Rhat MCSE
##            (Intercept)    55.88      4.61 [ 48.68, 62.90] 100.0% 2225 1.00 0.10
##                has_dx1     0.16      1.60 [ -2.42,  2.80]  54.1% 3543 1.00 0.03
##                has_ax1    -1.71      1.59 [ -4.32,  0.79]  86.7% 3788 1.00 0.03
##          noise_level.L    -0.11      2.38 [ -3.99,  3.97]  52.0% 3241 1.00 0.04
##          noise_level.Q     1.05      2.58 [ -3.10,  5.48]  66.0% 3369 1.00 0.05
##                gender1     1.30      1.35 [ -0.93,  3.40]  82.8% 4380 1.00 0.02
##             age_group1    -1.36      3.44 [ -6.89,  4.26]  64.6% 4297 1.00 0.05
##             age_group2     3.72      2.44 [ -0.52,  7.53]  92.7% 4060 1.00 0.04
##             age_group3     3.82      2.74 [ -0.61,  8.40]  91.5% 3791 1.00 0.05
##             age_group4     3.19      3.05 [ -1.47,  8.36]  85.5% 4528 1.00 0.05
##             age_group5    -0.96      3.43 [ -6.55,  4.63]  61.1% 3921 1.00 0.06
##             age_group6    -0.55      3.80 [ -6.06,  5.86]  55.3% 3909 1.00 0.06
##      robot_experience1     3.07      2.40 [ -0.53,  6.90]  91.2% 3484 1.00 0.04
##      robot_experience2     2.87      3.17 [ -2.01,  8.18]  81.9% 4033 1.00 0.05
##      robot_experience3     0.74      4.28 [ -6.04,  7.68]  57.0% 3939 1.00 0.07
##      robot_experience4    -8.53      3.91 [-14.99, -2.36]  97.3% 4200 1.00 0.06
##            num_optimal     1.35      0.86 [ -0.01,  2.74]  94.7% 2658 1.00 0.02
##  has_dx1.noise_level.L     2.10      2.56 [ -2.24,  6.01]  79.0% 3155 1.00 0.05
##  has_dx1.noise_level.Q     0.59      2.58 [ -3.52,  4.91]  59.3% 3713 1.00 0.04
##  has_ax1.noise_level.L    -0.99      2.50 [ -5.15,  2.94]  65.6% 3284 1.00 0.04
##  has_ax1.noise_level.Q     2.79      2.77 [ -1.57,  7.10]  85.0% 3433 1.00 0.05</code></pre>
<pre class="r"><code>print(performance::r2(sus.model.t))</code></pre>
<pre><code>## # Bayesian R2 with Standard Error
##
##   Conditional R2: 0.102 [0.025]</code></pre>
<pre class="r"><code>print(tidy_stan(sus.model.v))</code></pre>
<pre><code>## Warning: &#39;tidy_stan&#39; is deprecated.
## Use &#39;parameters::model_parameters()&#39; instead.
## See help(&quot;Deprecated&quot;)</code></pre>
<pre><code>##
## Summary Statistics of Stan-Model
##
## # Fixed effects
##
##               Parameter Estimate Std.Error        HDI(89%)     pd  ESS Rhat
##             (Intercept)    55.79      4.44 [ 48.62, 63.16] 100.0% 2849 1.00
##                 has_dx1     0.20      1.57 [ -2.16,  2.92]  55.2% 4192 1.00
##                 has_ax1    -1.74      1.63 [ -4.11,  0.86]  86.7% 4176 1.00
##          noise_level_f1     0.41      1.92 [ -2.50,  3.67]  58.8% 4110 1.00
##          noise_level_f2    -0.86      2.09 [ -4.29,  2.49]  65.4% 3642 1.00
##                 gender1     1.29      1.37 [ -0.78,  3.55]  83.2% 5176 1.00
##              age_group1    -1.49      3.40 [ -6.91,  4.32]  67.2% 5272 1.00
##              age_group2     3.62      2.49 [ -0.60,  7.65]  92.0% 4699 1.00
##              age_group3     3.71      2.79 [ -0.76,  8.33]  92.0% 4311 1.00
##              age_group4     3.08      3.13 [ -2.16,  7.70]  84.7% 5406 1.00
##              age_group5    -0.78      3.47 [ -6.16,  5.17]  59.2% 4518 1.00
##              age_group6    -0.38      3.72 [ -6.31,  5.54]  54.0% 4386 1.00
##       robot_experience1     3.08      2.38 [ -0.70,  6.91]  90.5% 3739 1.00
##       robot_experience2     2.84      3.17 [ -2.17,  8.07]  81.9% 3717 1.00
##       robot_experience3     0.63      4.19 [ -5.86,  7.85]  55.8% 3911 1.00
##       robot_experience4    -8.54      4.08 [-14.67, -1.47]  96.4% 4339 1.00
##             num_optimal     1.39      0.86 [  0.07,  2.81]  95.3% 3239 1.00
##  has_dx1:noise_level_f1    -1.27      1.93 [ -4.19,  2.00]  73.5% 4019 1.00
##  has_dx1:noise_level_f2    -0.51      2.17 [ -3.89,  3.07]  59.9% 3469 1.00
##  has_ax1:noise_level_f1     1.88      1.96 [ -1.21,  4.99]  82.2% 4200 1.00
##  has_ax1:noise_level_f2    -2.33      2.22 [ -6.05,  1.23]  85.0% 3455 1.00
##  MCSE
##  0.09
##  0.02
##  0.02
##  0.03
##  0.04
##  0.02
##  0.05
##  0.04
##  0.04
##  0.04
##  0.05
##  0.06
##  0.04
##  0.05
##  0.07
##  0.06
##  0.02
##  0.03
##  0.04
##  0.03
##  0.04</code></pre>
<pre class="r"><code>print(performance::r2(sus.model.v))</code></pre>
<pre><code>## # Bayesian R2 with Standard Error
##
##   Conditional R2: 0.103 [0.025]</code></pre>
</div>
<div id="diagnostic-5" class="section level2">
<h2>Diagnostic</h2>
<pre class="r"><code># Alternative diagnostics: diagnostic_posterior(scenario_completed.model.v)

# LOO cross-validation results
print(sus.model.t$criteria$loo$estimates)</code></pre>
<pre><code>##            Estimate        SE
## elpd_loo -903.97712  9.321776
## p_loo      18.07862  2.336033
## looic    1807.95424 18.643552</code></pre>
<pre class="r"><code>print(sus.model.v$criteria$loo$estimates)</code></pre>
<pre><code>##            Estimate        SE
## elpd_loo -904.25425  9.295856
## p_loo      18.13493  2.334296
## looic    1808.50849 18.591712</code></pre>
<pre class="r"><code>print(loo_compare(sus.model.null,
                  sus.model.t,
                  sus.model.v))</code></pre>
<pre><code>##                elpd_diff se_diff
## sus.model.null   0.0       0.0
## sus.model.t    -12.5       4.0
## sus.model.v    -12.8       4.0</code></pre>
<pre class="r"><code># Diagnostics of the sampling process
par(mfrow=c(1,2))
plot(sus.model.t$criteria$loo, main = &quot;Trend Model&quot;)
plot(sus.model.v$criteria$loo, main = &quot;Values Model&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-65-1.png' %}" width="1440" /></p>
<pre class="r"><code># Include posterior predictive checks of the sampling (are sampling according
# to the data that we actually have?)
mcmc_intervals(as.matrix(sus.model.null), regex_pars = &quot;b_&quot;, prob = 0.90, prob_outer = 0.95) +
  ggtitle(&quot;Null Model&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-65-2.png' %}" width="1440" /></p>
<pre class="r"><code>p1 = pp_check(sus.model.t, group = &quot;has_ax&quot;, stat = &quot;median&quot;) + ggtitle(&quot;t:has_ax&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;dens_overlay&#39; by default.</code></pre>
<pre><code>## Warning: The following arguments were unrecognized and ignored: stat, group</code></pre>
<pre class="r"><code>p2 = pp_check(sus.model.t, group = &quot;has_dx&quot;) + ggtitle(&quot;t:has_dx&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;dens_overlay&#39; by default.</code></pre>
<pre><code>## Warning: The following arguments were unrecognized and ignored: group</code></pre>
<pre class="r"><code>p3 = pp_check(sus.model.t, group = &quot;noise_level&quot;) + ggtitle(&quot;t:noise_level&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;dens_overlay&#39; by default.</code></pre>
<pre><code>## Warning: The following arguments were unrecognized and ignored: group</code></pre>
<pre class="r"><code>grid.arrange(p1, p2, p3, nrow = 3)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-65-3.png' %}" width="1440" /></p>
<pre class="r"><code>mcmc_intervals(as.matrix(sus.model.t), regex_pars = &quot;b_&quot;, prob = 0.90, prob_outer = 0.95) +
  ggtitle(&quot;Trends Model&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-65-4.png' %}" width="1440" /></p>
<pre class="r"><code>p1 = pp_check(sus.model.v, group = &quot;has_ax&quot;) + ggtitle(&quot;v:has_ax&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;dens_overlay&#39; by default.</code></pre>
<pre><code>## Warning: The following arguments were unrecognized and ignored: group</code></pre>
<pre class="r"><code>p2 = pp_check(sus.model.v, group = &quot;has_dx&quot;) + ggtitle(&quot;v:has_dx&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;dens_overlay&#39; by default.</code></pre>
<pre><code>## Warning: The following arguments were unrecognized and ignored: group</code></pre>
<pre class="r"><code>p3 = pp_check(sus.model.v, group = &quot;noise_level_f&quot;) + ggtitle(&quot;v:noise_level&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;dens_overlay&#39; by default.</code></pre>
<pre><code>## Warning: The following arguments were unrecognized and ignored: group</code></pre>
<pre class="r"><code>grid.arrange(p1, p2, p3, nrow = 3)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-65-5.png' %}" width="1440" /></p>
<pre class="r"><code>mcmc_intervals(as.matrix(sus.model.v), regex_pars = &quot;b_&quot;, prob = 0.90, prob_outer = 0.95) +
  ggtitle(&quot;Values Model&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-65-6.png' %}" width="1440" /></p>
<pre class="r"><code># Plot predictions vs original
preds_p_df = sus.model.null %&gt;% predict() %&gt;% as_tibble()
p1 =
  preds_p_df %&gt;%
    add_column(sus = sus.model.null$data$sus) %&gt;%
    ggplot(aes(x = sus, y = Estimate)) +
      geom_point() +
      labs(y = &quot;Predicted values&quot;) +
      ggtitle(&quot;Null Model&quot;)

preds_p_df = sus.model.t %&gt;% predict() %&gt;% as_tibble()
p3 =
  preds_p_df %&gt;%
    add_column(sus = sus.model.t$data$sus) %&gt;%
    ggplot(aes(x = sus, y = Estimate)) +
      geom_point() +
      labs(y = &quot;Predicted values&quot;) +
      ggtitle(&quot;Trends Model&quot;)

preds_p_df = sus.model.v %&gt;% predict() %&gt;% as_tibble()
p5 =
  preds_p_df %&gt;%
    add_column(sus = sus.model.v$data$sus) %&gt;%
    ggplot(aes(x = sus, y = Estimate)) +
      geom_point() +
      labs(y = &quot;Predicted values&quot;) +
      ggtitle(&quot;Values Model&quot;)

grid.arrange(p1, p3, p5, nrow = 1, ncol = 3)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-65-7.png' %}" width="1440" /></p>
</div>
<div id="inference-5" class="section level2">
<h2>Inference</h2>
<pre class="r"><code># Compute the bayes factor for each parameter value. Note: we compare
# here against the ROPE, and not the degenerate null hypothesis of 0
# Bayes-Factor values would be more significant in the latter case
# According to the Raftery, 1995 rules
# bf = 1 - 3: Weak
# bf = 3 - 20: Positive
# bf = 20 - 150: Strong
# bf &gt; 150: Very strong
print(bayesfactor_rope(sus.model.null), n = 30)
print(bayesfactor_rope(sus.model.t), n = 30)
print(bayesfactor_rope(sus.model.v), n = 30)</code></pre>
<pre class="r"><code># Compare the models. Note: we cannot do this because we don&#39;t have enough samples.
# The default is 4000; apparently these functions are only meaningful with 40000
# comparison = bayesfactor_models(scenario_completed.model.t, scenario_completed.model.v,
#                                 denominator = scenario_completed.model.null)
# print(comparison)
# print(bayesfactor_inclusion(comparison))

# # If we&#39;re using the brms functions, then use the following (make sure to update!)
# common_hyp_to_test = c(&quot;-2 * has_ax1 = 0&quot;, &quot;-2 * has_dx1 = 0&quot;)
# noise_levels_hyp_to_test = c(
#   &quot;Intercept-noise_level_f1 = 0&quot;, &quot;Intercept-noise_level_f2 = 0&quot;, &quot;Intercept-noise_level_f1-noise_level_f2 = 0&quot;,
#   &quot;Intercept-has_ax1:noise_level_f1 = 0&quot;, &quot;Intercept-has_dx1:noise_level_f1 = 0&quot;,
#   &quot;Intercept-has_ax1:noise_level_f2 = 0&quot;, &quot;Intercept-has_dx1:noise_level_f2 = 0&quot;,
#   &quot;Intercept-has_ax1:noise_level_f1-has_ax1:noise_level_f2 = 0&quot;,
#   &quot;Intercept-has_dx1:noise_level_f1-has_dx1:noise_level_f2 = 0&quot;
# )
# hyp_results = test_hypotheses(hypotheses_list = c(common_hyp_to_test, noise_levels_hyp_to_test), model = scenario_completed.model.v)
# print(hyp_results$hypothesis)
# plot(hyp_results, ask=F)

options(digits = 3)
# Check the significance of each hypothesis
h_df = as_tibble(insight::get_parameters(sus.model.t)) %&gt;%
  transmute(
    Intercept = b_Intercept,
    has_ax_test = -2 * b_has_ax1,
    has_dx_test = -2 * b_has_dx1,
    noise_levelL_test = b_noise_level.L,
    noise_levelQ_test = b_noise_level.Q
  )

hyp_results = test_hypotheses(h_df, rope_values = c(-2.3, 2.3))
print(hyp_results %&gt;% select(Parameter, pd, HDI_low, HDI_high, ROPE_Percentage, ROPE_Equivalence) %&gt;% as.matrix())</code></pre>
<pre><code>##      Parameter           pd      HDI_low  HDI_high ROPE_Percentage
## [1,] &quot;Intercept&quot;         &quot;1.000&quot; &quot; 38.55&quot; &quot;71.82&quot;  &quot;0.000&quot;
## [2,] &quot;has_ax_test&quot;       &quot;0.867&quot; &quot; -6.95&quot; &quot;15.03&quot;  &quot;0.335&quot;
## [3,] &quot;has_dx_test&quot;       &quot;0.541&quot; &quot;-11.63&quot; &quot;13.70&quot;  &quot;0.518&quot;
## [4,] &quot;noise_levelL_test&quot; &quot;0.520&quot; &quot; -8.60&quot; &quot; 8.96&quot;  &quot;0.647&quot;
## [5,] &quot;noise_levelQ_test&quot; &quot;0.660&quot; &quot; -9.61&quot; &quot; 9.56&quot;  &quot;0.579&quot;
##      ROPE_Equivalence
## [1,] &quot;Rejected&quot;
## [2,] &quot;Undecided&quot;
## [3,] &quot;Undecided&quot;
## [4,] &quot;Undecided&quot;
## [5,] &quot;Undecided&quot;</code></pre>
<pre class="r"><code>h_df = as_tibble(insight::get_parameters(sus.model.v)) %&gt;%
  transmute(
    Intercept = b_Intercept,
    has_ax_test = -2 * b_has_ax1,
    has_dx_test = -2 * b_has_dx1,
    noise_level1_test = b_noise_level_f1,
    noise_level2_test = b_noise_level_f2,
    noise_level3_test = - b_noise_level_f1 - b_noise_level_f2
  )

hyp_results = test_hypotheses(h_df, rope_values = c(-2.3, 2.3))
print(hyp_results %&gt;% select(Parameter, pd, HDI_low, HDI_high, ROPE_Percentage, ROPE_Equivalence) %&gt;% as.matrix())</code></pre>
<pre><code>##      Parameter           pd      HDI_low  HDI_high ROPE_Percentage
## [1,] &quot;Intercept&quot;         &quot;1.000&quot; &quot; 37.56&quot; &quot;70.59&quot;  &quot;0.000&quot;
## [2,] &quot;has_ax_test&quot;       &quot;0.867&quot; &quot; -6.30&quot; &quot;15.69&quot;  &quot;0.329&quot;
## [3,] &quot;has_dx_test&quot;       &quot;0.552&quot; &quot;-11.34&quot; &quot;10.88&quot;  &quot;0.527&quot;
## [4,] &quot;noise_level1_test&quot; &quot;0.588&quot; &quot; -6.85&quot; &quot; 6.95&quot;  &quot;0.758&quot;
## [5,] &quot;noise_level2_test&quot; &quot;0.654&quot; &quot; -8.86&quot; &quot; 8.25&quot;  &quot;0.684&quot;
## [6,] &quot;noise_level3_test&quot; &quot;0.573&quot; &quot; -6.81&quot; &quot; 9.60&quot;  &quot;0.683&quot;
##      ROPE_Equivalence
## [1,] &quot;Rejected&quot;
## [2,] &quot;Undecided&quot;
## [3,] &quot;Undecided&quot;
## [4,] &quot;Undecided&quot;
## [5,] &quot;Undecided&quot;
## [6,] &quot;Undecided&quot;</code></pre>
<pre class="r"><code>options(digits = 7)</code></pre>
<p><strong>Results</strong>:</p>
<p>There is no significant effect of any of the suggestions parameters on the SUS.</p>
<p>(The ROPE is defined as [-2.3, 2.3]. It corresponds to <code>0.1 * SD</code> of the output)</p>
<p>There are no posterior plots to show (all effects are supposedly non-existent).</p>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->
<script>
$(document).ready(function () {
  window.initializeCodeFolding("hide" === "show");
});
</script>

<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3,h4",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
