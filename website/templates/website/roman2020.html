{% load static %}

<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>RO-MAN 2020</title>

<script src="{% static 'roman2020_libs/jquery-1.11.3/jquery.min.js' %}"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="{% static 'roman2020_libs/bootstrap-3.3.5/css/bootstrap.min.css' %}" rel="stylesheet" />
<script src="{% static 'roman2020_libs/bootstrap-3.3.5/js/bootstrap.min.js' %}"></script>
<script src="{% static 'roman2020_libs/bootstrap-3.3.5/shim/html5shiv.min.js' %}"></script>
<script src="{% static 'roman2020_libs/bootstrap-3.3.5/shim/respond.min.js' %}"></script>
<script src="{% static 'roman2020_libs/jqueryui-1.11.4/jquery-ui.min.js' %}"></script>
<link href="{% static 'roman2020_libs/tocify-1.9.1/jquery.tocify.css' %}" rel="stylesheet" />
<script src="{% static 'roman2020_libs/tocify-1.9.1/jquery.tocify.js' %}"></script>
<script src="{% static 'roman2020_libs/navigation-1.1/tabsets.js' %}"></script>
<script src="{% static 'roman2020_libs/navigation-1.1/codefolding.js' %}"></script>
<link href="{% static 'roman2020_libs/highlightjs-9.12.0/default.css' %}" rel="stylesheet" />
<script src="{% static 'roman2020_libs/highlightjs-9.12.0/highlight.js' %}"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>



<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->
<style type="text/css">
.code-folding-btn { margin-bottom: 4px; }
</style>



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="fluid-row" id="header">

<div class="btn-group pull-right">
<button type="button" class="btn btn-default btn-xs dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"><span>Code</span> <span class="caret"></span></button>
<ul class="dropdown-menu" style="min-width: 50px;">
<li><a id="rmd-show-all-code" href="#">Show All Code</a></li>
<li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li>
</ul>
</div>



<h1 class="title toc-ignore">RO-MAN 2020</h1>

</div>


<pre class="r"><code>library(car)
library(stats)
library(grid)
library(gridExtra)
library(fitdistrplus)
# library(dplyr)
# library(forcats)
# library(tibble)
# library(tidyr)
# library(readr)
# library(ggplot2)
# library(stringr)
library(tidyverse)
library(ggsignif)
library(ggthemes)
library(jtools)
library(broom)
library(modelr)
library(loo)
# library(rstanarm)
library(brms)
library(bayesplot)
library(GGally)
library(tidybayes)
library(bayestestR)
library(sjstats)
library(report)
library(see)

# Setup for multiprocessing
options(mc.cores = 4)
# library(future)
# plan(multiprocess)

# Make the default coding of contrasts sum-coding; just in case
options(contrasts = c(&quot;contr.sum&quot;, &quot;contr.poly&quot;))</code></pre>
<p>Global options:</p>
<pre class="r"><code># Script execution globals
train_models = F     # Retrain the MCMC models. Also infer unknown demographics
plot_posteriors = T  # Plot the posterior distributions
plot_diagnostics = T # Plot the diagnostics
report_bayesfactors = F  # Report the Bayes Factor scores of model parameters
default_seed = 0x1331 # For repeatable models and diagnostics
data_folder = &quot;~/Documents/GT/Research/Data/arbitration/2019-12-09/results&quot;</code></pre>
<p>Helper functions:</p>
<pre class="r"><code># Create a simple coding contrasts matrix
contr.deviation = function(nlevels) {
  x1 = contr.treatment(nlevels)
  x2 = matrix(rep(1/nlevels, (nlevels-1) * nlevels), ncol = (nlevels-1))
  return(x1-x2)
}

# NOTE: For now we&#39;re ignoring this advice
# # Use a simple coding contrasts matrix to add our own contrasts columns
# # Explained in: http://talklab.psy.gla.ac.uk/tvw/catpred/
# create_simple_contrasts = function(d, column_name) {
#  factor_column = d %&gt;% pull(column_name)
#  contrast_matrix = contr.deviation(length(levels(factor_column)))
# }

# Save a model
saveModel = function(model, folder = data_folder) {
  saveRDS(model, file = file.path(folder, paste(substitute(model), &quot;.rds&quot;, sep = &#39;&#39;)))
}

# Load a model
loadModel = function(model_name, folder = data_folder) {
  return(readRDS(file.path(folder, paste(model_name, &#39;.rds&#39;, sep = &#39;&#39;))))
}

# Test hypotheses. The NULL arguments need to be provided in this case
test_hypotheses = function(h_df = NULL, rope_values = NULL, hypotheses_list = NULL, model = NULL) {
  # Option 1: Use the hypothesis function in brms
  if (!is.null(hypotheses_list)) {
    hyp_results = hypothesis(model, hypothesis = hypotheses_list, seed = default_seed)
    return(hyp_results)
  }

  # Option 2: Use the ROPE &amp; Overlap amount
  else {
    if (is.null(rope_values)) {
      rope_values = c(-0.1, 0.1)
    }
    hyp_results =
      h_df %&gt;%
        equivalence_test(range = rope_values, ci = 1.0) %&gt;%
        as_tibble() %&gt;%
        bind_cols(h_df %&gt;% pd() %&gt;% select(pd))
    return(hyp_results)
  }
}

# Fit distributions to vectors and print the results. Also return the fit, if necessary
fit_and_print_dist = function(vec, distr, string, use_mass = F) {
  if (!use_mass) {
    f = fitdist(vec, distr)
  } else {
    f = MASS::fitdistr(vec, distr)
  }
  print(paste(string, f$loglik, f$aic))
  return(f)
}</code></pre>
<p>Data loading:</p>
<pre class="r"><code># Load the CSV files. If the age_group fill model is run again and we get a different output,
# then remember to update the value here
loadCSV = function(filename, age_group_fill, contrast_func) {
  dat = read_csv(
    file.path(data_folder, filename),
    col_types = cols(
      study_condition = col_factor(),
      noise_level = col_factor(ordered = T),
      gender = col_factor(levels = c(&quot;F&quot;, &quot;M&quot;, &quot;U&quot;)),
      age_group = col_factor(levels = seq(from = 0, to = 8)),
      robot_experience = col_factor(levels = seq(from = 0, to = 4))
    )
  )

  # Relabel the factors
  dat = dat %&gt;%
    mutate(study_condition = fct_recode(study_condition,
                                        BASELINE=&quot;1&quot;,
                                        DX_100=&quot;2&quot;, AX_100=&quot;3&quot;, DXAX_100=&quot;4&quot;,
                                        DX_90=&quot;5&quot;, AX_90=&quot;6&quot;, DXAX_90=&quot;7&quot;,
                                        DX_80=&quot;8&quot;, AX_80=&quot;9&quot;, DXAX_80=&quot;10&quot;)) %&gt;%
    mutate(study_condition = fct_relevel(study_condition, c(&quot;DX_100&quot;, &quot;AX_100&quot;, &quot;DXAX_100&quot;,
                                                            &quot;DX_90&quot;, &quot;AX_90&quot;, &quot;DXAX_90&quot;,
                                                            &quot;DX_80&quot;, &quot;AX_80&quot;, &quot;DXAX_80&quot;)))

  # Relevel the non-binary gender
  dat$gender[dat$gender == &#39;U&#39;] = &#39;M&#39;

  # Change binary responses to integers
  dat$scenario_completed = as.integer(dat$scenario_completed)

  # Relevel age_group
  dat[dat$age_group == 0,]$age_group = age_group_fill

  # Create an unordered noise_level. Also drop unused levels
  dat$age_group = droplevels(dat$age_group)
  dat$gender = droplevels(dat$gender)
  dat$noise_level_f = factor(dat$noise_level, ordered = F)

  # Set the contrasts for everything
  contrasts(dat$age_group) = contrast_func(length(levels(dat$age_group)))
  contrasts(dat$robot_experience) = contrast_func(length(levels(dat$robot_experience)))
  contrasts(dat$gender) = contrast_func(length(levels(dat$gender)))

  contrasts(dat$has_ax) = contrast_func(2)
  contrasts(dat$has_dx) = contrast_func(2)
  contrasts(dat$has_noise) = contrast_func(2)
  contrasts(dat$has_dxax) = contrast_func(2)

  contrasts(dat$noise_level_f) = contrast_func(length(levels(dat$noise_level_f)))

  # Return the data frame
  return(dat)
}

# Get the users df and the actions df
users = loadCSV(&quot;users.csv&quot;, 4, contr.sum)
actions = loadCSV(&quot;actions.csv&quot;, 4, contr.sum)

# Change more binary responses to integers
actions$optimal_ax = as.integer(actions$optimal_ax)
actions$chose_ax = as.integer(actions$chose_ax)
actions$optimal_dx = as.integer(actions$optimal_dx)
actions$chose_dx = as.integer(actions$optimal_dx)

# Relabel user ids to be in the range 1-200. Otherwise, we&#39;re using DB ids
# The user ID column in the users table that we can now join on is X1
actions$user_id = actions %&gt;% group_indices(user_id)

# Rescale state ids
actions$state_idx_rescaled = scales::rescale(actions$state_idx)

# Code to relevel the age group factor to remove unused levels. Also, predict the
# value of the 1 unknown age from the other indicators of demographics
# NOTE: If we retrain the model and the output doesn&#39;t match the hardcoded
# values, then update those values
if (train_models) {
  plot_df = subset(users, users$age_group != 0)
  plot_df$age_group = factor(plot_df$age_group, levels = seq(from = 0, to = 8), ordered = T)
  plot_df$age_group = droplevels(plot_df$age_group)
  age_group_model = brm(age_group ~ gender + robot_experience, data = plot_df, family = &quot;cumulative&quot;)
  data_to_predict = users %&gt;% filter(age_group == 0) %&gt;% select(c(&quot;robot_experience&quot;, &quot;gender&quot;))
  as_tibble(predict(age_group_model, data_to_predict))
# # A tibble: 1 x 7
#   `P(Y = 2)` `P(Y = 3)` `P(Y = 4)` `P(Y = 5)` `P(Y = 6)` `P(Y = 7)` `P(Y = 8)`
#        &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;
# 1     0.0838      0.202      0.215      0.188      0.110      0.089      0.111
  rm(age_group_model)
}</code></pre>
<p>Note that in this notebook, the accuracy variable from the paper is coded as a noise level variable:</p>
<table>
<thead>
<tr class="header">
<th>Accuracy</th>
<th>Noise Level</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>100%</td>
<td>0</td>
</tr>
<tr class="even">
<td>90%</td>
<td>1</td>
</tr>
<tr class="odd">
<td>80%</td>
<td>2</td>
</tr>
</tbody>
</table>
<p>Since noise level increases as accuracy decreases, the “sign” of any linear trends observed in the following analyses should be reversed. Everything else stays pretty much the same.</p>
<p>For each of our dependent variables, there are 2 models:</p>
<ol style="list-style-type: decimal">
<li>With the noise variable as an ordered factor, so that we can make inferences on the trends in the variable</li>
<li>With the noise variable as an unordered factor, so that we can make inferences on the values of the variable</li>
</ol>
<p>We use Deviation / Sum coding in the coding of non-ordered factors so that we can test the influence of individual factor levels over and above the grand mean.</p>
<p>In all our models:</p>
<ul>
<li><span class="math inline">\(\beta_0\)</span> is assumed to be the intercept. In a mixed effects model, this is additionally indexed by <span class="math inline">\(i\)</span>, the user; i.e. <span class="math inline">\(\beta_{0i}\)</span>.</li>
<li><span class="math inline">\(\text{ax}_i\)</span> denotes if sample <span class="math inline">\(i\)</span> received AX suggestions or not</li>
<li><span class="math inline">\(\text{dx}_i\)</span> denotes if sample <span class="math inline">\(i\)</span> received DX suggestions or not</li>
<li><span class="math inline">\(\text{noise}_i\)</span> denotes the level of noise in the suggestions that sample <span class="math inline">\(i\)</span> received. 0 (100% accuracy) if none was present.</li>
<li><span class="math inline">\(\mathbf{X_{demo,i}}\)</span> is a vector of demographic information. For one participant, this information is imputed from a simple linear model of the other participants. Think of it almost as Propensity Score matching.</li>
<li><span class="math inline">\(\text{no}_i\)</span> denotes the number of optimal actions for the scenario present in sample <span class="math inline">\(i\)</span>. This is a proxy for a factor encoding of the start scenario</li>
<li><span class="math inline">\(\text{state}_{ij}\)</span> denotes the state the user <span class="math inline">\(i\)</span> visited on action number <span class="math inline">\(j\)</span>. The sample, in this case, is indexed by <span class="math inline">\(j\)</span>. The states are indexed according to the frequency of users visits (0 = most visited state), and then all the indices are rescaled into the range 0-1.</li>
</ul>
<p>We test the following hypotheses (the explanations are a statement of the null hypotheses; the coefficients are from the expected regression parameters, given sum coding):</p>
<ul>
<li><span class="math inline">\((\beta_0 - \beta_{ax_0}) - (\beta_0 + \beta_{ax_0}) = -2\beta_{ax_0} = 0 \Rightarrow \beta_{ax_0} = 0\)</span>: The main difference in effects from having action suggestions vs. not is not negligible (ceterus paribus)</li>
<li><span class="math inline">\((\beta_0 - \beta_{dx_0}) - (\beta_0 + \beta_{dx_0}) = -2\beta_{dx_0} = 0 \Rightarrow \beta_{dx_0} = 0\)</span>: The main difference in effects from having diagnosis suggestions vs. not is not neglible (ceterus paribus)</li>
<li><span class="math inline">\(\beta_{noise_L} = 0; \beta_{noise_Q} = 0\)</span>: Noise does not have a linear / quadratic effect on the outcome.</li>
</ul>
<p>We do not test the interaction effects (because it’s hard to make sense of what those mean).</p>
<p>The method of reporting and testing is based on the following papers:</p>
<ol style="list-style-type: decimal">
<li><a href="https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.12577">A protocol for conducting and presenting results of regression‐type analyses</a></li>
<li><a href="https://www.frontiersin.org/articles/10.3389/fpsyg.2019.02767/full">Indices of Effect Existence and Significance in the Bayesian Framework</a>. The paper is associated with <a href="https://easystats.github.io/bayestestR/articles/guidelines.html">this post</a> on how to present results, and <a href="https://easystats.github.io/bayestestR/articles/region_of_practical_equivalence.html">this post</a> giving a quick overview of terms (the posts are part of a package I’m using heavily in these analyses)</li>
</ol>
<p>Note, that unlike the previous version of this HTML page, and some of the references, we are NOT going to perform model-selection here. Based on what I’ve read, we’re doing confirmatory hypothesis testing, which is not where one should use model selection paradigms.</p>
<div id="frr-fault-resolution-rate" class="section level1">
<h1>FRR: Fault Resolution Rate</h1>
<p><strong>Did the person complete the scenario or not?</strong></p>
<p>In the code, this variable is called <code>scenario_completed</code>.</p>
<pre class="r"><code>plot_df = users %&gt;%
  select(X1, id, study_condition, start_condition, num_optimal,
         age_group, gender, robot_experience,
         noise_level, noise_level_f, has_noise, has_dx, has_ax,
         scenario_completed)
text_short(report(plot_df))</code></pre>
<pre><code>## Warning in report.numeric(model[[col]], median = median, centrality =
## centrality, : Variable `model[[col]]` contains only two different values.
## Consider converting it to a factor.</code></pre>
<pre><code>## The data contains 200 observations of the following variables:
##   - X1: Mean = 99.50, SD = 57.88, range = [0, 199], 0 missing
##   - id: Mean = 341.38, SD = 159.33, range = [75, 599], 0 missing
##   - study_condition: 10 levels: DX_100, n = 20; AX_100, n = 20; DXAX_100, n = 20; DX_90, n = 20; AX_90, n = 20; DXAX_90, n = 20; DX_80, n = 20; AX_80, n = 20; DXAX_80, n = 20 and BASELINE, n = 20
##   - start_condition: 4 entries: dt.kc.default.default.default.empty.kc, n = 50; kc.dt.default.default.default.empty.dt, n = 50; kc.dt.occluding.above_mug.default.empty.dt, n = 50 and 1 other
##   - num_optimal: Mean = 4.50, SD = 1.50, range = [3, 7], 0 missing
##   - age_group: 7 levels: 2, n = 21; 3, n = 47; 4, n = 44; 5, n = 33; 6, n = 19; 7, n = 16 and 8, n = 20
##   - gender: 2 levels: F, n = 73 and M, n = 127
##   - robot_experience: 5 levels: 0, n = 133; 1, n = 33; 2, n = 12; 3, n = 12 and 4, n = 10
##   - noise_level: 3 levels: 0.0, n = 80; 1.0, n = 60 and 2.0, n = 60
##   - noise_level_f: 3 levels: 0.0, n = 80; 1.0, n = 60 and 2.0, n = 60
##   - has_noise: 2 levels: FALSE, n = 80 and TRUE, n = 120
##   - has_dx: 2 levels: FALSE, n = 80 and TRUE, n = 120
##   - has_ax: 2 levels: FALSE, n = 80 and TRUE, n = 120
##   - scenario_completed: Mean = 0.81, SD = 0.39, range = [0, 1], 0 missing</code></pre>
<div id="data" class="section level2">
<h2>Data</h2>
<pre class="r"><code># Plot by the study condition
plot_df %&gt;%
  mutate(scenario_completed = factor(scenario_completed)) %&gt;%
  mutate(scenario_completed = fct_recode(scenario_completed, incomplete=&quot;0&quot;, complete = &quot;1&quot;)) %&gt;%
  count(study_condition, scenario_completed) %&gt;%
  ggplot(aes(study_condition, n / 20, fill=scenario_completed)) +
    geom_bar(stat=&quot;identity&quot;) +
    labs(y = &quot;Fraction completed&quot;) +
    scale_fill_economist()</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-6-1.png' %}" width="1440" /></p>
<pre class="r"><code># Visualize the data by the three variables that we care about
gg_df = plot_df %&gt;%
  mutate(scenario_completed = factor(scenario_completed)) %&gt;%
  mutate(scenario_completed = fct_recode(scenario_completed, incomplete=&quot;0&quot;, complete = &quot;1&quot;)) %&gt;%
  mutate(noise_level = fct_rev(noise_level)) %&gt;%
  count(has_ax, has_dx, noise_level, scenario_completed)

p1 = gg_df %&gt;%
  ggplot(aes(has_ax, n / 20, fill=scenario_completed)) +
    geom_bar(stat = &quot;identity&quot;) +
    facet_grid(rows = vars(noise_level), cols = vars(has_dx), labeller = label_both) +
    labs(y = &quot;Fraction completed&quot;) +
    legend_none() +
    scale_fill_economist()

p2 = gg_df %&gt;%
  ggplot(aes(has_dx, n / 20, fill=scenario_completed)) +
    geom_bar(stat = &quot;identity&quot;) +
    facet_grid(rows = vars(noise_level), cols = vars(has_ax), labeller = label_both) +
    labs(y = &quot;Fraction completed&quot;) +
    theme(legend.position = &quot;bottom&quot;) +
    scale_fill_economist()

p3 = gg_df %&gt;%
  mutate(noise_level = fct_rev(noise_level)) %&gt;%
  mutate(has_ax = factor(has_ax)) %&gt;%
  mutate(has_ax = fct_rev(has_ax)) %&gt;%
  ggplot(aes(noise_level, n / 20, fill=scenario_completed)) +
    geom_bar(stat = &quot;identity&quot;) +
    facet_grid(rows = vars(has_ax), cols = vars(has_dx), labeller = label_both) +
    labs(y = &quot;Fraction completed&quot;) +
    legend_none() +
    scale_fill_economist()

grid.arrange(p1, p2, p3, ncol = 3)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-6-2.png' %}" width="1440" /></p>
<pre class="r"><code># Visualize also the variation in each of the three levels that we care about
p1 = gg_df %&gt;%
  filter(scenario_completed == &#39;complete&#39;) %&gt;%
  ggplot(aes(has_ax, n/20)) +
    geom_violin() +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep=&quot;:&quot;), size = 2)) +
    stat_summary(fun.y = median, geom=&quot;point&quot;, shape=23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(color = &quot;AX:DX:Noise&quot;) +
    theme(legend.position = &quot;left&quot;) +
    guides(size = F)

p2 = gg_df %&gt;%
  filter(scenario_completed == &#39;complete&#39;) %&gt;%
  ggplot(aes(has_dx, n/20)) +
    geom_violin() +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep=&quot;:&quot;), size = 2)) +
    stat_summary(fun.y = median, geom=&quot;point&quot;, shape=23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    legend_none()

p3 = gg_df %&gt;%
  filter(scenario_completed == &#39;complete&#39;) %&gt;%
  ggplot(aes(noise_level, n/20)) +
    geom_violin() +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep=&quot;:&quot;), size = 2)) +
    stat_summary(fun.y = median, geom=&quot;point&quot;, shape=23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    scale_x_discrete(limits = rev(levels(gg_df$noise_level))) +
    legend_none()

grid.arrange(p1, p2, p3, ncol = 3)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-6-3.png' %}" width="1440" /></p>
</div>
<div id="model" class="section level2">
<h2>Model</h2>
<p>Based on the data, we assume the following structural model:</p>
<p><span class="math display">\[frr_i = Bernoulli(p_i)\]</span> <span class="math display">\[\begin{aligned}
logit^{-1}(p_i) &amp;= \beta_0 + \beta_{ax}\text{ax}_i + \beta_{dx}\text{dx}_i + \beta_{noise}\text{noise}_i +\\ &amp;\beta_{ax:noise}\text{ax}_i\text{noise}_i + \beta_{dx:noise}\text{dx}_i\text{noise}_i + \\
&amp;\beta_{no}\text{no}_i + \mathbf{\beta_{demo}X_{demo,i}}
\end{aligned}\]</span> <span class="math display">\[\beta_{.} \sim Normal(0, 10)\]</span></p>
<p>Model fitting:</p>
<pre class="r"><code># A null model to compare against
scenario_completed.model.null = brm(
  scenario_completed ~ 0 + Intercept,
  family = &quot;bernoulli&quot;,
  prior = set_prior(&quot;normal(0, 10)&quot;, class = &quot;b&quot;),
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
scenario_completed.model.null = add_criterion(scenario_completed.model.null, &quot;waic&quot;)
scenario_completed.model.null = add_criterion(scenario_completed.model.null, &quot;loo&quot;, reloo = T)
saveModel(scenario_completed.model.null)

# The trend model to see if there is a trend in the noise level variable
scenario_completed.model.t = brm(
  scenario_completed ~ 0 + Intercept + ((has_dx + has_ax) * noise_level) + (gender + age_group + robot_experience) + num_optimal,
  family = &quot;bernoulli&quot;,
  prior = set_prior(&quot;normal(0, 10)&quot;, class = &quot;b&quot;),
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
scenario_completed.model.t = add_criterion(scenario_completed.model.t, &quot;waic&quot;)
scenario_completed.model.t = add_criterion(scenario_completed.model.t, &quot;loo&quot;, reloo = T)
saveModel(scenario_completed.model.t)

# The values model, to see if specific values of the noise level variable are significant
scenario_completed.model.v = brm(
  scenario_completed ~ 0 + Intercept + ((has_dx + has_ax) * noise_level_f) + (gender + age_group + robot_experience) + num_optimal,
  family = &quot;bernoulli&quot;,
  prior = set_prior(&quot;normal(0, 10)&quot;, class = &quot;b&quot;),
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
scenario_completed.model.v = add_criterion(scenario_completed.model.v, &quot;waic&quot;)
scenario_completed.model.v = add_criterion(scenario_completed.model.v, &quot;loo&quot;, reloo = T)
saveModel(scenario_completed.model.v)
# More model plots are available at:
# https://bookdown.org/ajkurz/Statistical_Rethinking_recoded/counting-and-classification.html

# Attempted RStanarm models: The estimate contrasts doesn&#39;t seem to be as helpful?
# scenario_completed.model.v = stan_glm(
#   scenario_completed ~ ((has_dx + has_ax) * noise_level) + (gender + age_group + robot_experience) + num_optimal,
#   family = binomial(link = &quot;logit&quot;),
#   prior = normal(0, 10),
#   prior_intercept = normal(0, 10),
#   data = plot_df,
#   QR = T,
#   seed = default_seed
# )</code></pre>
<pre class="r"><code># Load the models
scenario_completed.model.null = loadModel(&quot;scenario_completed.model.null&quot;)
scenario_completed.model.t = loadModel(&quot;scenario_completed.model.t&quot;)
scenario_completed.model.v = loadModel(&quot;scenario_completed.model.v&quot;)</code></pre>
<p>Model fitting results:</p>
<pre class="r"><code># print(summary(scenario_completed.model.null))
print(performance::r2(scenario_completed.model.null))</code></pre>
<pre><code>## # Bayesian R2 with Standard Error
##
##   Conditional R2: 0.000 [0.000]</code></pre>
<pre class="r"><code># Print the parameters, and some initial diagnostics
print(tidy_stan(scenario_completed.model.t))</code></pre>
<pre><code>## Warning: &#39;tidy_stan&#39; is deprecated.
## Use &#39;parameters::model_parameters()&#39; instead.
## See help(&quot;Deprecated&quot;)</code></pre>
<pre><code>##
## Summary Statistics of Stan-Model
##
## # Fixed effects
##
##              Parameter Estimate Std.Error       HDI(89%)     pd  ESS Rhat MCSE
##            (Intercept)     2.33      0.82 [ 0.98,  3.63]  99.9% 2880 1.00 0.02
##                has_dx1     0.25      0.33 [-0.29,  0.79]  78.4% 3061 1.00 0.01
##                has_ax1    -1.12      0.29 [-1.59, -0.65] 100.0% 2834 1.00 0.01
##          noise_level.L    -0.33      0.59 [-1.34,  0.64]  70.8% 2490 1.00 0.01
##          noise_level.Q     1.26      0.51 [ 0.49,  2.12]  99.6% 3063 1.00 0.01
##                gender1    -0.11      0.24 [-0.51,  0.26]  67.4% 4574 1.00 0.00
##             age_group1     0.51      0.75 [-0.70,  1.63]  76.2% 4311 1.00 0.01
##             age_group2    -0.55      0.47 [-1.30,  0.19]  88.0% 4073 1.00 0.01
##             age_group3    -0.29      0.47 [-1.02,  0.44]  73.2% 4312 1.00 0.01
##             age_group4     1.18      0.68 [ 0.15,  2.34]  97.4% 4296 1.00 0.01
##             age_group5    -1.19      0.57 [-2.11, -0.20]  97.0% 3895 1.00 0.01
##             age_group6     1.22      0.87 [-0.19,  2.56]  93.3% 3105 1.00 0.02
##      robot_experience1     1.36      0.44 [ 0.69,  2.10]  99.9% 3514 1.00 0.01
##      robot_experience2     1.02      0.55 [ 0.16,  1.97]  97.0% 4030 1.00 0.01
##      robot_experience3    -1.26      0.73 [-2.44, -0.08]  95.3% 3700 1.00 0.01
##      robot_experience4    -1.08      0.75 [-2.28,  0.04]  92.8% 4402 1.00 0.01
##            num_optimal    -0.25      0.15 [-0.48, -0.01]  95.7% 3036 1.00 0.00
##  has_dx1.noise_level.L    -0.02      0.60 [-0.98,  0.98]  51.4% 2716 1.00 0.01
##  has_dx1.noise_level.Q     0.84      0.58 [-0.06,  1.76]  94.4% 2838 1.00 0.01
##  has_ax1.noise_level.L     0.41      0.49 [-0.34,  1.20]  80.7% 4024 1.00 0.01
##  has_ax1.noise_level.Q    -0.20      0.51 [-1.07,  0.58]  65.5% 3311 1.00 0.01</code></pre>
<pre class="r"><code>print(performance::r2(scenario_completed.model.t))</code></pre>
<pre><code>## # Bayesian R2 with Standard Error
##
##   Conditional R2: 0.267 [0.037]</code></pre>
<pre class="r"><code>print(tidy_stan(scenario_completed.model.v))</code></pre>
<pre><code>## Warning: &#39;tidy_stan&#39; is deprecated.
## Use &#39;parameters::model_parameters()&#39; instead.
## See help(&quot;Deprecated&quot;)</code></pre>
<pre><code>##
## Summary Statistics of Stan-Model
##
## # Fixed effects
##
##               Parameter Estimate Std.Error       HDI(89%)     pd  ESS Rhat MCSE
##             (Intercept)     2.34      0.85 [ 1.07,  3.78]  99.9% 3494 1.00 0.01
##                 has_dx1     0.26      0.34 [-0.29,  0.78]  78.2% 3215 1.00 0.01
##                 has_ax1    -1.11      0.30 [-1.58, -0.63] 100.0% 3906 1.00 0.00
##          noise_level_f1     0.73      0.45 [ 0.02,  1.46]  94.8% 2746 1.00 0.01
##          noise_level_f2    -1.04      0.40 [-1.69, -0.38]  99.4% 3105 1.00 0.01
##                 gender1    -0.11      0.24 [-0.49,  0.28]  67.4% 4667 1.00 0.00
##              age_group1     0.55      0.77 [-0.62,  1.82]  76.9% 4410 1.00 0.01
##              age_group2    -0.56      0.46 [-1.30,  0.20]  87.9% 5130 1.00 0.01
##              age_group3    -0.29      0.48 [-1.08,  0.48]  72.5% 4643 1.00 0.01
##              age_group4     1.19      0.71 [ 0.13,  2.41]  96.4% 4395 1.00 0.01
##              age_group5    -1.19      0.61 [-2.20, -0.23]  97.2% 3897 1.00 0.01
##              age_group6     1.19      0.86 [-0.16,  2.60]  93.4% 3512 1.00 0.01
##       robot_experience1     1.38      0.42 [ 0.64,  1.98] 100.0% 4260 1.00 0.01
##       robot_experience2     1.03      0.57 [ 0.14,  1.96]  96.9% 4121 1.00 0.01
##       robot_experience3    -1.22      0.72 [-2.42, -0.05]  95.2% 4016 1.00 0.01
##       robot_experience4    -1.07      0.71 [-2.24,  0.04]  93.8% 4284 1.00 0.01
##             num_optimal    -0.25      0.15 [-0.49, -0.01]  95.0% 3472 1.00 0.00
##  has_dx1:noise_level_f1     0.37      0.42 [-0.28,  1.06]  80.2% 3311 1.00 0.01
##  has_dx1:noise_level_f2    -0.70      0.45 [-1.47, -0.03]  95.1% 3187 1.00 0.01
##  has_ax1:noise_level_f1    -0.37      0.41 [-1.02,  0.29]  81.4% 3511 1.00 0.01
##  has_ax1:noise_level_f2     0.16      0.41 [-0.47,  0.86]  65.2% 3575 1.00 0.01</code></pre>
<pre class="r"><code>print(performance::r2(scenario_completed.model.v))</code></pre>
<pre><code>## # Bayesian R2 with Standard Error
##
##   Conditional R2: 0.266 [0.038]</code></pre>
</div>
<div id="diagnostic" class="section level2">
<h2>Diagnostic</h2>
<pre class="r"><code># Alternative diagnostics: diagnostic_posterior(scenario_completed.model.v)

# LOO cross-validation results
print(scenario_completed.model.null$criteria$loo$estimates)</code></pre>
<pre><code>##           Estimate         SE
## elpd_loo -98.28041  8.1859369
## p_loo      1.03581  0.1164151
## looic    196.56083 16.3718739</code></pre>
<pre class="r"><code>print(scenario_completed.model.t$criteria$loo$estimates)</code></pre>
<pre><code>##            Estimate        SE
## elpd_loo -105.49176 11.996691
## p_loo      30.47546  5.046771
## looic     210.98352 23.993381</code></pre>
<pre class="r"><code>print(scenario_completed.model.v$criteria$loo$estimates)</code></pre>
<pre><code>##            Estimate        SE
## elpd_loo -105.82929 12.128629
## p_loo      30.79796  5.201954
## looic     211.65859 24.257258</code></pre>
<pre class="r"><code>print(loo_compare(scenario_completed.model.null,
                  scenario_completed.model.t,
                  scenario_completed.model.v))</code></pre>
<pre><code>##                               elpd_diff se_diff
## scenario_completed.model.null  0.0       0.0
## scenario_completed.model.t    -7.2       8.6
## scenario_completed.model.v    -7.5       8.7</code></pre>
<pre class="r"><code># Diagnostics of the sampling process
par(mfrow=c(1,2))
plot(scenario_completed.model.t$criteria$loo, main = &quot;Trend Model&quot;)
plot(scenario_completed.model.v$criteria$loo, main = &quot;Values Model&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-10-1.png' %}" width="1440" /></p>
<pre class="r"><code># Include posterior predictive checks of the sampling (are sampling according
# to the data that we actually have?)
mcmc_intervals(as.matrix(scenario_completed.model.null), regex_pars = &quot;b_&quot;, prob = 0.90, prob_outer = 0.95) +
  ggtitle(&quot;Null Model&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-10-2.png' %}" width="1440" /></p>
<pre class="r"><code>p1 = pp_check(scenario_completed.model.t, type = &quot;bars_grouped&quot;, group = &quot;has_ax&quot;, stat = &quot;median&quot;) + ggtitle(&quot;t:has_ax&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars_grouped&#39; by default.</code></pre>
<pre><code>## Warning: The following arguments were unrecognized and ignored: stat</code></pre>
<pre class="r"><code>p2 = pp_check(scenario_completed.model.t, type = &quot;bars_grouped&quot;, group = &quot;has_dx&quot;) + ggtitle(&quot;t:has_dx&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars_grouped&#39; by default.</code></pre>
<pre class="r"><code>p3 = pp_check(scenario_completed.model.t, type = &quot;bars_grouped&quot;, group = &quot;noise_level&quot;) + ggtitle(&quot;t:noise_level&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars_grouped&#39; by default.</code></pre>
<pre class="r"><code>grid.arrange(p1, p2, p3, nrow = 3)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-10-3.png' %}" width="1440" /></p>
<pre class="r"><code>mcmc_intervals(as.matrix(scenario_completed.model.t), regex_pars = &quot;b_&quot;, prob = 0.90, prob_outer = 0.95) +
  ggtitle(&quot;Trends Model&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-10-4.png' %}" width="1440" /></p>
<pre class="r"><code>p1 = pp_check(scenario_completed.model.v, type = &quot;bars_grouped&quot;, group = &quot;has_ax&quot;) + ggtitle(&quot;v:has_ax&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars_grouped&#39; by default.</code></pre>
<pre class="r"><code>p2 = pp_check(scenario_completed.model.v, type = &quot;bars_grouped&quot;, group = &quot;has_dx&quot;) + ggtitle(&quot;v:has_dx&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars_grouped&#39; by default.</code></pre>
<pre class="r"><code>p3 = pp_check(scenario_completed.model.v, type = &quot;bars_grouped&quot;, group = &quot;noise_level_f&quot;) + ggtitle(&quot;v:noise_level&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars_grouped&#39; by default.</code></pre>
<pre class="r"><code>grid.arrange(p1, p2, p3, nrow = 3)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-10-5.png' %}" width="1440" /></p>
<pre class="r"><code>mcmc_intervals(as.matrix(scenario_completed.model.v), regex_pars = &quot;b_&quot;, prob = 0.90, prob_outer = 0.95) +
  ggtitle(&quot;Values Model&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-10-6.png' %}" width="1440" /></p>
<pre class="r"><code># Plot predictions vs original
preds_p_df = scenario_completed.model.null %&gt;% predict() %&gt;% as_tibble()
p1 =
  preds_p_df %&gt;%
    add_column(scenario_completed = scenario_completed.model.null$data$scenario_completed) %&gt;%
    ggplot(aes(x = Estimate, y = scenario_completed, color = scenario_completed)) +
      geom_point() +
      labs(x = &quot;Predicted values&quot;) +
      ylim(0, 1) +
      xlim(0, 1) +
      ggtitle(&quot;Null Model&quot;)

preds_p_df = plot_df %&gt;%
  add_predicted_draws(scenario_completed.model.null) %&gt;%
  group_by(scenario_completed, .prediction) %&gt;%
  count()
p2 =
  preds_p_df %&gt;%
    ggplot(aes(x = scenario_completed, y = .prediction, fill = n / sum(preds_p_df$n))) +
      geom_tile() +
      geom_text(aes(label = sprintf(&quot;%1.2f&quot;, n / sum(preds_p_df$n))), size = 10, vjust = 1) +
      scale_fill_gradient(limits = c(0, 1))

preds_p_df = scenario_completed.model.t %&gt;% predict() %&gt;% as_tibble()
p3 =
  preds_p_df %&gt;%
    add_column(scenario_completed = scenario_completed.model.t$data$scenario_completed) %&gt;%
    ggplot(aes(x = Estimate, y = scenario_completed, color = scenario_completed)) +
      geom_point() +
      labs(x = &quot;Predicted values&quot;) +
      ylim(0, 1) +
      xlim(0, 1) +
      ggtitle(&quot;Trends Model&quot;)

preds_p_df = plot_df %&gt;%
  add_predicted_draws(scenario_completed.model.t) %&gt;%
  group_by(scenario_completed, .prediction) %&gt;%
  count()
p4 =
  preds_p_df %&gt;%
    ggplot(aes(x = scenario_completed, y = .prediction, fill = n / sum(preds_p_df$n))) +
      geom_tile() +
      geom_text(aes(label = sprintf(&quot;%1.2f&quot;, n / sum(preds_p_df$n))), size = 10, vjust = 1) +
      scale_fill_gradient(limits = c(0, 1))

# Plot predictions vs original
preds_p_df = scenario_completed.model.v %&gt;% predict() %&gt;% as_tibble()
p5 =
  preds_p_df %&gt;%
    add_column(scenario_completed = scenario_completed.model.v$data$scenario_completed) %&gt;%
    ggplot(aes(x = Estimate, y = scenario_completed, color = scenario_completed)) +
      geom_point() +
      labs(x = &quot;Predicted values&quot;) +
      ylim(0, 1) +
      xlim(0, 1) +
      ggtitle(&quot;Values Model&quot;)

preds_p_df = plot_df %&gt;%
  add_predicted_draws(scenario_completed.model.v) %&gt;%
  group_by(scenario_completed, .prediction) %&gt;%
  count()
p6 =
  preds_p_df %&gt;%
    ggplot(aes(x = scenario_completed, y = .prediction, fill = n / sum(preds_p_df$n))) +
      geom_tile() +
      geom_text(aes(label = sprintf(&quot;%1.2f&quot;, n / sum(preds_p_df$n))), size = 10, vjust = 1) +
      scale_fill_gradient(limits = c(0, 1))

grid.arrange(p1, p3, p5, p2, p4, p6, nrow = 2, ncol = 3)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-10-7.png' %}" width="1440" /></p>
<p>We are able to recreate the data, and there is a mild improvement in the recreation / prediction as a result of our model. So we are fine, I think.</p>
</div>
<div id="inference" class="section level2">
<h2>Inference</h2>
<pre class="r"><code># Compute the bayes factor for each parameter value. Note: we compare
# here against the ROPE, and not the degenerate null hypothesis of 0
# Bayes-Factor values would be more significant in the latter case
# According to the Raftery, 1995 rules
# bf = 1 - 3: Weak
# bf = 3 - 20: Positive
# bf = 20 - 150: Strong
# bf &gt; 150: Very strong
print(bayesfactor_rope(scenario_completed.model.null, null = c(-0.055, 0.055)), n = 30)
print(bayesfactor_rope(scenario_completed.model.t, null = c(-0.055, 0.055)), n = 30)
print(bayesfactor_rope(scenario_completed.model.v, null = c(-0.055, 0.055)), n = 30)</code></pre>
<pre class="r"><code># Compare the models. Note: we cannot do this because we don&#39;t have enough samples.
# The default is 4000; apparently these functions are only meaningful with 40000
# comparison = bayesfactor_models(scenario_completed.model.t, scenario_completed.model.v,
#                                 denominator = scenario_completed.model.null)
# print(comparison)
# print(bayesfactor_inclusion(comparison))

# # If we&#39;re using the brms functions, then use the following (make sure to update!)
# common_hyp_to_test = c(&quot;-2 * has_ax1 = 0&quot;, &quot;-2 * has_dx1 = 0&quot;)
# noise_levels_hyp_to_test = c(
#   &quot;Intercept-noise_level_f1 = 0&quot;, &quot;Intercept-noise_level_f2 = 0&quot;, &quot;Intercept-noise_level_f1-noise_level_f2 = 0&quot;,
#   &quot;Intercept-has_ax1:noise_level_f1 = 0&quot;, &quot;Intercept-has_dx1:noise_level_f1 = 0&quot;,
#   &quot;Intercept-has_ax1:noise_level_f2 = 0&quot;, &quot;Intercept-has_dx1:noise_level_f2 = 0&quot;,
#   &quot;Intercept-has_ax1:noise_level_f1-has_ax1:noise_level_f2 = 0&quot;,
#   &quot;Intercept-has_dx1:noise_level_f1-has_dx1:noise_level_f2 = 0&quot;
# )
# hyp_results = test_hypotheses(hypotheses_list = c(common_hyp_to_test, noise_levels_hyp_to_test), model = scenario_completed.model.v)
# print(hyp_results$hypothesis)
# plot(hyp_results, ask=F)

options(digits = 3)
# Check the significance of each hypothesis
h_df = as_tibble(insight::get_parameters(scenario_completed.model.t)) %&gt;%
  transmute(
    Intercept = b_Intercept,
    has_ax_test = -2 * b_has_ax1,
    has_dx_test = -2 * b_has_dx1,
    noise_levelL_test = b_noise_level.L,
    noise_levelQ_test = b_noise_level.Q
  )

hyp_results = test_hypotheses(h_df, rope_values = c(-0.055, 0.055))
print(hyp_results %&gt;% select(Parameter, pd, HDI_low, HDI_high, ROPE_Percentage, ROPE_Equivalence) %&gt;% as.matrix())</code></pre>
<pre><code>##      Parameter           pd      HDI_low  HDI_high ROPE_Percentage
## [1,] &quot;Intercept&quot;         &quot;0.999&quot; &quot;-0.486&quot; &quot;5.36&quot;   &quot;0.00075&quot;
## [2,] &quot;has_ax_test&quot;       &quot;1.000&quot; &quot; 0.461&quot; &quot;4.34&quot;   &quot;0.00000&quot;
## [3,] &quot;has_dx_test&quot;       &quot;0.784&quot; &quot;-3.463&quot; &quot;2.12&quot;   &quot;0.04900&quot;
## [4,] &quot;noise_levelL_test&quot; &quot;0.708&quot; &quot;-2.597&quot; &quot;2.79&quot;   &quot;0.06200&quot;
## [5,] &quot;noise_levelQ_test&quot; &quot;0.996&quot; &quot;-1.005&quot; &quot;3.09&quot;   &quot;0.00150&quot;
##      ROPE_Equivalence
## [1,] &quot;Rejected&quot;
## [2,] &quot;Rejected&quot;
## [3,] &quot;Undecided&quot;
## [4,] &quot;Undecided&quot;
## [5,] &quot;Rejected&quot;</code></pre>
<pre class="r"><code>h_df = as_tibble(insight::get_parameters(scenario_completed.model.v)) %&gt;%
  transmute(
    Intercept = b_Intercept,
    has_ax_test = -2 * b_has_ax1,
    has_dx_test = -2 * b_has_dx1,
    noise_level1_test = b_noise_level_f1,
    noise_level2_test = b_noise_level_f2,
    noise_level3_test = - b_noise_level_f1 - b_noise_level_f2
  )

hyp_results = test_hypotheses(h_df, rope_values = c(-0.055, 0.055))
print(hyp_results %&gt;% select(Parameter, pd, HDI_low, HDI_high, ROPE_Percentage, ROPE_Equivalence) %&gt;% as.matrix())</code></pre>
<pre><code>##      Parameter           pd      HDI_low  HDI_high ROPE_Percentage
## [1,] &quot;Intercept&quot;         &quot;0.999&quot; &quot;-0.490&quot; &quot;5.537&quot;  &quot;0.00075&quot;
## [2,] &quot;has_ax_test&quot;       &quot;1.000&quot; &quot; 0.114&quot; &quot;4.494&quot;  &quot;0.00000&quot;
## [3,] &quot;has_dx_test&quot;       &quot;0.782&quot; &quot;-3.782&quot; &quot;1.624&quot;  &quot;0.05150&quot;
## [4,] &quot;noise_level1_test&quot; &quot;0.948&quot; &quot;-0.909&quot; &quot;2.554&quot;  &quot;0.02700&quot;
## [5,] &quot;noise_level2_test&quot; &quot;0.994&quot; &quot;-2.853&quot; &quot;0.437&quot;  &quot;0.00450&quot;
## [6,] &quot;noise_level3_test&quot; &quot;0.704&quot; &quot;-1.947&quot; &quot;3.051&quot;  &quot;0.08350&quot;
##      ROPE_Equivalence
## [1,] &quot;Rejected&quot;
## [2,] &quot;Rejected&quot;
## [3,] &quot;Undecided&quot;
## [4,] &quot;Undecided&quot;
## [5,] &quot;Rejected&quot;
## [6,] &quot;Undecided&quot;</code></pre>
<pre class="r"><code>options(digits = 7)</code></pre>
<p><strong>Results</strong>:</p>
<ol style="list-style-type: decimal">
<li>AX suggestions
<ul>
<li>Positive effect pd = 100%</li>
<li>Median = 2.24, 89% CI [1.30, 3.18]</li>
<li>Medium, Std.Median = 0.58</li>
<li>0.0% in ROPE</li>
</ul></li>
<li>Noise Level Quadratic
<ul>
<li>Positive (convex-shape) pd = 99.6%</li>
<li>Median = 1.26, 89% CI [0.49, 2.12]</li>
<li>Medium, Std.Median = 0.51</li>
<li>0.15% in ROPE</li>
</ul></li>
</ol>
<p>(The ROPE is defined as [-0.055, 0.055]. It is the range is suggested in the literature for logistic models and it corresponds to a probability range of 0.11. Therefore, a probability change of less than 0.055 is considered no different from a probability change of 0)</p>
</div>
<div id="posterior-plots" class="section level2">
<h2>Posterior Plots</h2>
<pre class="r"><code># A visualization data frame
gg_df = plot_df %&gt;%
  mutate(scenario_completed = factor(scenario_completed)) %&gt;%
  mutate(scenario_completed = fct_recode(scenario_completed, incomplete=&quot;0&quot;, complete = &quot;1&quot;)) %&gt;%
  mutate(noise_level = fct_rev(noise_level)) %&gt;%
  count(has_ax, has_dx, noise_level, scenario_completed)

# Simulate predictions on new data
p_df =
  plot_df %&gt;%
  data_grid(scenario_completed,
            has_ax,
            has_dx,
            noise_level_f,
            gender,
            age_group,
            robot_experience,
            num_optimal,
            .model = scenario_completed.model.v)

fits_p_df = p_df %&gt;% add_fitted_draws(scenario_completed.model.v)
pars_p_df = scenario_completed.model.t %&gt;% extract_draws(newdata = p_df %&gt;% rename(noise_level = noise_level_f))
# We don&#39;t want predicted draws because that&#39;s a raw count of ones and zeros; not the
# probability distribution
# preds_p_df = p_df %&gt;% add_predicted_draws(scenario_completed.model.v)

# Plot the posterior distributions
# rope_value = inv_logit_scaled(fixef(scenario_completed.model.null)[&quot;Intercept&quot;,&quot;Estimate&quot;])
rope_value = gg_df %&gt;%
  filter(scenario_completed == &#39;complete&#39;, has_ax == F) %&gt;%
  summarise(.value = median(n) / 20) %&gt;%
  pull(.value)
gg_df %&gt;%
  filter(scenario_completed == &#39;complete&#39;) %&gt;%
  ggplot(aes(x = has_ax, y = n / 20)) +
    geom_rect(ymin = rope_value-0.055, ymax = rope_value+0.055,
              xmin = -Inf, xmax = Inf,
              color = &quot;transparent&quot;, fill = &quot;white&quot;) +
    annotate(&quot;text&quot;, x = 0.5, y = rope_value, label = &quot;ROPE&quot;, angle = 90) +
    stat_halfeye(aes(y = .value), data = fits_p_df, position = position_nudge(x = 0.1)) +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep = &quot;:&quot;)), size = 2) +
    stat_summary(fun.y = median, geom = &quot;point&quot;, shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(y = &quot;scenario_completed&quot;, color = &quot;AX:DX:Noise&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-13-1.png' %}" width="672" /></p>
<pre class="r"><code>rope_value = gg_df %&gt;%
  filter(scenario_completed == &#39;complete&#39;, has_dx == F) %&gt;%
  summarise(.value = median(n) / 20) %&gt;%
  pull(.value)
gg_df %&gt;%
  filter(scenario_completed == &#39;complete&#39;) %&gt;%
  ggplot(aes(x = has_dx, y = n / 20)) +
    geom_rect(ymin = rope_value-0.055, ymax = rope_value+0.055,
              xmin = -Inf, xmax = Inf,
              color = &quot;transparent&quot;, fill = &quot;white&quot;) +
    annotate(&quot;text&quot;, x = 0.5, y = rope_value, label = &quot;ROPE&quot;, angle = 90) +
    stat_halfeye(aes(y = .value), data = fits_p_df, position = position_nudge(x = 0.1)) +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep = &quot;:&quot;)), size = 2) +
    stat_summary(fun.y = median, geom = &quot;point&quot;, shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(y = &quot;scenario_completed&quot;, color = &quot;AX:DX:Noise&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-13-2.png' %}" width="672" /></p>
<pre class="r"><code>rope_value = gg_df %&gt;%
  filter(scenario_completed == &#39;complete&#39;, noise_level == &quot;0.0&quot;) %&gt;%
  summarise(.value = median(n) / 20) %&gt;%
  pull(.value)
gg_df %&gt;%
  filter(scenario_completed == &#39;complete&#39;) %&gt;%
  mutate(noise_level = fct_rev(noise_level)) %&gt;%
  ggplot(aes(x = noise_level, y = n / 20)) +
    geom_rect(ymin = rope_value-0.055, ymax = rope_value+0.055,
              xmin = -Inf, xmax = Inf,
              color = &quot;transparent&quot;, fill = &quot;white&quot;) +
    annotate(&quot;text&quot;, x = 0.5, y = rope_value, label = &quot;ROPE&quot;, angle = 90) +
    stat_halfeye(aes(y = .value, x = noise_level_f), data = fits_p_df, position = position_nudge(x = 0.1)) +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep = &quot;:&quot;)), size = 2) +
    stat_summary(fun.y = median, geom = &quot;point&quot;, shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(y = &quot;scenario_completed&quot;, color = &quot;AX:DX:Noise&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-13-3.png' %}" width="672" /></p>
<pre class="r"><code># Create a plot to show the trend
pars_p_df_X = pars_p_df$dpars$mu$fe$X[,c(&quot;Intercept&quot;, &quot;noise_level.Q&quot;)]
pars_p_df_b = pars_p_df$dpars$mu$fe$b[,c(&quot;b_Intercept&quot;, &quot;b_noise_level.Q&quot;)]
pars_p_df_y = pars_p_df_X %*% t(pars_p_df_b)[,as.integer(seq(from = 1, to = 4000, length.out = 100))]
pars_p_df_y = inv_logit_scaled(pars_p_df_y)
p_df =
  p_df %&gt;%
  bind_cols(as_tibble(pars_p_df_y)) %&gt;%
  gather(key = &quot;.sample&quot;, value = &quot;.value&quot;, V1:V100) %&gt;%
  rename(noise_level = noise_level_f)</code></pre>
<pre><code>## Warning: `as_tibble.matrix()` requires a matrix with column names or a `.name_repair` argument. Using compatibility `.name_repair`.
## This warning is displayed once per session.</code></pre>
<pre class="r"><code>rope_value = gg_df %&gt;%
  filter(scenario_completed == &#39;complete&#39;, noise_level == &quot;0.0&quot;) %&gt;%
  summarise(.value = median(n) / 20) %&gt;%
  pull(.value)
gg_df %&gt;%
  filter(scenario_completed == &#39;complete&#39;) %&gt;%
  mutate(noise_level = fct_rev(noise_level)) %&gt;%
  ggplot(aes(x = noise_level, y = n / 20)) +
    geom_rect(ymin = rope_value-0.055, ymax = rope_value+0.055,
              xmin = -Inf, xmax = Inf,
              color = &quot;transparent&quot;, fill = &quot;white&quot;) +
    annotate(&quot;text&quot;, x = 0.5, y = rope_value, label = &quot;ROPE&quot;, angle = 90) +
    stat_halfeye(aes(y = .value, x = noise_level_f), data = fits_p_df, position = position_nudge(x = 0.1)) +
    geom_line(aes(y = .value, group = .sample, alpha = 0.3), data = p_df) +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep = &quot;:&quot;)), size = 2) +
    stat_summary(fun.y = median, geom = &quot;point&quot;, shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(y = &quot;scenario_completed&quot;, color = &quot;AX:DX:Noise&quot;) +
    guides(alpha = F)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-13-4.png' %}" width="672" /></p>
<pre class="r"><code># Remove the giant fits data frames
rm(fits_p_df, pars_p_df, pars_p_df_b, pars_p_df_X, pars_p_df_y, p_df, gg_df)</code></pre>
<pre class="r"><code>eff = fixef(scenario_completed.model.v)

gg_df = plot_df %&gt;%
  mutate(scenario_completed = factor(scenario_completed)) %&gt;%
  mutate(scenario_completed = fct_recode(scenario_completed, Unresolved=&quot;0&quot;, Resolved = &quot;1&quot;)) %&gt;%
  count(has_ax, has_dx, noise_level, scenario_completed) %&gt;%
  mutate(has_ax = fct_recode(has_ax, &quot;No AX&quot;=&quot;FALSE&quot;, &quot;AX&quot;=&quot;TRUE&quot;),
         has_dx = fct_recode(has_dx, &quot;No DX&quot;=&quot;FALSE&quot;, &quot;DX&quot;=&quot;TRUE&quot;),
         noise_level = fct_recode(noise_level, &quot;Acc: 100%&quot;=&quot;0.0&quot;, &quot;Acc: 90%&quot;=&quot;1.0&quot;, &quot;Acc: 80%&quot;=&quot;2.0&quot;),
         estimate = -1) %&gt;%
  mutate(estimate = if_else(has_ax == &quot;No AX&quot;, inv_logit_scaled(eff[&quot;Intercept&quot;, &quot;Estimate&quot;] + eff[&quot;has_ax1&quot;,&quot;Estimate&quot;]), estimate)) %&gt;%
  mutate(estimate = if_else(has_ax == &quot;AX&quot;, inv_logit_scaled(eff[&quot;Intercept&quot;, &quot;Estimate&quot;] - eff[&quot;has_ax1&quot;, &quot;Estimate&quot;]), estimate))

gg_df %&gt;%
  filter(scenario_completed == &#39;Resolved&#39;) %&gt;%
  ggplot(aes(has_ax, n/20, group = has_ax, colour = has_ax)) +
    geom_count(aes(colour = NULL)) +
    geom_boxplot(aes(y = estimate)) +
    geom_signif(y_position = 1.05, xmin = 1, xmax = 2, annotation = &quot;**&quot;, textsize = 8, color = &quot;black&quot;) +
    scale_y_continuous(limits = c(0.5, 1.1)) +
    labs(y = &quot;Fraction of participants that resolved fault&quot;, x = NULL) +
    scale_colour_manual(values = c(&quot;#d62728&quot;, &quot;#2ca02c&quot;, &quot;#ff7f0e&quot;)) +
    theme_economist_white(base_size = 13) +
    theme(plot.background = element_rect(fill = &quot;white&quot;),
          axis.title.y = element_blank()) +
    legend_none()</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-14-1.png' %}" width="672" /></p>
<pre class="r"><code>gg_df = plot_df %&gt;%
  mutate(scenario_completed = factor(scenario_completed)) %&gt;%
  mutate(scenario_completed = fct_recode(scenario_completed, Unresolved=&quot;0&quot;, Resolved = &quot;1&quot;)) %&gt;%
  count(has_ax, has_dx, noise_level, scenario_completed) %&gt;%
  mutate(has_ax = fct_recode(has_ax, &quot;No AX&quot;=&quot;FALSE&quot;, &quot;AX&quot;=&quot;TRUE&quot;),
         has_dx = fct_recode(has_dx, &quot;No DX&quot;=&quot;FALSE&quot;, &quot;DX&quot;=&quot;TRUE&quot;),
         noise_level = fct_recode(noise_level, &quot;Acc: 100%&quot;=&quot;0.0&quot;, &quot;Acc: 90%&quot;=&quot;1.0&quot;, &quot;Acc: 80%&quot;=&quot;2.0&quot;),
         estimate = -1) %&gt;%
  mutate(estimate = if_else(noise_level == &quot;Acc: 100%&quot;, inv_logit_scaled(eff[&quot;noise_level_f1&quot;,&quot;Estimate&quot;] + eff[&quot;Intercept&quot;, &quot;Estimate&quot;]), estimate)) %&gt;%
  mutate(estimate = if_else(noise_level == &quot;Acc: 90%&quot;, inv_logit_scaled(eff[&quot;noise_level_f2&quot;,&quot;Estimate&quot;] + eff[&quot;Intercept&quot;, &quot;Estimate&quot;]), estimate)) %&gt;%
  mutate(estimate = if_else(noise_level == &quot;Acc: 80%&quot;, inv_logit_scaled(eff[&quot;Intercept&quot;, &quot;Estimate&quot;] - eff[&quot;noise_level_f1&quot;,&quot;Estimate&quot;] - eff[&quot;noise_level_f2&quot;,&quot;Estimate&quot;]), estimate)) %&gt;%
  mutate(noise_level = fct_rev(noise_level))

gg_df %&gt;%
  filter(scenario_completed == &#39;Resolved&#39;) %&gt;%
  ggplot(aes(noise_level, n/20, group = noise_level, colour = noise_level)) +
    geom_hline(yintercept = inv_logit_scaled(eff[&quot;Intercept&quot;, &quot;Estimate&quot;]),
               size = .7, linetype = &quot;dashed&quot;, color = &quot;grey&quot;) +
    geom_count(aes(colour = NULL)) +
    geom_boxplot(aes(y = estimate)) +
    annotate(&quot;segment&quot;,
             x = 3,
             y = inv_logit_scaled(eff[&quot;noise_level_f1&quot;,&quot;Estimate&quot;] + eff[&quot;Intercept&quot;, &quot;Estimate&quot;]),
             xend = 2,
             yend = inv_logit_scaled(eff[&quot;noise_level_f2&quot;,&quot;Estimate&quot;] + eff[&quot;Intercept&quot;, &quot;Estimate&quot;]),
             colour = &quot;black&quot;) +
    annotate(&quot;segment&quot;,
             x = 1,
             y = inv_logit_scaled(eff[&quot;Intercept&quot;, &quot;Estimate&quot;] - eff[&quot;noise_level_f1&quot;,&quot;Estimate&quot;] - eff[&quot;noise_level_f2&quot;,&quot;Estimate&quot;]),
             xend = 2,
             yend = inv_logit_scaled(eff[&quot;noise_level_f2&quot;,&quot;Estimate&quot;] + eff[&quot;Intercept&quot;, &quot;Estimate&quot;]),
             color = &quot;black&quot;) +
    annotate(&quot;text&quot;,
             x = 3.5,
             y = inv_logit_scaled(eff[&quot;Intercept&quot;, &quot;Estimate&quot;] - 0.85),
             label = &quot;Mean of all levels&quot;,
             angle = 90,
             group = NA,
             colour = &quot;black&quot;) +
    scale_y_continuous(limits = c(0.5, 1.1), breaks = c(0.5, 1.0)) +
    labs(y = &quot;Fraction of participants that resolved fault&quot;, x = NULL) +
    scale_colour_manual(values = c(&quot;#d62728&quot;, &quot;#2ca02c&quot;, &quot;#ff7f0e&quot;)) +
    scale_size_area(max_size = 1) +
    theme_economist_white(base_size = 13) +
    theme(plot.background = element_rect(fill = &quot;white&quot;),
          axis.title.y = element_blank()) +
    legend_none()</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-14-2.png' %}" width="672" /></p>
</div>
</div>
<div id="rax-rate-of-optimal-action-selection" class="section level1">
<h1>RAX: Rate of Optimal Action Selection</h1>
<p><strong>Did the user take an optimal action given the state that they were in?</strong></p>
<p>This is also a measure of Reliance on Suggestions. In the code, the variable might be referred to as <code>optimal_ax</code>, <code>correct_ax</code>, etc. depending on the version of the codebase</p>
<pre class="r"><code>plot_df = actions %&gt;%
  select(X1, id, user_id, study_condition, start_condition, num_optimal,
         state_idx, state_idx_rescaled,
         age_group, gender, robot_experience,
         noise_level, noise_level_f, has_noise, has_dx, has_ax,
         scenario_completed, num_actions, optimal_ax)

text_short(report(plot_df))</code></pre>
<pre><code>## Warning in report.numeric(model[[col]], median = median, centrality =
## centrality, : Variable `model[[col]]` contains only two different values.
## Consider converting it to a factor.

## Warning in report.numeric(model[[col]], median = median, centrality =
## centrality, : Variable `model[[col]]` contains only two different values.
## Consider converting it to a factor.</code></pre>
<pre><code>## The data contains 2126 observations of the following variables:
##   - X1: Mean = 1062.50, SD = 613.87, range = [0, 2125], 0 missing
##   - id: Mean = 1063.50, SD = 613.87, range = [1, 2126], 0 missing
##   - user_id: Mean = 102.65, SD = 57.16, range = [1, 200], 0 missing
##   - study_condition: 10 levels: DX_100, n = 247; AX_100, n = 152; DXAX_100, n = 193; DX_90, n = 254; AX_90, n = 217; DXAX_90, n = 183; DX_80, n = 256; AX_80, n = 204; DXAX_80, n = 189 and BASELINE, n = 231
##   - start_condition: 4 entries: kc.dt.occluding.above_mug.default.empty.dt, n = 672; kc.kc.occluding.default.default.empty.dt, n = 595; dt.kc.default.default.default.empty.kc, n = 498 and 1 other
##   - num_optimal: Mean = 4.78, SD = 1.55, range = [3, 7], 0 missing
##   - state_idx: Mean = 86.63, SD = 59.16, range = [1, 171], 0 missing
##   - state_idx_rescaled: Mean = 0.50, SD = 0.35, range = [0, 1], 0 missing
##   - age_group: 7 levels: 2, n = 226; 3, n = 493; 4, n = 497; 5, n = 283; 6, n = 222; 7, n = 170 and 8, n = 235
##   - gender: 2 levels: F, n = 844 and M, n = 1282
##   - robot_experience: 5 levels: 0, n = 1318; 1, n = 346; 2, n = 161; 3, n = 169 and 4, n = 132
##   - noise_level: 3 levels: 0.0, n = 823; 1.0, n = 654 and 2.0, n = 649
##   - noise_level_f: 3 levels: 0.0, n = 823; 1.0, n = 654 and 2.0, n = 649
##   - has_noise: 2 levels: FALSE, n = 823 and TRUE, n = 1303
##   - has_dx: 2 levels: FALSE, n = 804 and TRUE, n = 1322
##   - has_ax: 2 levels: FALSE, n = 988 and TRUE, n = 1138
##   - scenario_completed: Mean = 0.64, SD = 0.48, range = [0, 1], 0 missing
##   - num_actions: Mean = 13.90, SD = 5.85, range = [3, 20], 0 missing
##   - optimal_ax: Mean = 0.39, SD = 0.49, range = [0, 1], 0 missing</code></pre>
<div id="data-1" class="section level2">
<h2>Data</h2>
<pre class="r"><code>gg_df =
  plot_df %&gt;%
    mutate(noise_level = fct_rev(noise_level)) %&gt;%
    mutate(optimal_ax = factor(optimal_ax)) %&gt;%
    mutate(user_id = factor(user_id)) %&gt;%
    mutate(optimal_ax = fct_recode(optimal_ax, incorrect=&quot;0&quot;, correct = &quot;1&quot;)) %&gt;%
    mutate(.fill_column = paste(noise_level, !(study_condition == &#39;BASELINE&#39;), sep = &quot;:&quot;)) %&gt;%
    count(user_id, study_condition, .fill_column, optimal_ax, num_actions, has_ax, has_dx, noise_level,
          name = &quot;num_optimal_ax&quot;)

# Plot by the study condition
gg_df %&gt;%
  ggplot(aes(user_id, num_optimal_ax / num_actions, fill=optimal_ax)) +
    geom_bar(stat=&quot;identity&quot;) +
    geom_hline(yintercept = 0.5) +
    facet_wrap(vars(study_condition), nrow = 1, scales = &quot;free_y&quot;) +
    labs(y = &quot;Fraction correct actions&quot;) +
    coord_flip() +
    scale_fill_economist()</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-17-1.png' %}" width="1440" /></p>
<pre class="r"><code>gg_df %&gt;%
  filter(optimal_ax == &quot;correct&quot;) %&gt;%
  ggplot(aes(study_condition, num_optimal_ax / num_actions, fill=.fill_column)) +
    geom_violin() +
    geom_boxplot(width = 0.1) +
    labs(y = &quot;Fraction correct actions&quot;, fill = &quot;noise:has_suggestions&quot;) +
    scale_fill_economist()</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-17-2.png' %}" width="1440" /></p>
<pre class="r"><code># Visualize the data facetted by the variables that we care about
p1 = gg_df %&gt;%
  filter(optimal_ax == &quot;correct&quot;) %&gt;%
  ggplot(aes(has_ax, num_optimal_ax / num_actions, fill=.fill_column)) +
    geom_boxplot() +
    geom_count() +
    facet_grid(rows = vars(noise_level), cols = vars(has_dx), labeller = label_both) +
    labs(y = &quot;Fraction correct actions&quot;) +
    scale_fill_economist() +
    theme(legend.position = &quot;left&quot;)

p2 = gg_df %&gt;%
  filter(optimal_ax == &quot;correct&quot;) %&gt;%
  ggplot(aes(has_dx, num_optimal_ax / num_actions, fill=.fill_column)) +
    geom_boxplot() +
    geom_count() +
    facet_grid(rows = vars(noise_level), cols = vars(has_ax), labeller = label_both) +
    labs(y = &quot;Fraction correct actions&quot;) +
    scale_fill_economist() +
    legend_none()

p3 = gg_df %&gt;%
  filter(optimal_ax == &quot;correct&quot;) %&gt;%
  mutate(noise_level = fct_rev(noise_level)) %&gt;%
  mutate(has_ax = factor(has_ax)) %&gt;%
  mutate(has_ax = fct_rev(has_ax)) %&gt;%
  ggplot(aes(noise_level, num_optimal_ax / num_actions, fill=.fill_column)) +
    geom_boxplot() +
    geom_count() +
    facet_grid(rows = vars(has_ax), cols = vars(has_dx), labeller = label_both) +
    labs(y = &quot;Fraction correct actions&quot;) +
    scale_fill_economist() +
    legend_none()

grid.arrange(p1, p2, p3, ncol = 3)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-17-3.png' %}" width="1440" /></p>
<pre class="r"><code># Visualize also the variation in each of the three levels that we care about
p1 = gg_df %&gt;%
  filter(optimal_ax == &#39;correct&#39;) %&gt;%
  ggplot(aes(has_ax, num_optimal_ax/num_actions)) +
    geom_violin() +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep=&quot;:&quot;), size = 2)) +
    stat_summary(fun.y = median, geom=&quot;point&quot;, shape=23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(color = &quot;AX:DX:Noise&quot;) +
    theme(legend.position = &quot;left&quot;) +
    guides(size = F)

p2 = gg_df %&gt;%
  filter(optimal_ax == &#39;correct&#39;) %&gt;%
  ggplot(aes(has_dx, num_optimal_ax/num_actions)) +
    geom_violin() +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep=&quot;:&quot;), size = 2)) +
    stat_summary(fun.y = median, geom=&quot;point&quot;, shape=23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    legend_none()

p3 = gg_df %&gt;%
  filter(optimal_ax == &#39;correct&#39;) %&gt;%
  ggplot(aes(noise_level, num_optimal_ax/num_actions)) +
    geom_violin() +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep=&quot;:&quot;), size = 2)) +
    stat_summary(fun.y = median, geom=&quot;point&quot;, shape=23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    scale_x_discrete(limits = rev(levels(gg_df$noise_level))) +
    legend_none()

grid.arrange(p1, p2, p3, ncol = 3)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-17-4.png' %}" width="1440" /></p>
</div>
<div id="model-1" class="section level2">
<h2>Model</h2>
<p>Based on the data, we assume the following structural model:</p>
<p><span class="math display">\[rax_{ij} = Bernoulli(p_{ij})\]</span> <span class="math display">\[\begin{aligned}
logit^{-1}(p_{ij}) &amp;= \beta_0 + \beta_{0i} + \beta_{ax}\text{ax}_i + \beta_{dx}\text{dx}_i + \beta_{noise}\text{noise}_i +\\
&amp;\beta_{ax:noise}\text{ax}_i\text{noise}_i + \beta_{dx:noise}\text{dx}_i\text{noise}_i + \\
&amp;\beta_{no}\text{no}_i + \mathbf{\beta_{demo}X_{demo,i}} + \beta_{state} \text{state}_{ij}
\end{aligned}\]</span> <span class="math display">\[\begin{aligned}
\beta_{.} &amp;\sim Normal(0, 10) \\
\beta_{0i} &amp;\sim Normal(0, \sigma_i) \\
\sigma_i &amp;\sim HalfStudent(3, 0, 10)
\end{aligned}\]</span></p>
<p>The prior for the <span class="math inline">\(\sigma_i\)</span> parameter is the default used in <code>brms</code>; I see no need to change it.</p>
<p>Model fitting:</p>
<pre class="r"><code># The null model
optimal_ax.model.null = brm(
  optimal_ax ~ 0 + Intercept + (1 | user_id),
  family = &quot;bernoulli&quot;,
  prior = c(set_prior(&quot;normal(0, 10)&quot;, class = &quot;b&quot;),
            set_prior(&quot;student_t(3, 0, 10)&quot;, class = &quot;sd&quot;)),
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
optimal_ax.model.null = add_criterion(optimal_ax.model.null, &quot;waic&quot;)
optimal_ax.model.null = add_criterion(optimal_ax.model.null, &quot;loo&quot;, reloo = T)
saveModel(optimal_ax.model.null)

# The trend model to see if there is a trend in the noise level variable
optimal_ax.model.t = brm(
  optimal_ax ~ 0 + Intercept + ((has_dx + has_ax) * noise_level) + (age_group + robot_experience + gender) + num_optimal + state_idx_rescaled + (1 | user_id),
  family = bernoulli,
  prior = c(set_prior(&quot;normal(0, 10)&quot;, class = &quot;b&quot;),
            set_prior(&quot;student_t(3, 0, 10)&quot;, class = &quot;sd&quot;)),
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
optimal_ax.model.t = add_criterion(optimal_ax.model.t, &quot;waic&quot;)
optimal_ax.model.t = add_criterion(optimal_ax.model.t, &quot;loo&quot;, reloo = T)
saveModel(optimal_ax.model.t)

# The values model, to see if specific values of the noise level variable are significant
optimal_ax.model.v = brm(
  optimal_ax ~ 0 + Intercept + ((has_dx + has_ax) * noise_level_f) + (age_group + robot_experience + gender) + num_optimal + state_idx_rescaled + (1 | user_id),
  family = bernoulli,
  prior = c(set_prior(&quot;normal(0, 10)&quot;, class = &quot;b&quot;),
            set_prior(&quot;student_t(3, 0, 10)&quot;, class = &quot;sd&quot;)),
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
optimal_ax.model.v = add_criterion(optimal_ax.model.v, &quot;waic&quot;)
optimal_ax.model.v = add_criterion(optimal_ax.model.v, &quot;loo&quot;, reloo = T)
saveModel(optimal_ax.model.v)</code></pre>
<pre class="r"><code># Load the models
optimal_ax.model.null = loadModel(&quot;optimal_ax.model.null&quot;)
optimal_ax.model.t = loadModel(&quot;optimal_ax.model.t&quot;)
optimal_ax.model.v = loadModel(&quot;optimal_ax.model.v&quot;)</code></pre>
<p>Model fitting results:</p>
<pre class="r"><code># print(tidy_stan(optimal_ax.model.null))
print(performance::r2(optimal_ax.model.null))</code></pre>
<pre><code>## # Bayesian R2 with Standard Error
##
##   Conditional R2: 0.284 [0.013]
##      Marginal R2: 0.000 [0.000]</code></pre>
<pre class="r"><code># Print the parameters, and some initial diagnostics
print(tidy_stan(optimal_ax.model.t, effects = &quot;fixed&quot;))</code></pre>
<pre><code>## Warning: &#39;tidy_stan&#39; is deprecated.
## Use &#39;parameters::model_parameters()&#39; instead.
## See help(&quot;Deprecated&quot;)</code></pre>
<pre><code>##
## Summary Statistics of Stan-Model
##
## # Fixed effects
##
##              Parameter Estimate Std.Error       HDI(89%)     pd  ESS Rhat MCSE
##            (Intercept)    -0.22      0.50 [-1.01,  0.55]  66.5% 1952 1.00 0.01
##                has_dx1    -0.10      0.16 [-0.36,  0.16]  72.0% 1806 1.00 0.00
##                has_ax1    -0.62      0.16 [-0.89, -0.38] 100.0% 1970 1.00 0.00
##          noise_level.L    -0.23      0.27 [-0.66,  0.20]  80.2% 2134 1.00 0.01
##          noise_level.Q     0.42      0.30 [-0.10,  0.87]  92.1% 1789 1.00 0.01
##             age_group1     0.10      0.40 [-0.55,  0.70]  60.6% 2272 1.00 0.01
##             age_group2    -0.24      0.30 [-0.70,  0.27]  78.7% 2023 1.00 0.01
##             age_group3    -0.05      0.28 [-0.51,  0.39]  57.2% 2155 1.00 0.01
##             age_group4     0.77      0.34 [ 0.16,  1.28]  98.8% 2216 1.00 0.01
##             age_group5    -0.61      0.39 [-1.27,  0.01]  94.0% 2098 1.00 0.01
##             age_group6     0.58      0.46 [-0.22,  1.26]  89.5% 1889 1.00 0.01
##      robot_experience1     1.08      0.26 [ 0.67,  1.52] 100.0% 2144 1.00 0.01
##      robot_experience2     0.85      0.35 [ 0.27,  1.38]  99.1% 2324 1.00 0.01
##      robot_experience3    -0.25      0.50 [-1.05,  0.49]  70.1% 2380 1.00 0.01
##      robot_experience4    -0.84      0.49 [-1.66, -0.09]  96.2% 2464 1.00 0.01
##                gender1    -0.25      0.14 [-0.48, -0.03]  96.2% 2195 1.00 0.00
##            num_optimal     0.07      0.09 [-0.07,  0.22]  77.1% 1896 1.00 0.00
##     state_idx_rescaled    -2.25      0.20 [-2.58, -1.93] 100.0% 5969 1.00 0.00
##  has_dx1.noise_level.L    -0.31      0.27 [-0.73,  0.12]  88.1% 2102 1.00 0.01
##  has_dx1.noise_level.Q     0.37      0.31 [-0.08,  0.88]  89.1% 1643 1.00 0.01
##  has_ax1.noise_level.L     0.24      0.28 [-0.21,  0.68]  81.1% 2017 1.00 0.01
##  has_ax1.noise_level.Q     0.07      0.29 [-0.42,  0.52]  60.2% 1852 1.00 0.01</code></pre>
<pre class="r"><code>print(performance::r2(optimal_ax.model.t))</code></pre>
<pre><code>## # Bayesian R2 with Standard Error
##
##   Conditional R2: 0.364 [0.013]
##      Marginal R2: 0.217 [0.017]</code></pre>
<pre class="r"><code>print(tidy_stan(optimal_ax.model.v, effects = &quot;fixed&quot;))</code></pre>
<pre><code>## Warning: &#39;tidy_stan&#39; is deprecated.
## Use &#39;parameters::model_parameters()&#39; instead.
## See help(&quot;Deprecated&quot;)</code></pre>
<pre><code>##
## Summary Statistics of Stan-Model
##
## # Fixed effects
##
##               Parameter Estimate Std.Error       HDI(89%)     pd  ESS Rhat MCSE
##             (Intercept)    -0.21      0.51 [-1.03,  0.60]  66.0% 1687 1.00 0.01
##                 has_dx1    -0.09      0.16 [-0.35,  0.16]  71.9% 2013 1.00 0.00
##                 has_ax1    -0.63      0.17 [-0.90, -0.36] 100.0% 1755 1.00 0.00
##          noise_level_f1     0.34      0.21 [ 0.01,  0.68]  94.8% 1850 1.00 0.00
##          noise_level_f2    -0.35      0.24 [-0.68,  0.09]  91.6% 1778 1.00 0.01
##              age_group1     0.11      0.42 [-0.57,  0.78]  60.7% 1941 1.00 0.01
##              age_group2    -0.23      0.29 [-0.70,  0.25]  79.2% 2065 1.00 0.01
##              age_group3    -0.03      0.29 [-0.51,  0.40]  53.9% 2067 1.00 0.01
##              age_group4     0.76      0.34 [ 0.23,  1.28]  98.8% 2164 1.00 0.01
##              age_group5    -0.62      0.41 [-1.31, -0.01]  93.8% 1875 1.00 0.01
##              age_group6     0.58      0.45 [-0.11,  1.34]  90.5% 2255 1.00 0.01
##       robot_experience1     1.09      0.27 [ 0.65,  1.48] 100.0% 2083 1.00 0.01
##       robot_experience2     0.83      0.36 [ 0.26,  1.43]  99.2% 1801 1.00 0.01
##       robot_experience3    -0.26      0.49 [-1.04,  0.52]  71.0% 2326 1.00 0.01
##       robot_experience4    -0.86      0.50 [-1.69, -0.11]  96.0% 2430 1.00 0.01
##                 gender1    -0.25      0.15 [-0.50, -0.02]  95.2% 1978 1.00 0.00
##             num_optimal     0.07      0.09 [-0.09,  0.22]  76.5% 1676 1.00 0.00
##      state_idx_rescaled    -2.25      0.20 [-2.58, -1.94] 100.0% 5193 1.00 0.00
##  has_dx1:noise_level_f1     0.37      0.21 [ 0.02,  0.67]  96.2% 1748 1.00 0.00
##  has_dx1:noise_level_f2    -0.30      0.24 [-0.65,  0.11]  88.8% 1806 1.00 0.01
##  has_ax1:noise_level_f1    -0.14      0.22 [-0.48,  0.21]  75.0% 1842 1.00 0.01
##  has_ax1:noise_level_f2    -0.06      0.24 [-0.41,  0.36]  58.4% 1632 1.00 0.01</code></pre>
<pre class="r"><code>print(performance::r2(optimal_ax.model.v))</code></pre>
<pre><code>## # Bayesian R2 with Standard Error
##
##   Conditional R2: 0.364 [0.013]
##      Marginal R2: 0.217 [0.018]</code></pre>
</div>
<div id="diagnostic-1" class="section level2">
<h2>Diagnostic</h2>
<pre class="r"><code># Alternative diagnostics: diagnostic_posterior(scenario_completed.model.v)

# LOO cross-validation results
print(optimal_ax.model.t$criteria$loo$estimates)</code></pre>
<pre><code>##            Estimate        SE
## elpd_loo -1079.5735 24.227816
## p_loo      153.6992  4.594544
## looic     2159.1469 48.455632</code></pre>
<pre class="r"><code>print(optimal_ax.model.v$criteria$loo$estimates)</code></pre>
<pre><code>##            Estimate        SE
## elpd_loo -1079.6880 24.229272
## p_loo      153.8937  4.600758
## looic     2159.3760 48.458543</code></pre>
<pre class="r"><code>print(loo_compare(optimal_ax.model.null, optimal_ax.model.t, optimal_ax.model.v))</code></pre>
<pre><code>##                       elpd_diff se_diff
## optimal_ax.model.t      0.0       0.0
## optimal_ax.model.v     -0.1       0.4
## optimal_ax.model.null -83.2      12.8</code></pre>
<pre class="r"><code># Diagnostics of the sampling process
par(mfrow=c(1,2))
plot(optimal_ax.model.t$criteria$loo, main = &quot;Trend Model&quot;)
plot(optimal_ax.model.v$criteria$loo, main = &quot;Values Model&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-21-1.png' %}" width="1440" /></p>
<pre class="r"><code># Include posterior predictive checks of the sampling (are sampling according
# to the data that we actually have?)
p1 = pp_check(optimal_ax.model.t, type = &quot;bars_grouped&quot;, group = &quot;has_ax&quot;, stat = &quot;median&quot;) + ggtitle(&quot;t:has_ax&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars_grouped&#39; by default.</code></pre>
<pre><code>## Warning: The following arguments were unrecognized and ignored: stat</code></pre>
<pre class="r"><code>p2 = pp_check(optimal_ax.model.t, type = &quot;bars_grouped&quot;, group = &quot;has_dx&quot;) + ggtitle(&quot;t:has_dx&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars_grouped&#39; by default.</code></pre>
<pre class="r"><code>p3 = pp_check(optimal_ax.model.t, type = &quot;bars_grouped&quot;, group = &quot;noise_level&quot;) + ggtitle(&quot;t:noise_level&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars_grouped&#39; by default.</code></pre>
<pre class="r"><code>grid.arrange(p1, p2, p3, nrow = 3)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-21-2.png' %}" width="1440" /></p>
<pre class="r"><code>mcmc_intervals(as.matrix(optimal_ax.model.t), regex_pars = &quot;b_|^shape&quot;, prob = 0.90, prob_outer = 0.95) +
  ggtitle(&quot;Trends Model&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-21-3.png' %}" width="1440" /></p>
<pre class="r"><code>p1 = pp_check(optimal_ax.model.v, type = &quot;bars_grouped&quot;, group = &quot;has_ax&quot;) + ggtitle(&quot;v:has_ax&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars_grouped&#39; by default.</code></pre>
<pre class="r"><code>p2 = pp_check(optimal_ax.model.v, type = &quot;bars_grouped&quot;, group = &quot;has_dx&quot;) + ggtitle(&quot;v:has_dx&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars_grouped&#39; by default.</code></pre>
<pre class="r"><code>p3 = pp_check(optimal_ax.model.v, type = &quot;bars_grouped&quot;, group = &quot;noise_level_f&quot;) + ggtitle(&quot;v:noise_level&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars_grouped&#39; by default.</code></pre>
<pre class="r"><code>grid.arrange(p1, p2, p3, nrow = 3)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-21-4.png' %}" width="1440" /></p>
<pre class="r"><code>mcmc_intervals(as.matrix(optimal_ax.model.v), regex_pars = &quot;b_|^shape&quot;, prob = 0.90, prob_outer = 0.95) +
  ggtitle(&quot;Values Model&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-21-5.png' %}" width="1440" /></p>
<pre class="r"><code># Plot predictions vs original
preds_p_df = optimal_ax.model.null %&gt;% predict() %&gt;% as_tibble()
p1 =
  preds_p_df %&gt;%
    add_column(optimal_ax = optimal_ax.model.null$data$optimal_ax) %&gt;%
    ggplot(aes(x = Estimate, y = optimal_ax, color = optimal_ax)) +
      geom_point() +
      labs(x = &quot;Predicted values&quot;) +
      ylim(0, 1) +
      xlim(0, 1) +
      ggtitle(&quot;Null Model&quot;)

preds_p_df = plot_df %&gt;%
  add_predicted_draws(optimal_ax.model.null) %&gt;%
  group_by(optimal_ax, .prediction) %&gt;%
  count()
p2 =
  preds_p_df %&gt;%
    ggplot(aes(x = optimal_ax, y = .prediction, fill = n / sum(preds_p_df$n))) +
      geom_tile() +
      geom_text(aes(label = sprintf(&quot;%1.2f&quot;, n / sum(preds_p_df$n))), size = 10, vjust = 1) +
      scale_fill_gradient(limits = c(0, 1))

preds_p_df = optimal_ax.model.t %&gt;% predict() %&gt;% as_tibble()
p3 =
  preds_p_df %&gt;%
    add_column(optimal_ax = optimal_ax.model.t$data$optimal_ax) %&gt;%
    ggplot(aes(x = Estimate, y = optimal_ax, color = optimal_ax)) +
      geom_point() +
      labs(x = &quot;Predicted values&quot;) +
      ylim(0, 1) +
      xlim(0, 1) +
      ggtitle(&quot;Trends Model&quot;)

preds_p_df = plot_df %&gt;%
  add_predicted_draws(optimal_ax.model.t) %&gt;%
  group_by(optimal_ax, .prediction) %&gt;%
  count()
p4 =
  preds_p_df %&gt;%
    ggplot(aes(x = optimal_ax, y = .prediction, fill = n / sum(preds_p_df$n))) +
      geom_tile() +
      geom_text(aes(label = sprintf(&quot;%1.2f&quot;, n / sum(preds_p_df$n))), size = 10, vjust = 1) +
      scale_fill_gradient(limits = c(0, 1))

# Plot predictions vs original
preds_p_df = optimal_ax.model.v %&gt;% predict() %&gt;% as_tibble()
p5 =
  preds_p_df %&gt;%
    add_column(optimal_ax = optimal_ax.model.v$data$optimal_ax) %&gt;%
    ggplot(aes(x = Estimate, y = optimal_ax, color = optimal_ax)) +
      geom_point() +
      labs(x = &quot;Predicted values&quot;) +
      ylim(0, 1) +
      xlim(0, 1) +
      ggtitle(&quot;Values Model&quot;)

preds_p_df = plot_df %&gt;%
  add_predicted_draws(optimal_ax.model.v) %&gt;%
  group_by(optimal_ax, .prediction) %&gt;%
  count()
p6 =
  preds_p_df %&gt;%
    ggplot(aes(x = optimal_ax, y = .prediction, fill = n / sum(preds_p_df$n))) +
      geom_tile() +
      geom_text(aes(label = sprintf(&quot;%1.2f&quot;, n / sum(preds_p_df$n))), size = 10, vjust = 1) +
      scale_fill_gradient(limits = c(0, 1))

grid.arrange(p1, p3, p5, p2, p4, p6, nrow = 2, ncol = 3)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-21-6.png' %}" width="1440" /></p>
</div>
<div id="inference-1" class="section level2">
<h2>Inference</h2>
<pre class="r"><code># Compute the bayes factor for each parameter value. Note: we compare
# here against the ROPE, and not the degenerate null hypothesis of 0
# Bayes-Factor values would be more significant in the latter case
# According to the Raftery, 1995 rules
# bf = 1 - 3: Weak
# bf = 3 - 20: Positive
# bf = 20 - 150: Strong
# bf &gt; 150: Very strong
print(bayesfactor_rope(optimal_ax.model.null, null = c(-0.055, 0.055)), n = 30)
print(bayesfactor_rope(optimal_ax.model.t, null = c(-0.055, 0.055)), n = 30)
print(bayesfactor_rope(optimal_ax.model.v, null = c(-0.055, 0.055)), n = 30)</code></pre>
<pre class="r"><code>options(digits = 3)
# Check the significance of each hypothesis
h_df = as_tibble(insight::get_parameters(optimal_ax.model.t)) %&gt;%
  transmute(
    Intercept = b_Intercept,
    has_ax_test = -2 * b_has_ax1,
    has_dx_test = -2 * b_has_dx1,
    noise_levelL_test = b_noise_level.L,
    noise_levelQ_test = b_noise_level.Q
  )

hyp_results = test_hypotheses(h_df, rope_values = c(-0.055, 0.055))
print(hyp_results %&gt;% select(Parameter, pd, HDI_low, HDI_high, ROPE_Percentage, ROPE_Equivalence) %&gt;% as.matrix())</code></pre>
<pre><code>##      Parameter           pd      HDI_low  HDI_high ROPE_Percentage
## [1,] &quot;Intercept&quot;         &quot;0.664&quot; &quot;-1.848&quot; &quot;1.44&quot;   &quot;0.0785&quot;
## [2,] &quot;has_ax_test&quot;       &quot;1.000&quot; &quot; 0.109&quot; &quot;2.39&quot;   &quot;0.0000&quot;
## [3,] &quot;has_dx_test&quot;       &quot;0.720&quot; &quot;-1.017&quot; &quot;1.25&quot;   &quot;0.1195&quot;
## [4,] &quot;noise_levelL_test&quot; &quot;0.802&quot; &quot;-1.474&quot; &quot;0.70&quot;   &quot;0.1120&quot;
## [5,] &quot;noise_levelQ_test&quot; &quot;0.921&quot; &quot;-0.542&quot; &quot;1.59&quot;   &quot;0.0530&quot;
##      ROPE_Equivalence
## [1,] &quot;Undecided&quot;
## [2,] &quot;Rejected&quot;
## [3,] &quot;Undecided&quot;
## [4,] &quot;Undecided&quot;
## [5,] &quot;Undecided&quot;</code></pre>
<pre class="r"><code>h_df = as_tibble(insight::get_parameters(optimal_ax.model.v)) %&gt;%
  transmute(
    Intercept = b_Intercept,
    has_ax_test = -2 * b_has_ax1,
    has_dx_test = -2 * b_has_dx1,
    noise_level1_test = b_noise_level_f1,
    noise_level2_test = b_noise_level_f2,
    noise_level3_test = - b_noise_level_f1 - b_noise_level_f2
  )

hyp_results = test_hypotheses(h_df, rope_values = c(-0.055, 0.055))
print(hyp_results %&gt;% select(Parameter, pd, HDI_low, HDI_high, ROPE_Percentage, ROPE_Equivalence) %&gt;% as.matrix())</code></pre>
<pre><code>##      Parameter           pd      HDI_low  HDI_high ROPE_Percentage
## [1,] &quot;Intercept&quot;         &quot;0.659&quot; &quot;-1.873&quot; &quot;1.886&quot;  &quot;0.0785&quot;
## [2,] &quot;has_ax_test&quot;       &quot;1.000&quot; &quot; 0.152&quot; &quot;2.700&quot;  &quot;0.0000&quot;
## [3,] &quot;has_dx_test&quot;       &quot;0.719&quot; &quot;-1.172&quot; &quot;1.419&quot;  &quot;0.1235&quot;
## [4,] &quot;noise_level1_test&quot; &quot;0.948&quot; &quot;-0.376&quot; &quot;1.064&quot;  &quot;0.0550&quot;
## [5,] &quot;noise_level2_test&quot; &quot;0.916&quot; &quot;-1.213&quot; &quot;0.397&quot;  &quot;0.0670&quot;
## [6,] &quot;noise_level3_test&quot; &quot;0.505&quot; &quot;-0.886&quot; &quot;0.971&quot;  &quot;0.1857&quot;
##      ROPE_Equivalence
## [1,] &quot;Undecided&quot;
## [2,] &quot;Rejected&quot;
## [3,] &quot;Undecided&quot;
## [4,] &quot;Undecided&quot;
## [5,] &quot;Undecided&quot;
## [6,] &quot;Undecided&quot;</code></pre>
<pre class="r"><code>options(digits = 7)</code></pre>
<p><strong>Results</strong>:</p>
<ol style="list-style-type: decimal">
<li>AX suggestions
<ul>
<li>Positive effect pd = 100%</li>
<li>Median = 1.24, 89% CI [0.76, 1.78]</li>
<li>Small, Std.Median = 0.32</li>
<li>0.0% in ROPE</li>
</ul></li>
</ol>
<p>(The ROPE is defined as [-0.055, 0.055]. It is the range is suggested in the literature for logistic models and it corresponds to a probability range of 0.11. Therefore, a probability change of less than 0.055 is considered no different from a probability change of 0)</p>
</div>
<div id="posterior-plots-1" class="section level2">
<h2>Posterior Plots</h2>
<pre class="r"><code># A visualization data frame
gg_df = plot_df %&gt;%
    mutate(noise_level = fct_rev(noise_level)) %&gt;%
    mutate(optimal_ax = factor(optimal_ax)) %&gt;%
    mutate(user_id = factor(user_id)) %&gt;%
    mutate(optimal_ax = fct_recode(optimal_ax, incorrect=&quot;0&quot;, correct = &quot;1&quot;)) %&gt;%
    mutate(.fill_column = paste(noise_level, !(study_condition == &#39;BASELINE&#39;), sep = &quot;:&quot;)) %&gt;%
    count(user_id, study_condition, .fill_column, optimal_ax, num_actions, has_ax, has_dx, noise_level,
          name = &quot;num_optimal_ax&quot;)

# Simulate predictions on new data
p_df =
  plot_df %&gt;%
  data_grid(optimal_ax,
            has_ax,
            has_dx,
            noise_level_f,
            gender,
            age_group,
            robot_experience,
            num_optimal,
            state_idx_rescaled = seq_range(state_idx_rescaled, n = 4),
            .model = optimal_ax.model.v)

fits_p_df = p_df %&gt;% add_fitted_draws(optimal_ax.model.v,
                                      re_formula = NA,
                                      n = 20,
                                      seed = default_seed)
gc()</code></pre>
<pre><code>##            used  (Mb) gc trigger   (Mb)   max used    (Mb)
## Ncells  3645074 194.7   11598611  619.5   11598611   619.5
## Vcells 33069069 252.3 1196983400 9132.3 1458218023 11125.4</code></pre>
<pre class="r"><code># We don&#39;t want predicted draws because that&#39;s a raw count of ones and zeros; not the
# probability distribution
# preds_p_df = p_df %&gt;% add_predicted_draws(scenario_completed.model.v)

# Plot the posterior distributions
rope_value = gg_df %&gt;%
  filter(optimal_ax == &#39;correct&#39;, has_ax == F) %&gt;%
  summarise(.value = median(num_optimal_ax / num_actions)) %&gt;%
  pull(.value)
gg_df %&gt;%
  filter(optimal_ax == &#39;correct&#39;) %&gt;%
  ggplot(aes(x = has_ax, y = num_optimal_ax / num_actions)) +
    geom_rect(ymin = rope_value-0.055, ymax = rope_value+0.055,
              xmin = -Inf, xmax = Inf,
              color = &quot;transparent&quot;, fill = &quot;white&quot;) +
    annotate(&quot;text&quot;, x = 0.5, y = rope_value, label = &quot;ROPE&quot;, angle = 90) +
    stat_halfeye(aes(y = .value), data = fits_p_df, position = position_nudge(x = 0.1)) +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep = &quot;:&quot;)), size = 2) +
    stat_summary(fun.y = median, geom = &quot;point&quot;, shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(y = &quot;correct_ax&quot;, color = &quot;AX:DX:Noise&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-24-1.png' %}" width="672" /></p>
<pre class="r"><code>rope_value = gg_df %&gt;%
  filter(optimal_ax == &#39;correct&#39;, has_dx == F) %&gt;%
  summarise(.value = median(num_optimal_ax / num_actions)) %&gt;%
  pull(.value)
gg_df %&gt;%
  filter(optimal_ax == &#39;correct&#39;) %&gt;%
  ggplot(aes(x = has_dx, y = num_optimal_ax / num_actions)) +
    geom_rect(ymin = rope_value-0.055, ymax = rope_value+0.055,
              xmin = -Inf, xmax = Inf,
              color = &quot;transparent&quot;, fill = &quot;white&quot;) +
    annotate(&quot;text&quot;, x = 0.5, y = rope_value, label = &quot;ROPE&quot;, angle = 90) +
    stat_halfeye(aes(y = .value), data = fits_p_df, position = position_nudge(x = 0.1)) +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep = &quot;:&quot;)), size = 2) +
    stat_summary(fun.y = median, geom = &quot;point&quot;, shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(y = &quot;correct_ax&quot;, color = &quot;AX:DX:Noise&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-24-2.png' %}" width="672" /></p>
<pre class="r"><code>rope_value = gg_df %&gt;%
  filter(optimal_ax == &#39;correct&#39;, noise_level == &quot;0.0&quot;) %&gt;%
  summarise(.value = median(num_optimal_ax / num_actions)) %&gt;%
  pull(.value)
gg_df %&gt;%
  filter(optimal_ax == &#39;correct&#39;) %&gt;%
  mutate(noise_level = fct_rev(noise_level)) %&gt;%
  ggplot(aes(x = noise_level, y = num_optimal_ax / num_actions)) +
    geom_rect(ymin = rope_value-0.055, ymax = rope_value+0.055,
              xmin = -Inf, xmax = Inf,
              color = &quot;transparent&quot;, fill = &quot;white&quot;) +
    annotate(&quot;text&quot;, x = 0.5, y = rope_value, label = &quot;ROPE&quot;, angle = 90) +
    stat_halfeye(aes(y = .value, x = noise_level_f), data = fits_p_df, position = position_nudge(x = 0.1)) +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep = &quot;:&quot;)), size = 2) +
    stat_summary(fun.y = median, geom = &quot;point&quot;, shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(y = &quot;correct_ax&quot;, color = &quot;AX:DX:Noise&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-24-3.png' %}" width="672" /></p>
<pre class="r"><code># Remove the giant fits data frames
rm(fits_p_df, p_df, gg_df)</code></pre>
<pre class="r"><code>plot_df = actions %&gt;%
  select(X1, id, user_id, study_condition, start_condition, num_optimal,
         state_idx, state_idx_rescaled,
         age_group, gender, robot_experience,
         noise_level, noise_level_f, has_noise, has_dx, has_ax,
         scenario_completed, num_actions, optimal_ax) %&gt;%
  mutate(scenario_completed = factor(scenario_completed)) %&gt;%
  mutate(scenario_completed = fct_recode(scenario_completed, Unresolved=&quot;0&quot;, Resolved = &quot;1&quot;),
         has_ax = fct_recode(has_ax, &quot;No AX&quot;=&quot;FALSE&quot;, &quot;AX&quot;=&quot;TRUE&quot;),
         has_dx = fct_recode(has_dx, &quot;No DX&quot;=&quot;FALSE&quot;, &quot;DX&quot;=&quot;TRUE&quot;),
         noise_level = fct_recode(noise_level, &quot;Acc: 100%&quot;=&quot;0.0&quot;, &quot;Acc: 90%&quot;=&quot;1.0&quot;, &quot;Acc: 80%&quot;=&quot;2.0&quot;))

eff = fixef(optimal_ax.model.v)

gg_df = plot_df %&gt;%
  mutate(.fill_column = paste(noise_level, !(study_condition == &#39;BASELINE&#39;), sep = &quot;:&quot;)) %&gt;%
  count(user_id, study_condition, .fill_column, optimal_ax, num_actions, has_ax, has_dx, noise_level,
        name = &quot;num_optimal_ax&quot;) %&gt;%
  mutate(estimate = -1) %&gt;%
  mutate(estimate = if_else(has_ax == &quot;No AX&quot;, inv_logit_scaled(eff[&quot;Intercept&quot;, &quot;Estimate&quot;] + eff[&quot;has_ax1&quot;,&quot;Estimate&quot;]), estimate)) %&gt;%
  mutate(estimate = if_else(has_ax == &quot;AX&quot;, inv_logit_scaled(eff[&quot;Intercept&quot;, &quot;Estimate&quot;] - eff[&quot;has_ax1&quot;, &quot;Estimate&quot;]), estimate))

gg_df %&gt;%
  filter(optimal_ax == &quot;1&quot;) %&gt;%
  ggplot(aes(has_ax, num_optimal_ax / num_actions, group = has_ax, colour=has_ax)) +
    geom_count(aes(colour = NULL)) +
    geom_boxplot(aes(y = estimate)) +
    geom_signif(y_position = 1.07, xmin = 1, xmax = 2, annotation = &quot;*&quot;, textsize = 8, color = &quot;black&quot;) +
    scale_y_continuous(limits = c(0.0, 1.1), breaks = c(0.0, 0.5, 1.0)) +
    labs(y = NULL, x = NULL) +
    scale_colour_manual(values = c(&quot;#d62728&quot;, &quot;#2ca02c&quot;, &quot;#ff7f0e&quot;)) +
    theme_economist_white(base_size = 13) +
    theme(plot.background = element_rect(fill = &quot;white&quot;),
          axis.title.y = element_blank()) +
    legend_none()</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-25-1.png' %}" width="672" /></p>
</div>
</div>
<div id="rdx-rate-of-correct-diagnosis-selection" class="section level1">
<h1>RDX: Rate of Correct Diagnosis Selection</h1>
<p><strong>Did the user figure out the correct diagnoses for their situation?</strong></p>
<p>This is also a measure of Reliance on Suggestions. In the code, the variable might be referred to as <code>optimal_dx</code>, <code>correct_dx</code>, etc. depending on the version of the codebase</p>
<pre class="r"><code>plot_df = actions %&gt;%
  select(X1, id, user_id, study_condition, start_condition, num_optimal,
         state_idx, state_idx_rescaled,
         age_group, gender, robot_experience,
         noise_level, noise_level_f, has_noise, has_dx, has_ax,
         scenario_completed, num_actions, optimal_dx)

text_short(report(plot_df))</code></pre>
<pre><code>## Warning in report.numeric(model[[col]], median = median, centrality =
## centrality, : Variable `model[[col]]` contains only two different values.
## Consider converting it to a factor.

## Warning in report.numeric(model[[col]], median = median, centrality =
## centrality, : Variable `model[[col]]` contains only two different values.
## Consider converting it to a factor.</code></pre>
<pre><code>## The data contains 2126 observations of the following variables:
##   - X1: Mean = 1062.50, SD = 613.87, range = [0, 2125], 0 missing
##   - id: Mean = 1063.50, SD = 613.87, range = [1, 2126], 0 missing
##   - user_id: Mean = 102.65, SD = 57.16, range = [1, 200], 0 missing
##   - study_condition: 10 levels: DX_100, n = 247; AX_100, n = 152; DXAX_100, n = 193; DX_90, n = 254; AX_90, n = 217; DXAX_90, n = 183; DX_80, n = 256; AX_80, n = 204; DXAX_80, n = 189 and BASELINE, n = 231
##   - start_condition: 4 entries: kc.dt.occluding.above_mug.default.empty.dt, n = 672; kc.kc.occluding.default.default.empty.dt, n = 595; dt.kc.default.default.default.empty.kc, n = 498 and 1 other
##   - num_optimal: Mean = 4.78, SD = 1.55, range = [3, 7], 0 missing
##   - state_idx: Mean = 86.63, SD = 59.16, range = [1, 171], 0 missing
##   - state_idx_rescaled: Mean = 0.50, SD = 0.35, range = [0, 1], 0 missing
##   - age_group: 7 levels: 2, n = 226; 3, n = 493; 4, n = 497; 5, n = 283; 6, n = 222; 7, n = 170 and 8, n = 235
##   - gender: 2 levels: F, n = 844 and M, n = 1282
##   - robot_experience: 5 levels: 0, n = 1318; 1, n = 346; 2, n = 161; 3, n = 169 and 4, n = 132
##   - noise_level: 3 levels: 0.0, n = 823; 1.0, n = 654 and 2.0, n = 649
##   - noise_level_f: 3 levels: 0.0, n = 823; 1.0, n = 654 and 2.0, n = 649
##   - has_noise: 2 levels: FALSE, n = 823 and TRUE, n = 1303
##   - has_dx: 2 levels: FALSE, n = 804 and TRUE, n = 1322
##   - has_ax: 2 levels: FALSE, n = 988 and TRUE, n = 1138
##   - scenario_completed: Mean = 0.64, SD = 0.48, range = [0, 1], 0 missing
##   - num_actions: Mean = 13.90, SD = 5.85, range = [3, 20], 0 missing
##   - optimal_dx: Mean = 0.63, SD = 0.48, range = [0, 1], 0 missing</code></pre>
<div id="data-2" class="section level2">
<h2>Data</h2>
<pre class="r"><code>gg_df =
  plot_df %&gt;%
    mutate(noise_level = fct_rev(noise_level)) %&gt;%
    mutate(optimal_dx = factor(optimal_dx)) %&gt;%
    mutate(user_id = factor(user_id)) %&gt;%
    mutate(optimal_dx = fct_recode(optimal_dx, incorrect=&quot;0&quot;, correct = &quot;1&quot;)) %&gt;%
    mutate(.fill_column = paste(noise_level, !(study_condition == &#39;BASELINE&#39;), sep = &quot;:&quot;)) %&gt;%
    count(user_id, study_condition, .fill_column, optimal_dx, num_actions, has_ax, has_dx, noise_level,
          name = &quot;num_optimal_dx&quot;)

# Plot by the study condition
gg_df %&gt;%
  ggplot(aes(user_id, num_optimal_dx / num_actions, fill=optimal_dx)) +
    geom_bar(stat=&quot;identity&quot;) +
    geom_hline(yintercept = 0.5) +
    facet_wrap(vars(study_condition), nrow = 1, scales = &quot;free_y&quot;) +
    labs(y = &quot;Fraction correct diagnoses&quot;) +
    coord_flip() +
    scale_fill_economist()</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-28-1.png' %}" width="1440" /></p>
<pre class="r"><code>gg_df %&gt;%
  filter(optimal_dx == &quot;correct&quot;) %&gt;%
  ggplot(aes(study_condition, num_optimal_dx / num_actions, fill=.fill_column)) +
    geom_violin() +
    geom_boxplot(width = 0.1) +
    labs(y = &quot;Fraction correct diagnoses&quot;, fill = &quot;noise:has_suggestions&quot;) +
    scale_fill_economist()</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-28-2.png' %}" width="1440" /></p>
<pre class="r"><code># Visualize the data facetted by the variables that we care about
p1 = gg_df %&gt;%
  filter(optimal_dx == &quot;correct&quot;) %&gt;%
  ggplot(aes(has_ax, num_optimal_dx / num_actions, fill=.fill_column)) +
    geom_boxplot() +
    geom_count() +
    facet_grid(rows = vars(noise_level), cols = vars(has_dx), labeller = label_both) +
    labs(y = &quot;Fraction correct diagnoses&quot;) +
    scale_fill_economist() +
    theme(legend.position = &quot;left&quot;)

p2 = gg_df %&gt;%
  filter(optimal_dx == &quot;correct&quot;) %&gt;%
  ggplot(aes(has_dx, num_optimal_dx / num_actions, fill=.fill_column)) +
    geom_boxplot() +
    geom_count() +
    facet_grid(rows = vars(noise_level), cols = vars(has_ax), labeller = label_both) +
    labs(y = &quot;Fraction correct diagnoses&quot;) +
    scale_fill_economist() +
    legend_none()

p3 = gg_df %&gt;%
  filter(optimal_dx == &quot;correct&quot;) %&gt;%
  mutate(noise_level = fct_rev(noise_level)) %&gt;%
  mutate(has_ax = factor(has_ax)) %&gt;%
  mutate(has_ax = fct_rev(has_ax)) %&gt;%
  ggplot(aes(noise_level, num_optimal_dx / num_actions, fill=.fill_column)) +
    geom_boxplot() +
    geom_count() +
    facet_grid(rows = vars(has_ax), cols = vars(has_dx), labeller = label_both) +
    labs(y = &quot;Fraction correct diagnoses&quot;) +
    scale_fill_economist() +
    legend_none()

grid.arrange(p1, p2, p3, ncol = 3)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-28-3.png' %}" width="1440" /></p>
<pre class="r"><code># Visualize also the variation in each of the three levels that we care about
p1 = gg_df %&gt;%
  filter(optimal_dx == &#39;correct&#39;) %&gt;%
  ggplot(aes(has_ax, num_optimal_dx/num_actions)) +
    geom_violin() +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep=&quot;:&quot;), size = 2)) +
    stat_summary(fun.y = median, geom=&quot;point&quot;, shape=23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(color = &quot;AX:DX:Noise&quot;) +
    theme(legend.position = &quot;left&quot;) +
    guides(size = F)

p2 = gg_df %&gt;%
  filter(optimal_dx == &#39;correct&#39;) %&gt;%
  ggplot(aes(has_dx, num_optimal_dx/num_actions)) +
    geom_violin() +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep=&quot;:&quot;), size = 2)) +
    stat_summary(fun.y = median, geom=&quot;point&quot;, shape=23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    legend_none()

p3 = gg_df %&gt;%
  filter(optimal_dx == &#39;correct&#39;) %&gt;%
  ggplot(aes(noise_level, num_optimal_dx/num_actions)) +
    geom_violin() +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep=&quot;:&quot;), size = 2)) +
    stat_summary(fun.y = median, geom=&quot;point&quot;, shape=23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    scale_x_discrete(limits = rev(levels(gg_df$noise_level))) +
    legend_none()

grid.arrange(p1, p2, p3, ncol = 3)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-28-4.png' %}" width="1440" /></p>
</div>
<div id="model-2" class="section level2">
<h2>Model</h2>
<p>Based on the data, we assume the following structural model:</p>
<p><span class="math display">\[rdx_{ij} = Bernoulli(p_{ij})\]</span> <span class="math display">\[\begin{aligned}
logit^{-1}(p_{ij}) &amp;= \beta_0 + \beta_{0i} + \beta_{ax}\text{ax}_i + \beta_{dx}\text{dx}_i + \beta_{noise}\text{noise}_i +\\
&amp;\beta_{ax:noise}\text{ax}_i\text{noise}_i + \beta_{dx:noise}\text{dx}_i\text{noise}_i + \\
&amp;\beta_{no}\text{no}_i + \mathbf{\beta_{demo}X_{demo,i}} + \beta_{state} \text{state}_{ij}
\end{aligned}\]</span> <span class="math display">\[\begin{aligned}
\beta_{.} &amp;\sim Normal(0, 10) \\
\beta_{0i} &amp;\sim Normal(0, \sigma_i) \\
\sigma_i &amp;\sim HalfStudent(3, 0, 10)
\end{aligned}\]</span></p>
<p>The prior for the <span class="math inline">\(\sigma_i\)</span> parameter is the default used in <code>brms</code>; I see no need to change it.</p>
<p>Model fitting:</p>
<pre class="r"><code># The null model
optimal_dx.model.null = brm(
  optimal_dx ~ 0 + Intercept + (1 | user_id),
  family = &quot;bernoulli&quot;,
  prior = c(set_prior(&quot;normal(0, 10)&quot;, class = &quot;b&quot;),
            set_prior(&quot;student_t(3, 0, 10)&quot;, class = &quot;sd&quot;)),
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
optimal_dx.model.null = add_criterion(optimal_dx.model.null, &quot;waic&quot;)
optimal_dx.model.null = add_criterion(optimal_dx.model.null, &quot;loo&quot;, reloo = T)
saveModel(optimal_dx.model.null)

# The trend model to see if there is a trend in the noise level variable
optimal_dx.model.t = brm(
  optimal_dx ~ 0 + Intercept + ((has_dx + has_ax) * noise_level) + (age_group + robot_experience + gender) + num_optimal + state_idx_rescaled + (1 | user_id),
  family = bernoulli,
  prior = c(set_prior(&quot;normal(0, 10)&quot;, class = &quot;b&quot;),
            set_prior(&quot;student_t(3, 0, 10)&quot;, class = &quot;sd&quot;)),
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
optimal_dx.model.t = add_criterion(optimal_dx.model.t, &quot;waic&quot;)
optimal_dx.model.t = add_criterion(optimal_dx.model.t, &quot;loo&quot;, reloo = T)
saveModel(optimal_dx.model.t)

# The values model, to see if specific values of the noise level variable are significant
optimal_dx.model.v = brm(
  optimal_dx ~ 0 + Intercept + ((has_dx + has_ax) * noise_level_f) + (age_group + robot_experience + gender) + num_optimal + state_idx_rescaled + (1 | user_id),
  family = bernoulli,
  prior = c(set_prior(&quot;normal(0, 10)&quot;, class = &quot;b&quot;),
            set_prior(&quot;student_t(3, 0, 10)&quot;, class = &quot;sd&quot;)),
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
optimal_dx.model.v = add_criterion(optimal_dx.model.v, &quot;waic&quot;)
optimal_dx.model.v = add_criterion(optimal_dx.model.v, &quot;loo&quot;, reloo = T)
saveModel(optimal_dx.model.v)</code></pre>
<pre class="r"><code># Load the models
optimal_dx.model.null = loadModel(&quot;optimal_dx.model.null&quot;)
optimal_dx.model.t = loadModel(&quot;optimal_dx.model.t&quot;)
optimal_dx.model.v = loadModel(&quot;optimal_dx.model.v&quot;)</code></pre>
<p>Model fitting results:</p>
<pre class="r"><code># print(tidy_stan(optimal_dx.model.null))
print(performance::r2(optimal_dx.model.null))</code></pre>
<pre><code>## # Bayesian R2 with Standard Error
##
##   Conditional R2: 0.228 [0.014]
##      Marginal R2: 0.000 [0.000]</code></pre>
<pre class="r"><code># Print the parameters, and some initial diagnostics
print(tidy_stan(optimal_dx.model.t, effects = &quot;fixed&quot;))</code></pre>
<pre><code>## Warning: &#39;tidy_stan&#39; is deprecated.
## Use &#39;parameters::model_parameters()&#39; instead.
## See help(&quot;Deprecated&quot;)</code></pre>
<pre><code>##
## Summary Statistics of Stan-Model
##
## # Fixed effects
##
##              Parameter Estimate Std.Error       HDI(89%)    pd  ESS Rhat MCSE
##            (Intercept)     0.21      0.38 [-0.41,  0.84] 70.7%  953 1.00 0.01
##                has_dx1    -0.27      0.13 [-0.47, -0.06] 98.4% 1095 1.00 0.00
##                has_ax1    -0.10      0.12 [-0.29,  0.12] 79.5% 1328 1.00 0.00
##          noise_level.L    -0.42      0.21 [-0.78, -0.10] 97.9% 1173 1.00 0.01
##          noise_level.Q    -0.00      0.22 [-0.35,  0.36] 50.7% 1267 1.00 0.01
##             age_group1     0.49      0.31 [-0.02,  0.97] 94.9% 1368 1.00 0.01
##             age_group2    -0.20      0.22 [-0.55,  0.18] 79.7% 1182 1.00 0.01
##             age_group3    -0.25      0.21 [-0.59,  0.10] 86.1% 1451 1.00 0.01
##             age_group4     0.16      0.25 [-0.28,  0.55] 73.8% 1484 1.00 0.01
##             age_group5    -0.34      0.33 [-0.86,  0.16] 84.9% 1251 1.00 0.01
##             age_group6     0.28      0.36 [-0.26,  0.84] 78.1% 1605 1.00 0.01
##      robot_experience1     0.54      0.20 [ 0.23,  0.86] 99.6% 1384 1.01 0.01
##      robot_experience2     0.24      0.27 [-0.22,  0.64] 80.4% 1195 1.00 0.01
##      robot_experience3    -0.14      0.36 [-0.68,  0.46] 65.2% 1470 1.00 0.01
##      robot_experience4    -0.32      0.37 [-0.87,  0.26] 80.3% 1429 1.00 0.01
##                gender1    -0.16      0.11 [-0.34,  0.02] 92.3% 1496 1.00 0.00
##            num_optimal     0.02      0.07 [-0.09,  0.14] 59.5%  915 1.00 0.00
##     state_idx_rescaled     0.46      0.17 [ 0.19,  0.72] 99.7% 3468 1.00 0.00
##  has_dx1.noise_level.L     0.14      0.21 [-0.20,  0.49] 74.6% 1151 1.00 0.01
##  has_dx1.noise_level.Q    -0.03      0.23 [-0.39,  0.35] 55.5% 1337 1.00 0.01
##  has_ax1.noise_level.L    -0.12      0.20 [-0.44,  0.20] 73.0% 1443 1.00 0.01
##  has_ax1.noise_level.Q     0.00      0.23 [-0.35,  0.40] 50.3% 1364 1.00 0.01</code></pre>
<pre class="r"><code>print(performance::r2(optimal_dx.model.t))</code></pre>
<pre><code>## # Bayesian R2 with Standard Error
##
##   Conditional R2: 0.241 [0.013]
##      Marginal R2: 0.086 [0.020]</code></pre>
<pre class="r"><code>print(tidy_stan(optimal_dx.model.v, effects = &quot;fixed&quot;))</code></pre>
<pre><code>## Warning: &#39;tidy_stan&#39; is deprecated.
## Use &#39;parameters::model_parameters()&#39; instead.
## See help(&quot;Deprecated&quot;)</code></pre>
<pre><code>##
## Summary Statistics of Stan-Model
##
## # Fixed effects
##
##               Parameter Estimate Std.Error       HDI(89%)    pd  ESS Rhat MCSE
##             (Intercept)     0.20      0.40 [-0.45,  0.83] 70.1% 1691 1.00 0.01
##                 has_dx1    -0.27      0.13 [-0.48, -0.08] 98.7% 1887 1.00 0.00
##                 has_ax1    -0.11      0.13 [-0.30,  0.10] 79.9% 1773 1.00 0.00
##          noise_level_f1     0.29      0.17 [ 0.03,  0.55] 96.4% 1924 1.00 0.00
##          noise_level_f2     0.00      0.19 [-0.31,  0.28] 50.7% 1487 1.00 0.00
##              age_group1     0.50      0.31 [-0.00,  1.01] 94.0% 1681 1.00 0.01
##              age_group2    -0.19      0.23 [-0.55,  0.20] 79.5% 2128 1.00 0.01
##              age_group3    -0.25      0.22 [-0.60,  0.10] 87.2% 1722 1.00 0.01
##              age_group4     0.16      0.26 [-0.28,  0.55] 72.6% 2202 1.00 0.01
##              age_group5    -0.35      0.33 [-0.87,  0.18] 85.5% 1650 1.00 0.01
##              age_group6     0.29      0.35 [-0.24,  0.92] 80.9% 1938 1.00 0.01
##       robot_experience1     0.53      0.20 [ 0.22,  0.87] 99.6% 1798 1.00 0.00
##       robot_experience2     0.24      0.26 [-0.22,  0.64] 81.1% 1567 1.00 0.01
##       robot_experience3    -0.13      0.36 [-0.71,  0.42] 64.2% 1701 1.00 0.01
##       robot_experience4    -0.31      0.37 [-0.85,  0.30] 79.2% 1897 1.00 0.01
##                 gender1    -0.16      0.12 [-0.34,  0.03] 91.8% 1999 1.00 0.00
##             num_optimal     0.02      0.07 [-0.10,  0.13] 60.4% 1822 1.00 0.00
##      state_idx_rescaled     0.46      0.17 [ 0.17,  0.72] 99.7% 4494 1.00 0.00
##  has_dx1:noise_level_f1    -0.11      0.16 [-0.36,  0.15] 74.9% 1787 1.00 0.00
##  has_dx1:noise_level_f2     0.01      0.18 [-0.28,  0.32] 53.1% 1805 1.00 0.00
##  has_ax1:noise_level_f1     0.09      0.17 [-0.17,  0.37] 70.7% 1922 1.00 0.00
##  has_ax1:noise_level_f2    -0.02      0.20 [-0.35,  0.28] 54.1% 1602 1.00 0.00</code></pre>
<pre class="r"><code>print(performance::r2(optimal_dx.model.v))</code></pre>
<pre><code>## # Bayesian R2 with Standard Error
##
##   Conditional R2: 0.241 [0.014]
##      Marginal R2: 0.086 [0.019]</code></pre>
</div>
<div id="diagnostic-2" class="section level2">
<h2>Diagnostic</h2>
<pre class="r"><code># Alternative diagnostics: diagnostic_posterior(scenario_completed.model.v)

# LOO cross-validation results
print(optimal_dx.model.t$criteria$loo$estimates)</code></pre>
<pre><code>##            Estimate        SE
## elpd_loo -1214.6403 21.523132
## p_loo      131.9603  3.788277
## looic     2429.2806 43.046264</code></pre>
<pre class="r"><code>print(optimal_dx.model.v$criteria$loo$estimates)</code></pre>
<pre><code>##            Estimate        SE
## elpd_loo -1215.0062 21.553824
## p_loo      132.3998  3.814729
## looic     2430.0123 43.107648</code></pre>
<pre class="r"><code>print(loo_compare(optimal_dx.model.null, optimal_dx.model.t, optimal_dx.model.v))</code></pre>
<pre><code>##                       elpd_diff se_diff
## optimal_dx.model.null  0.0       0.0
## optimal_dx.model.t    -2.3       4.0
## optimal_dx.model.v    -2.7       4.0</code></pre>
<pre class="r"><code># Diagnostics of the sampling process
par(mfrow=c(1,2))
plot(optimal_dx.model.t$criteria$loo, main = &quot;Trend Model&quot;)
plot(optimal_dx.model.v$criteria$loo, main = &quot;Values Model&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-32-1.png' %}" width="1440" /></p>
<pre class="r"><code># Include posterior predictive checks of the sampling (are sampling according
# to the data that we actually have?)
p1 = pp_check(optimal_dx.model.t, type = &quot;bars_grouped&quot;, group = &quot;has_ax&quot;, stat = &quot;median&quot;) + ggtitle(&quot;t:has_ax&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars_grouped&#39; by default.</code></pre>
<pre><code>## Warning: The following arguments were unrecognized and ignored: stat</code></pre>
<pre class="r"><code>p2 = pp_check(optimal_dx.model.t, type = &quot;bars_grouped&quot;, group = &quot;has_dx&quot;) + ggtitle(&quot;t:has_dx&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars_grouped&#39; by default.</code></pre>
<pre class="r"><code>p3 = pp_check(optimal_dx.model.t, type = &quot;bars_grouped&quot;, group = &quot;noise_level&quot;) + ggtitle(&quot;t:noise_level&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars_grouped&#39; by default.</code></pre>
<pre class="r"><code>grid.arrange(p1, p2, p3, nrow = 3)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-32-2.png' %}" width="1440" /></p>
<pre class="r"><code>mcmc_intervals(as.matrix(optimal_dx.model.t), regex_pars = &quot;b_|^shape&quot;, prob = 0.90, prob_outer = 0.95) +
  ggtitle(&quot;Trends Model&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-32-3.png' %}" width="1440" /></p>
<pre class="r"><code>p1 = pp_check(optimal_dx.model.v, type = &quot;bars_grouped&quot;, group = &quot;has_ax&quot;) + ggtitle(&quot;v:has_ax&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars_grouped&#39; by default.</code></pre>
<pre class="r"><code>p2 = pp_check(optimal_dx.model.v, type = &quot;bars_grouped&quot;, group = &quot;has_dx&quot;) + ggtitle(&quot;v:has_dx&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars_grouped&#39; by default.</code></pre>
<pre class="r"><code>p3 = pp_check(optimal_dx.model.v, type = &quot;bars_grouped&quot;, group = &quot;noise_level_f&quot;) + ggtitle(&quot;v:noise_level&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars_grouped&#39; by default.</code></pre>
<pre class="r"><code>grid.arrange(p1, p2, p3, nrow = 3)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-32-4.png' %}" width="1440" /></p>
<pre class="r"><code>mcmc_intervals(as.matrix(optimal_dx.model.v), regex_pars = &quot;b_|^shape&quot;, prob = 0.90, prob_outer = 0.95) +
  ggtitle(&quot;Values Model&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-32-5.png' %}" width="1440" /></p>
<pre class="r"><code># Plot predictions vs original
preds_p_df = optimal_dx.model.null %&gt;% predict() %&gt;% as_tibble()
p1 =
  preds_p_df %&gt;%
    add_column(optimal_dx = optimal_dx.model.null$data$optimal_dx) %&gt;%
    ggplot(aes(x = Estimate, y = optimal_dx, color = optimal_dx)) +
      geom_point() +
      labs(x = &quot;Predicted values&quot;) +
      ylim(0, 1) +
      xlim(0, 1) +
      ggtitle(&quot;Null Model&quot;)

preds_p_df = plot_df %&gt;%
  add_predicted_draws(optimal_dx.model.null) %&gt;%
  group_by(optimal_dx, .prediction) %&gt;%
  count()
p2 =
  preds_p_df %&gt;%
    ggplot(aes(x = optimal_dx, y = .prediction, fill = n / sum(preds_p_df$n))) +
      geom_tile() +
      geom_text(aes(label = sprintf(&quot;%1.2f&quot;, n / sum(preds_p_df$n))), size = 10, vjust = 1) +
      scale_fill_gradient(limits = c(0, 1))

preds_p_df = optimal_dx.model.t %&gt;% predict() %&gt;% as_tibble()
p3 =
  preds_p_df %&gt;%
    add_column(optimal_dx = optimal_dx.model.t$data$optimal_dx) %&gt;%
    ggplot(aes(x = Estimate, y = optimal_dx, color = optimal_dx)) +
      geom_point() +
      labs(x = &quot;Predicted values&quot;) +
      ylim(0, 1) +
      xlim(0, 1) +
      ggtitle(&quot;Trends Model&quot;)

preds_p_df = plot_df %&gt;%
  add_predicted_draws(optimal_dx.model.t) %&gt;%
  group_by(optimal_dx, .prediction) %&gt;%
  count()
p4 =
  preds_p_df %&gt;%
    ggplot(aes(x = optimal_dx, y = .prediction, fill = n / sum(preds_p_df$n))) +
      geom_tile() +
      geom_text(aes(label = sprintf(&quot;%1.2f&quot;, n / sum(preds_p_df$n))), size = 10, vjust = 1) +
      scale_fill_gradient(limits = c(0, 1))

# Plot predictions vs original
preds_p_df = optimal_dx.model.v %&gt;% predict() %&gt;% as_tibble()
p5 =
  preds_p_df %&gt;%
    add_column(optimal_dx = optimal_dx.model.v$data$optimal_dx) %&gt;%
    ggplot(aes(x = Estimate, y = optimal_dx, color = optimal_dx)) +
      geom_point() +
      labs(x = &quot;Predicted values&quot;) +
      ylim(0, 1) +
      xlim(0, 1) +
      ggtitle(&quot;Values Model&quot;)

preds_p_df = plot_df %&gt;%
  add_predicted_draws(optimal_dx.model.v) %&gt;%
  group_by(optimal_dx, .prediction) %&gt;%
  count()
p6 =
  preds_p_df %&gt;%
    ggplot(aes(x = optimal_dx, y = .prediction, fill = n / sum(preds_p_df$n))) +
      geom_tile() +
      geom_text(aes(label = sprintf(&quot;%1.2f&quot;, n / sum(preds_p_df$n))), size = 10, vjust = 1) +
      scale_fill_gradient(limits = c(0, 1))

grid.arrange(p1, p3, p5, p2, p4, p6, nrow = 2, ncol = 3)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-32-6.png' %}" width="1440" /></p>
</div>
<div id="inference-2" class="section level2">
<h2>Inference</h2>
<pre class="r"><code># Compute the bayes factor for each parameter value. Note: we compare
# here against the ROPE, and not the degenerate null hypothesis of 0
# Bayes-Factor values would be more significant in the latter case
# According to the Raftery, 1995 rules
# bf = 1 - 3: Weak
# bf = 3 - 20: Positive
# bf = 20 - 150: Strong
# bf &gt; 150: Very strong
print(bayesfactor_rope(optimal_dx.model.null, null = c(-0.055, 0.055)), n = 30)
print(bayesfactor_rope(optimal_dx.model.t, null = c(-0.055, 0.055)), n = 30)
print(bayesfactor_rope(optimal_dx.model.v, null = c(-0.055, 0.055)), n = 30)</code></pre>
<pre class="r"><code>options(digits = 3)
# Check the significance of each hypothesis
h_df = as_tibble(insight::get_parameters(optimal_dx.model.t)) %&gt;%
  transmute(
    Intercept = b_Intercept,
    has_ax_test = -2 * b_has_ax1,
    has_dx_test = -2 * b_has_dx1,
    noise_levelL_test = b_noise_level.L,
    noise_levelQ_test = b_noise_level.Q
  )

hyp_results = test_hypotheses(h_df, rope_values = c(-0.055, 0.055))
print(hyp_results %&gt;% select(Parameter, pd, HDI_low, HDI_high, ROPE_Percentage, ROPE_Equivalence) %&gt;% as.matrix())</code></pre>
<pre><code>##      Parameter           pd      HDI_low  HDI_high ROPE_Percentage
## [1,] &quot;Intercept&quot;         &quot;0.707&quot; &quot;-1.169&quot; &quot;1.672&quot;  &quot;0.1000&quot;
## [2,] &quot;has_ax_test&quot;       &quot;0.794&quot; &quot;-0.662&quot; &quot;1.249&quot;  &quot;0.1222&quot;
## [3,] &quot;has_dx_test&quot;       &quot;0.984&quot; &quot;-0.444&quot; &quot;1.447&quot;  &quot;0.0198&quot;
## [4,] &quot;noise_levelL_test&quot; &quot;0.979&quot; &quot;-1.156&quot; &quot;0.345&quot;  &quot;0.0267&quot;
## [5,] &quot;noise_levelQ_test&quot; &quot;0.507&quot; &quot;-0.777&quot; &quot;0.988&quot;  &quot;0.1905&quot;
##      ROPE_Equivalence
## [1,] &quot;Undecided&quot;
## [2,] &quot;Undecided&quot;
## [3,] &quot;Rejected&quot;
## [4,] &quot;Undecided&quot;
## [5,] &quot;Undecided&quot;</code></pre>
<pre class="r"><code>h_df = as_tibble(insight::get_parameters(optimal_dx.model.v)) %&gt;%
  transmute(
    Intercept = b_Intercept,
    has_ax_test = -2 * b_has_ax1,
    has_dx_test = -2 * b_has_dx1,
    noise_level1_test = b_noise_level_f1,
    noise_level2_test = b_noise_level_f2,
    noise_level3_test = - b_noise_level_f1 - b_noise_level_f2
  )

hyp_results = test_hypotheses(h_df, rope_values = c(-0.055, 0.055))
print(hyp_results %&gt;% select(Parameter, pd, HDI_low, HDI_high, ROPE_Percentage, ROPE_Equivalence) %&gt;% as.matrix())</code></pre>
<pre><code>##      Parameter           pd      HDI_low  HDI_high ROPE_Percentage
## [1,] &quot;Intercept&quot;         &quot;0.701&quot; &quot;-1.142&quot; &quot;1.707&quot;  &quot;0.0990&quot;
## [2,] &quot;has_ax_test&quot;       &quot;0.799&quot; &quot;-0.711&quot; &quot;1.148&quot;  &quot;0.1195&quot;
## [3,] &quot;has_dx_test&quot;       &quot;0.987&quot; &quot;-0.499&quot; &quot;1.419&quot;  &quot;0.0177&quot;
## [4,] &quot;noise_level1_test&quot; &quot;0.964&quot; &quot;-0.229&quot; &quot;0.932&quot;  &quot;0.0648&quot;
## [5,] &quot;noise_level2_test&quot; &quot;0.507&quot; &quot;-0.595&quot; &quot;0.736&quot;  &quot;0.2200&quot;
## [6,] &quot;noise_level3_test&quot; &quot;0.949&quot; &quot;-1.048&quot; &quot;0.429&quot;  &quot;0.0630&quot;
##      ROPE_Equivalence
## [1,] &quot;Undecided&quot;
## [2,] &quot;Undecided&quot;
## [3,] &quot;Rejected&quot;
## [4,] &quot;Undecided&quot;
## [5,] &quot;Undecided&quot;
## [6,] &quot;Undecided&quot;</code></pre>
<pre class="r"><code>options(digits = 7)</code></pre>
<p><strong>Results</strong>:</p>
<ol style="list-style-type: decimal">
<li>DX suggestions
<ul>
<li>Positive effect pd = 98.4%</li>
<li>Median = 0.54, 89% CI [0.12, 0.94]</li>
<li>Small, Std.Median = 0.26</li>
<li>1.98% in ROPE</li>
</ul></li>
<li>Noise Level Linear
<ul>
<li>Negative (negative slope) pd = 97.9%</li>
<li>Median = -0.42, 89% CI [-0.78, -0.10]</li>
<li>Small, Std.Median = 0.21</li>
<li>2.67% in ROPE (n.s.)</li>
</ul></li>
</ol>
<p>(The ROPE is defined as [-0.055, 0.055]. It is the range is suggested in the literature for logistic models and it corresponds to a probability range of 0.11. Therefore, a probability change of less than 0.055 is considered no different from a probability change of 0)</p>
</div>
<div id="posterior-plots-2" class="section level2">
<h2>Posterior Plots</h2>
<pre class="r"><code># A visualization data frame
gg_df = plot_df %&gt;%
    mutate(noise_level = fct_rev(noise_level)) %&gt;%
    mutate(optimal_dx = factor(optimal_dx)) %&gt;%
    mutate(user_id = factor(user_id)) %&gt;%
    mutate(optimal_dx = fct_recode(optimal_dx, incorrect=&quot;0&quot;, correct = &quot;1&quot;)) %&gt;%
    mutate(.fill_column = paste(noise_level, !(study_condition == &#39;BASELINE&#39;), sep = &quot;:&quot;)) %&gt;%
    count(user_id, study_condition, .fill_column, optimal_dx, num_actions, has_ax, has_dx, noise_level,
          name = &quot;num_optimal_dx&quot;)

# Simulate predictions on new data
p_df =
  plot_df %&gt;%
  data_grid(optimal_dx,
            has_ax,
            has_dx,
            noise_level_f,
            gender,
            age_group,
            robot_experience,
            num_optimal,
            state_idx_rescaled = seq_range(state_idx_rescaled, n = 4),
            .model = optimal_dx.model.v)

fits_p_df = p_df %&gt;% add_fitted_draws(optimal_dx.model.v,
                                      re_formula = NA,
                                      n = 20,
                                      seed = default_seed)
gc()</code></pre>
<pre><code>##            used  (Mb) gc trigger   (Mb)   max used    (Mb)
## Ncells  4075199 217.7    9278889  495.6   11598611   619.5
## Vcells 33569028 256.2 1151111057 8782.3 1458218023 11125.4</code></pre>
<pre class="r"><code>pars_p_df = optimal_dx.model.t %&gt;%
  extract_draws(newdata = p_df %&gt;% rename(noise_level = noise_level_f),
                re_formula = NA,
                nsamples = 100)
gc()</code></pre>
<pre><code>##            used  (Mb) gc trigger   (Mb)   max used    (Mb)
## Ncells  4075288 217.7    9278889  495.6   11598611   619.5
## Vcells 34045211 259.8  920888846 7025.9 1458218023 11125.4</code></pre>
<pre class="r"><code># We don&#39;t want predicted draws because that&#39;s a raw count of ones and zeros; not the
# probability distribution
# preds_p_df = p_df %&gt;% add_predicted_draws(scenario_completed.model.v)

# Plot the posterior distributions
rope_value = gg_df %&gt;%
  filter(optimal_dx == &#39;correct&#39;, has_ax == F) %&gt;%
  summarise(.value = median(num_optimal_dx / num_actions)) %&gt;%
  pull(.value)
gg_df %&gt;%
  filter(optimal_dx == &#39;correct&#39;) %&gt;%
  ggplot(aes(x = has_ax, y = num_optimal_dx / num_actions)) +
    geom_rect(ymin = rope_value-0.055, ymax = rope_value+0.055,
              xmin = -Inf, xmax = Inf,
              color = &quot;transparent&quot;, fill = &quot;white&quot;) +
    annotate(&quot;text&quot;, x = 0.5, y = rope_value, label = &quot;ROPE&quot;, angle = 90) +
    stat_halfeye(aes(y = .value), data = fits_p_df, position = position_nudge(x = 0.1)) +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep = &quot;:&quot;)), size = 2) +
    stat_summary(fun.y = median, geom = &quot;point&quot;, shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(y = &quot;correct_ax&quot;, color = &quot;AX:DX:Noise&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-35-1.png' %}" width="672" /></p>
<pre class="r"><code>rope_value = gg_df %&gt;%
  filter(optimal_dx == &#39;correct&#39;, has_dx == F) %&gt;%
  summarise(.value = median(num_optimal_dx / num_actions)) %&gt;%
  pull(.value)
gg_df %&gt;%
  filter(optimal_dx == &#39;correct&#39;) %&gt;%
  ggplot(aes(x = has_dx, y = num_optimal_dx / num_actions)) +
    geom_rect(ymin = rope_value-0.055, ymax = rope_value+0.055,
              xmin = -Inf, xmax = Inf,
              color = &quot;transparent&quot;, fill = &quot;white&quot;) +
    annotate(&quot;text&quot;, x = 0.5, y = rope_value, label = &quot;ROPE&quot;, angle = 90) +
    stat_halfeye(aes(y = .value), data = fits_p_df, position = position_nudge(x = 0.1)) +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep = &quot;:&quot;)), size = 2) +
    stat_summary(fun.y = median, geom = &quot;point&quot;, shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(y = &quot;correct_ax&quot;, color = &quot;AX:DX:Noise&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-35-2.png' %}" width="672" /></p>
<pre class="r"><code>rope_value = gg_df %&gt;%
  filter(optimal_dx == &#39;correct&#39;, noise_level == &quot;0.0&quot;) %&gt;%
  summarise(.value = median(num_optimal_dx / num_actions)) %&gt;%
  pull(.value)
gg_df %&gt;%
  filter(optimal_dx == &#39;correct&#39;) %&gt;%
  mutate(noise_level = fct_rev(noise_level)) %&gt;%
  ggplot(aes(x = noise_level, y = num_optimal_dx / num_actions)) +
    geom_rect(ymin = rope_value-0.055, ymax = rope_value+0.055,
              xmin = -Inf, xmax = Inf,
              color = &quot;transparent&quot;, fill = &quot;white&quot;) +
    annotate(&quot;text&quot;, x = 0.5, y = rope_value, label = &quot;ROPE&quot;, angle = 90) +
    stat_halfeye(aes(y = .value, x = noise_level_f), data = fits_p_df, position = position_nudge(x = 0.1)) +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep = &quot;:&quot;)), size = 2) +
    stat_summary(fun.y = median, geom = &quot;point&quot;, shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(y = &quot;correct_ax&quot;, color = &quot;AX:DX:Noise&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-35-3.png' %}" width="672" /></p>
<pre class="r"><code># Create a plot to show the trend
pars_p_df_X = pars_p_df$dpars$mu$fe$X[,c(&quot;Intercept&quot;, &quot;noise_level.L&quot;)]
pars_p_df_b = pars_p_df$dpars$mu$fe$b[,c(&quot;b_Intercept&quot;, &quot;b_noise_level.L&quot;)]
pars_p_df_y = pars_p_df_X %*% t(pars_p_df_b)
pars_p_df_y = inv_logit_scaled(pars_p_df_y)
p_df =
  p_df %&gt;%
  bind_cols(as_tibble(pars_p_df_y)) %&gt;%
  gather(key = &quot;.sample&quot;, value = &quot;.value&quot;, V1:V100) %&gt;%
  rename(noise_level = noise_level_f)

rope_value = gg_df %&gt;%
  filter(optimal_dx == &#39;correct&#39;, noise_level == &quot;0.0&quot;) %&gt;%
  summarise(.value = median(num_optimal_dx / num_actions)) %&gt;%
  pull(.value)
gg_df %&gt;%
  filter(optimal_dx == &#39;correct&#39;) %&gt;%
  mutate(noise_level = fct_rev(noise_level)) %&gt;%
  ggplot(aes(x = noise_level, y = num_optimal_dx / num_actions)) +
    geom_rect(ymin = rope_value-0.055, ymax = rope_value+0.055,
              xmin = -Inf, xmax = Inf,
              color = &quot;transparent&quot;, fill = &quot;white&quot;) +
    annotate(&quot;text&quot;, x = 0.5, y = rope_value, label = &quot;ROPE&quot;, angle = 90) +
    stat_halfeye(aes(y = .value, x = noise_level_f), data = fits_p_df, position = position_nudge(x = 0.1)) +
    geom_line(aes(y = .value, group = .sample, alpha = 0.3), data = p_df) +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep = &quot;:&quot;)), size = 2) +
    stat_summary(fun.y = median, geom = &quot;point&quot;, shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(y = &quot;scenario_completed&quot;, color = &quot;AX:DX:Noise&quot;) +
    guides(alpha = F)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-35-4.png' %}" width="672" /></p>
<pre class="r"><code># Remove the giant fits data frames
rm(fits_p_df, pars_p_df, pars_p_df_b, pars_p_df_X, pars_p_df_y, p_df, gg_df)</code></pre>
<pre class="r"><code>plot_df = actions %&gt;%
  select(X1, id, user_id, study_condition, start_condition, num_optimal,
         state_idx, state_idx_rescaled,
         age_group, gender, robot_experience,
         noise_level, noise_level_f, has_noise, has_dx, has_ax,
         scenario_completed, num_actions, optimal_dx) %&gt;%
  mutate(scenario_completed = factor(scenario_completed)) %&gt;%
  mutate(scenario_completed = fct_recode(scenario_completed, Unresolved=&quot;0&quot;, Resolved = &quot;1&quot;),
         has_ax = fct_recode(has_ax, &quot;No AX&quot;=&quot;FALSE&quot;, &quot;AX&quot;=&quot;TRUE&quot;),
         has_dx = fct_recode(has_dx, &quot;No DX&quot;=&quot;FALSE&quot;, &quot;DX&quot;=&quot;TRUE&quot;),
         noise_level = fct_recode(noise_level, &quot;Acc: 100%&quot;=&quot;0.0&quot;, &quot;Acc: 90%&quot;=&quot;1.0&quot;, &quot;Acc: 80%&quot;=&quot;2.0&quot;))

eff = fixef(optimal_dx.model.v)

gg_df = plot_df %&gt;%
  mutate(.fill_column = paste(noise_level, !(study_condition == &#39;BASELINE&#39;), sep = &quot;:&quot;)) %&gt;%
  count(user_id, study_condition, .fill_column, optimal_dx, num_actions, has_ax, has_dx, noise_level,
        name = &quot;num_correct_dx&quot;) %&gt;%
  mutate(estimate = -1) %&gt;%
  mutate(estimate = if_else(has_dx == &quot;No DX&quot;, inv_logit_scaled(eff[&quot;Intercept&quot;, &quot;Estimate&quot;] + eff[&quot;has_dx1&quot;,&quot;Estimate&quot;]), estimate)) %&gt;%
  mutate(estimate = if_else(has_dx == &quot;DX&quot;, inv_logit_scaled(eff[&quot;Intercept&quot;, &quot;Estimate&quot;] - eff[&quot;has_dx1&quot;, &quot;Estimate&quot;]), estimate))

gg_df %&gt;%
  filter(optimal_dx == &quot;1&quot;) %&gt;%
  ggplot(aes(has_dx, num_correct_dx / num_actions, group = has_dx, colour=has_dx)) +
    geom_count(aes(colour = NULL)) +
    geom_boxplot(aes(y = estimate)) +
    geom_signif(y_position = 1.15, xmin = 1, xmax = 2, annotation = &quot;*&quot;, textsize = 8, color = &quot;black&quot;) +
    scale_y_continuous(limits = c(0.0, 1.25), breaks = c(0.0, 0.5, 1.0)) +
    labs(y = NULL, x = NULL) +
    scale_colour_manual(values = c(&quot;#d62728&quot;, &quot;#2ca02c&quot;, &quot;#ff7f0e&quot;)) +
    theme_economist_white(base_size = 13) +
    theme(plot.background = element_rect(fill = &quot;white&quot;),
          axis.title.y = element_blank()) +
    legend_none()</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-36-1.png' %}" width="672" /></p>
<pre class="r"><code>gg_df = plot_df %&gt;%
  mutate(.fill_column = paste(noise_level, !(study_condition == &#39;BASELINE&#39;), sep = &quot;:&quot;)) %&gt;%
  count(user_id, study_condition, .fill_column, optimal_dx, num_actions, has_ax, has_dx, noise_level,
        name = &quot;num_correct_dx&quot;) %&gt;%
  mutate(estimate = -1) %&gt;%
  mutate(estimate = if_else(noise_level == &quot;Acc: 100%&quot;, inv_logit_scaled(eff[&quot;noise_level_f1&quot;,&quot;Estimate&quot;] + eff[&quot;Intercept&quot;, &quot;Estimate&quot;]), estimate)) %&gt;%
  mutate(estimate = if_else(noise_level == &quot;Acc: 90%&quot;, inv_logit_scaled(eff[&quot;noise_level_f2&quot;,&quot;Estimate&quot;] + eff[&quot;Intercept&quot;, &quot;Estimate&quot;]), estimate)) %&gt;%
  mutate(estimate = if_else(noise_level == &quot;Acc: 80%&quot;, inv_logit_scaled(eff[&quot;Intercept&quot;, &quot;Estimate&quot;] - eff[&quot;noise_level_f1&quot;,&quot;Estimate&quot;] - eff[&quot;noise_level_f2&quot;,&quot;Estimate&quot;]), estimate)) %&gt;%
  mutate(noise_level = fct_rev(noise_level))

gg_df %&gt;%
  filter(optimal_dx == &quot;1&quot;) %&gt;%
  ggplot(aes(noise_level, num_correct_dx / num_actions, group = noise_level, colour = noise_level)) +
    geom_hline(yintercept = inv_logit_scaled(eff[&quot;Intercept&quot;, &quot;Estimate&quot;]),
               size = .7, linetype = &quot;dashed&quot;, color = &quot;grey&quot;) +
    geom_count(aes(colour = NULL)) +
    geom_boxplot(aes(y = estimate)) +
    annotate(&quot;segment&quot;,
             x = 3,
             y = inv_logit_scaled(eff[&quot;noise_level_f1&quot;,&quot;Estimate&quot;] + eff[&quot;Intercept&quot;, &quot;Estimate&quot;]),
             xend = 2,
             yend = inv_logit_scaled(eff[&quot;noise_level_f2&quot;,&quot;Estimate&quot;] + eff[&quot;Intercept&quot;, &quot;Estimate&quot;]),
             colour = &quot;black&quot;) +
    annotate(&quot;segment&quot;,
             x = 1,
             y = inv_logit_scaled(eff[&quot;Intercept&quot;, &quot;Estimate&quot;] - eff[&quot;noise_level_f1&quot;,&quot;Estimate&quot;] - eff[&quot;noise_level_f2&quot;,&quot;Estimate&quot;]),
             xend = 2,
             yend = inv_logit_scaled(eff[&quot;noise_level_f2&quot;,&quot;Estimate&quot;] + eff[&quot;Intercept&quot;, &quot;Estimate&quot;]),
             color = &quot;black&quot;) +
    annotate(&quot;text&quot;,
             x = 3.5,
             y = inv_logit_scaled(eff[&quot;Intercept&quot;, &quot;Estimate&quot;] - 0.85),
             label = &quot;Mean of all levels&quot;,
             angle = 90,
             group = NA,
             colour = &quot;black&quot;) +
    scale_y_continuous(limits = c(0.0, 1.0), breaks = c(0.0, 0.5, 1.0)) +
    labs(y = &quot;Fraction of participants that resolved fault&quot;, x = NULL) +
    scale_colour_manual(values = c(&quot;#d62728&quot;, &quot;#2ca02c&quot;, &quot;#ff7f0e&quot;)) +
    scale_size_area(max_size = 1) +
    theme_economist_white(base_size = 13) +
    theme(plot.background = element_rect(fill = &quot;white&quot;),
          axis.title.y = element_blank()) +
    legend_none()</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-36-2.png' %}" width="672" /></p>
</div>
</div>
<div id="cax-compliance-with-ax-suggestions" class="section level1">
<h1>CAX: Compliance with AX Suggestions</h1>
<p><strong>Did the user follow the AX suggestions that were provided to them?</strong></p>
<p>In the code, the variable might be referred to as <code>chose_ax</code>, <code>follow_ax</code>, etc. depending on the version of the codebase</p>
<pre class="r"><code>plot_df = actions %&gt;%
  filter(has_ax == T) %&gt;%
  select(X1, id, user_id, study_condition, start_condition, num_optimal,
         state_idx, state_idx_rescaled,
         age_group, gender, robot_experience,
         noise_level, noise_level_f, has_noise, has_dx, has_ax,
         scenario_completed, num_actions, chose_ax)

text_short(report(plot_df))</code></pre>
<pre><code>## Warning in report.numeric(model[[col]], median = median, centrality =
## centrality, : Variable `model[[col]]` contains only two different values.
## Consider converting it to a factor.

## Warning in report.numeric(model[[col]], median = median, centrality =
## centrality, : Variable `model[[col]]` contains only two different values.
## Consider converting it to a factor.</code></pre>
<pre><code>## The data contains 1138 observations of the following variables:
##   - X1: Mean = 1080.22, SD = 583.33, range = [0, 2095], 0 missing
##   - id: Mean = 1085.37, SD = 578.89, range = [18, 2096], 0 missing
##   - user_id: Mean = 104.34, SD = 54.25, range = [1, 198], 0 missing
##   - study_condition: 10 levels: DX_100, n = 0; AX_100, n = 152; DXAX_100, n = 193; DX_90, n = 0; AX_90, n = 217; DXAX_90, n = 183; DX_80, n = 0; AX_80, n = 204; DXAX_80, n = 189 and BASELINE, n = 0
##   - start_condition: 4 entries: kc.dt.occluding.above_mug.default.empty.dt, n = 366; kc.kc.occluding.default.default.empty.dt, n = 342; dt.kc.default.default.default.empty.kc, n = 247 and 1 other
##   - num_optimal: Mean = 4.80, SD = 1.55, range = [3, 7], 0 missing
##   - state_idx: Mean = 82.94, SD = 58.02, range = [1, 171], 0 missing
##   - state_idx_rescaled: Mean = 0.48, SD = 0.34, range = [0, 1], 0 missing
##   - age_group: 7 levels: 2, n = 160; 3, n = 323; 4, n = 224; 5, n = 140; 6, n = 127; 7, n = 49 and 8, n = 115
##   - gender: 2 levels: F, n = 453 and M, n = 685
##   - robot_experience: 5 levels: 0, n = 665; 1, n = 260; 2, n = 55; 3, n = 89 and 4, n = 69
##   - noise_level: 3 levels: 0.0, n = 345; 1.0, n = 400 and 2.0, n = 393
##   - noise_level_f: 3 levels: 0.0, n = 345; 1.0, n = 400 and 2.0, n = 393
##   - has_noise: 2 levels: FALSE, n = 345 and TRUE, n = 793
##   - has_dx: 2 levels: FALSE, n = 573 and TRUE, n = 565
##   - has_ax: 2 levels: FALSE, n = 0 and TRUE, n = 1138
##   - scenario_completed: Mean = 0.79, SD = 0.41, range = [0, 1], 0 missing
##   - num_actions: Mean = 12.51, SD = 5.71, range = [3, 20], 0 missing
##   - chose_ax: Mean = 0.54, SD = 0.50, range = [0, 1], 0 missing</code></pre>
<div id="data-3" class="section level2">
<h2>Data</h2>
<pre class="r"><code>gg_df =
  plot_df %&gt;%
    mutate(noise_level = fct_rev(noise_level)) %&gt;%
    mutate(chose_ax = factor(chose_ax)) %&gt;%
    mutate(user_id = factor(user_id)) %&gt;%
    mutate(chose_ax = fct_recode(chose_ax, no_follow=&quot;0&quot;, follow = &quot;1&quot;)) %&gt;%
    mutate(.fill_column = paste(noise_level, !(study_condition == &#39;BASELINE&#39;), sep = &quot;:&quot;)) %&gt;%
    count(user_id, study_condition, .fill_column, chose_ax, num_actions, has_ax, has_dx, noise_level,
          name = &quot;num_follow_ax&quot;)

# Plot by the study condition
gg_df %&gt;%
  ggplot(aes(user_id, num_follow_ax / num_actions, fill=chose_ax)) +
    geom_bar(stat=&quot;identity&quot;) +
    geom_hline(yintercept = 0.5) +
    facet_wrap(vars(study_condition), nrow = 1, scales = &quot;free_y&quot;) +
    labs(y = &quot;Fraction followed AX&quot;) +
    coord_flip() +
    scale_fill_economist()</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-39-1.png' %}" width="1440" /></p>
<pre class="r"><code>gg_df %&gt;%
  filter(chose_ax == &quot;follow&quot;) %&gt;%
  ggplot(aes(study_condition, num_follow_ax / num_actions, fill=.fill_column)) +
    geom_violin() +
    geom_boxplot(width = 0.1) +
    labs(y = &quot;Fraction followed AX&quot;, fill = &quot;noise:has_suggestions&quot;) +
    scale_fill_economist()</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-39-2.png' %}" width="1440" /></p>
<pre class="r"><code># Visualize the data facetted by the variables that we care about
p2 = gg_df %&gt;%
  filter(chose_ax == &quot;follow&quot;) %&gt;%
  ggplot(aes(has_dx, num_follow_ax / num_actions, fill=.fill_column)) +
    geom_boxplot() +
    geom_count() +
    facet_grid(rows = vars(noise_level), labeller = label_both) +
    labs(y = &quot;Fraction followed AX&quot;) +
    scale_fill_economist() +
    theme(legend.position = &quot;right&quot;)

p3 = gg_df %&gt;%
  filter(chose_ax == &quot;follow&quot;) %&gt;%
  mutate(noise_level = fct_rev(noise_level)) %&gt;%
  ggplot(aes(noise_level, num_follow_ax / num_actions, fill=.fill_column)) +
    geom_boxplot() +
    geom_count() +
    facet_grid(cols = vars(has_dx), labeller = label_both) +
    labs(y = &quot;Fraction followed AX&quot;) +
    scale_fill_economist() +
    legend_none()

grid.arrange(p2, p3, ncol = 2)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-39-3.png' %}" width="1440" /></p>
<pre class="r"><code># Visualize also the variation in each of the three levels that we care about
p2 = gg_df %&gt;%
  filter(chose_ax == &#39;follow&#39;) %&gt;%
  ggplot(aes(has_dx, num_follow_ax/num_actions)) +
    geom_violin() +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep=&quot;:&quot;), size = 2)) +
    stat_summary(fun.y = median, geom=&quot;point&quot;, shape=23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    theme(legend.position = &quot;right&quot;)

p3 = gg_df %&gt;%
  filter(chose_ax == &#39;follow&#39;) %&gt;%
  ggplot(aes(noise_level, num_follow_ax/num_actions)) +
    geom_violin() +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep=&quot;:&quot;), size = 2)) +
    stat_summary(fun.y = median, geom=&quot;point&quot;, shape=23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    scale_x_discrete(limits = rev(levels(gg_df$noise_level))) +
    legend_none()

grid.arrange(p2, p3, ncol = 2)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-39-4.png' %}" width="1440" /></p>
</div>
<div id="model-3" class="section level2">
<h2>Model</h2>
<p>Based on the data, we assume the following structural model:</p>
<p><span class="math display">\[cax_{ij} = Bernoulli(p_{ij})\]</span> <span class="math display">\[\begin{aligned}
logit^{-1}(p_{ij}) &amp;= \beta_0 + \beta_{0i} + \beta_{dx}\text{dx}_i + \beta_{noise}\text{noise}_i + \beta_{dx:noise}\text{dx}_i\text{noise}_i + \\
&amp;\beta_{no}\text{no}_i + \mathbf{\beta_{demo}X_{demo,i}} + \beta_{state} \text{state}_{ij}
\end{aligned}\]</span> <span class="math display">\[\begin{aligned}
\beta_{.} &amp;\sim Normal(0, 10) \\
\beta_{0i} &amp;\sim Normal(0, \sigma_i) \\
\sigma_i &amp;\sim HalfStudent(3, 0, 10)
\end{aligned}\]</span></p>
<p>The prior for the <span class="math inline">\(\sigma_i\)</span> parameter is the default used in <code>brms</code>; I see no need to change it.</p>
<p>Model fitting:</p>
<pre class="r"><code># The null model
chose_ax.model.null = brm(
  chose_ax ~ 0 + Intercept + (1 | user_id),
  family = &quot;bernoulli&quot;,
  prior = c(set_prior(&quot;normal(0, 10)&quot;, class = &quot;b&quot;),
            set_prior(&quot;student_t(3, 0, 10)&quot;, class = &quot;sd&quot;)),
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
chose_ax.model.null = add_criterion(chose_ax.model.null, &quot;waic&quot;)
chose_ax.model.null = add_criterion(chose_ax.model.null, &quot;loo&quot;, reloo = T)
saveModel(chose_ax.model.null)

# The trend model to see if there is a trend in the noise level variable
chose_ax.model.t = brm(
  chose_ax ~ 0 + Intercept + (has_dx * noise_level) + (age_group + robot_experience + gender) + num_optimal + state_idx_rescaled + (1 | user_id),
  family = bernoulli,
  prior = c(set_prior(&quot;normal(0, 10)&quot;, class = &quot;b&quot;),
            set_prior(&quot;student_t(3, 0, 10)&quot;, class = &quot;sd&quot;)),
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
chose_ax.model.t = add_criterion(chose_ax.model.t, &quot;waic&quot;)
chose_ax.model.t = add_criterion(chose_ax.model.t, &quot;loo&quot;, reloo = T)
saveModel(chose_ax.model.t)

# The values model, to see if specific values of the noise level variable are significant
chose_ax.model.v = brm(
  chose_ax ~ 0 + Intercept + (has_dx * noise_level_f) + (age_group + robot_experience + gender) + num_optimal + state_idx_rescaled + (1 | user_id),
  family = bernoulli,
  prior = c(set_prior(&quot;normal(0, 10)&quot;, class = &quot;b&quot;),
            set_prior(&quot;student_t(3, 0, 10)&quot;, class = &quot;sd&quot;)),
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
chose_ax.model.v = add_criterion(chose_ax.model.v, &quot;waic&quot;)
chose_ax.model.v = add_criterion(chose_ax.model.v, &quot;loo&quot;, reloo = T)
saveModel(chose_ax.model.v)</code></pre>
<pre class="r"><code># Load the models
chose_ax.model.null = loadModel(&quot;chose_ax.model.null&quot;)
chose_ax.model.t = loadModel(&quot;chose_ax.model.t&quot;)
chose_ax.model.v = loadModel(&quot;chose_ax.model.v&quot;)</code></pre>
<p>Model fitting results:</p>
<pre class="r"><code># print(tidy_stan(chose_ax.model.null))
print(performance::r2(chose_ax.model.null))</code></pre>
<pre><code>## # Bayesian R2 with Standard Error
##
##   Conditional R2: 0.154 [0.020]
##      Marginal R2: 0.000 [0.000]</code></pre>
<pre class="r"><code># Print the parameters, and some initial diagnostics
print(tidy_stan(chose_ax.model.t, effects = &quot;fixed&quot;))</code></pre>
<pre><code>## Warning: &#39;tidy_stan&#39; is deprecated.
## Use &#39;parameters::model_parameters()&#39; instead.
## See help(&quot;Deprecated&quot;)</code></pre>
<pre><code>##
## Summary Statistics of Stan-Model
##
## # Fixed effects
##
##              Parameter Estimate Std.Error       HDI(89%)     pd  ESS Rhat MCSE
##            (Intercept)     0.59      0.47 [-0.11,  1.41]  90.1% 1036 1.00 0.01
##                has_dx1    -0.14      0.13 [-0.33,  0.08]  86.0% 1481 1.00 0.00
##          noise_level.L    -0.39      0.23 [-0.75, -0.01]  95.6% 1703 1.00 0.01
##          noise_level.Q     0.18      0.22 [-0.18,  0.54]  78.7% 1713 1.00 0.01
##             age_group1     0.12      0.33 [-0.40,  0.67]  65.8% 1475 1.00 0.01
##             age_group2    -0.24      0.26 [-0.65,  0.16]  82.4% 1411 1.00 0.01
##             age_group3    -0.07      0.29 [-0.54,  0.39]  58.9% 1275 1.00 0.01
##             age_group4     0.54      0.33 [ 0.02,  1.09]  94.8% 1483 1.00 0.01
##             age_group5    -0.78      0.40 [-1.48, -0.19]  97.0% 1262 1.00 0.01
##             age_group6     0.40      0.57 [-0.60,  1.31]  75.6% 1406 1.01 0.02
##      robot_experience1     0.62      0.25 [ 0.22,  1.04]  99.2% 1638 1.00 0.01
##      robot_experience2     0.38      0.31 [-0.15,  0.82]  88.4% 1528 1.00 0.01
##      robot_experience3     0.04      0.53 [-0.86,  0.86]  53.0% 1616 1.00 0.01
##      robot_experience4    -0.26      0.43 [-0.92,  0.48]  70.8% 1707 1.00 0.01
##                gender1    -0.21      0.13 [-0.44, -0.01]  94.5% 1753 1.00 0.00
##            num_optimal     0.06      0.08 [-0.07,  0.20]  76.4% 1084 1.00 0.00
##     state_idx_rescaled    -1.55      0.23 [-1.94, -1.21] 100.0% 3180 1.00 0.00
##  has_dx1.noise_level.L    -0.27      0.23 [-0.62,  0.12]  87.8% 1680 1.00 0.01
##  has_dx1.noise_level.Q     0.22      0.24 [-0.17,  0.60]  82.6% 1304 1.00 0.01</code></pre>
<pre class="r"><code>print(performance::r2(chose_ax.model.t))</code></pre>
<pre><code>## # Bayesian R2 with Standard Error
##
##   Conditional R2: 0.226 [0.018]
##      Marginal R2: 0.131 [0.021]</code></pre>
<pre class="r"><code>print(tidy_stan(chose_ax.model.v, effects = &quot;fixed&quot;))</code></pre>
<pre><code>## Warning: &#39;tidy_stan&#39; is deprecated.
## Use &#39;parameters::model_parameters()&#39; instead.
## See help(&quot;Deprecated&quot;)</code></pre>
<pre><code>##
## Summary Statistics of Stan-Model
##
## # Fixed effects
##
##               Parameter Estimate Std.Error       HDI(89%)     pd  ESS Rhat MCSE
##             (Intercept)     0.57      0.46 [-0.11,  1.37]  90.2% 1786 1.00 0.01
##                 has_dx1    -0.14      0.13 [-0.37,  0.06]  85.8% 2008 1.00 0.00
##          noise_level_f1     0.36      0.20 [ 0.05,  0.67]  96.8% 1846 1.00 0.00
##          noise_level_f2    -0.16      0.19 [-0.47,  0.15]  79.9% 1773 1.00 0.00
##              age_group1     0.13      0.34 [-0.40,  0.69]  65.0% 1827 1.00 0.01
##              age_group2    -0.24      0.26 [-0.64,  0.17]  81.8% 1833 1.00 0.01
##              age_group3    -0.06      0.29 [-0.53,  0.38]  57.8% 2040 1.00 0.01
##              age_group4     0.56      0.33 [-0.02,  1.01]  95.6% 1975 1.00 0.01
##              age_group5    -0.78      0.39 [-1.42, -0.13]  97.2% 1632 1.00 0.01
##              age_group6     0.35      0.58 [-0.56,  1.26]  73.5% 2087 1.00 0.01
##       robot_experience1     0.62      0.26 [ 0.22,  1.04]  99.5% 1769 1.00 0.01
##       robot_experience2     0.37      0.32 [-0.17,  0.86]  87.5% 1648 1.00 0.01
##       robot_experience3     0.03      0.54 [-0.83,  0.93]  52.4% 1847 1.00 0.01
##       robot_experience4    -0.24      0.44 [-0.91,  0.51]  69.5% 2314 1.00 0.01
##                 gender1    -0.22      0.14 [-0.43,  0.01]  93.9% 1991 1.00 0.00
##             num_optimal     0.06      0.08 [-0.06,  0.20]  77.6% 1864 1.00 0.00
##      state_idx_rescaled    -1.55      0.24 [-1.92, -1.18] 100.0% 3994 1.00 0.00
##  has_dx1:noise_level_f1     0.28      0.19 [-0.04,  0.58]  92.7% 2196 1.00 0.00
##  has_dx1:noise_level_f2    -0.19      0.18 [-0.48,  0.11]  85.2% 1953 1.00 0.00</code></pre>
<pre class="r"><code>print(performance::r2(chose_ax.model.v))</code></pre>
<pre><code>## # Bayesian R2 with Standard Error
##
##   Conditional R2: 0.226 [0.019]
##      Marginal R2: 0.132 [0.021]</code></pre>
</div>
<div id="diagnostic-3" class="section level2">
<h2>Diagnostic</h2>
<pre class="r"><code># Alternative diagnostics: diagnostic_posterior(scenario_completed.model.v)

# LOO cross-validation results
print(chose_ax.model.t$criteria$loo$estimates)</code></pre>
<pre><code>##            Estimate        SE
## elpd_loo -704.42143 14.263889
## p_loo      81.19384  2.449307
## looic    1408.84285 28.527779</code></pre>
<pre class="r"><code>print(chose_ax.model.v$criteria$loo$estimates)</code></pre>
<pre><code>##            Estimate        SE
## elpd_loo -703.73137 14.267155
## p_loo      80.60261  2.440302
## looic    1407.46274 28.534309</code></pre>
<pre class="r"><code>print(loo_compare(chose_ax.model.null, chose_ax.model.t, chose_ax.model.v))</code></pre>
<pre><code>##                     elpd_diff se_diff
## chose_ax.model.v      0.0       0.0
## chose_ax.model.t     -0.7       0.3
## chose_ax.model.null -25.3       7.8</code></pre>
<pre class="r"><code># Diagnostics of the sampling process
par(mfrow=c(1,2))
plot(chose_ax.model.t$criteria$loo, main = &quot;Trend Model&quot;)
plot(chose_ax.model.v$criteria$loo, main = &quot;Values Model&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-43-1.png' %}" width="1440" /></p>
<pre class="r"><code># Include posterior predictive checks of the sampling (are sampling according
# to the data that we actually have?)
p2 = pp_check(chose_ax.model.t, type = &quot;bars_grouped&quot;, group = &quot;has_dx&quot;) + ggtitle(&quot;t:has_dx&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars_grouped&#39; by default.</code></pre>
<pre class="r"><code>p3 = pp_check(chose_ax.model.t, type = &quot;bars_grouped&quot;, group = &quot;noise_level&quot;) + ggtitle(&quot;t:noise_level&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars_grouped&#39; by default.</code></pre>
<pre class="r"><code>grid.arrange(p2, p3, ncol = 2)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-43-2.png' %}" width="1440" /></p>
<pre class="r"><code>mcmc_intervals(as.matrix(chose_ax.model.t), regex_pars = &quot;b_|^shape&quot;, prob = 0.90, prob_outer = 0.95) +
  ggtitle(&quot;Trends Model&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-43-3.png' %}" width="1440" /></p>
<pre class="r"><code>p2 = pp_check(chose_ax.model.v, type = &quot;bars_grouped&quot;, group = &quot;has_dx&quot;) + ggtitle(&quot;v:has_dx&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars_grouped&#39; by default.</code></pre>
<pre class="r"><code>p3 = pp_check(chose_ax.model.v, type = &quot;bars_grouped&quot;, group = &quot;noise_level_f&quot;) + ggtitle(&quot;v:noise_level&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars_grouped&#39; by default.</code></pre>
<pre class="r"><code>grid.arrange(p2, p3, nrow = 2)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-43-4.png' %}" width="1440" /></p>
<pre class="r"><code>mcmc_intervals(as.matrix(chose_ax.model.v), regex_pars = &quot;b_|^shape&quot;, prob = 0.90, prob_outer = 0.95) +
  ggtitle(&quot;Values Model&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-43-5.png' %}" width="1440" /></p>
<pre class="r"><code># Plot predictions vs original
preds_p_df = chose_ax.model.null %&gt;% predict() %&gt;% as_tibble()
p1 =
  preds_p_df %&gt;%
    add_column(chose_ax = chose_ax.model.null$data$chose_ax) %&gt;%
    ggplot(aes(x = Estimate, y = chose_ax, color = chose_ax)) +
      geom_point() +
      labs(x = &quot;Predicted values&quot;) +
      ylim(0, 1) +
      xlim(0, 1) +
      ggtitle(&quot;Null Model&quot;)

preds_p_df = plot_df %&gt;%
  add_predicted_draws(chose_ax.model.null) %&gt;%
  group_by(chose_ax, .prediction) %&gt;%
  count()
p2 =
  preds_p_df %&gt;%
    ggplot(aes(x = chose_ax, y = .prediction, fill = n / sum(preds_p_df$n))) +
      geom_tile() +
      geom_text(aes(label = sprintf(&quot;%1.2f&quot;, n / sum(preds_p_df$n))), size = 10, vjust = 1) +
      scale_fill_gradient(limits = c(0, 1))

preds_p_df = chose_ax.model.t %&gt;% predict() %&gt;% as_tibble()
p3 =
  preds_p_df %&gt;%
    add_column(chose_ax = chose_ax.model.t$data$chose_ax) %&gt;%
    ggplot(aes(x = Estimate, y = chose_ax, color = chose_ax)) +
      geom_point() +
      labs(x = &quot;Predicted values&quot;) +
      ylim(0, 1) +
      xlim(0, 1) +
      ggtitle(&quot;Trends Model&quot;)

preds_p_df = plot_df %&gt;%
  add_predicted_draws(chose_ax.model.t) %&gt;%
  group_by(chose_ax, .prediction) %&gt;%
  count()
p4 =
  preds_p_df %&gt;%
    ggplot(aes(x = chose_ax, y = .prediction, fill = n / sum(preds_p_df$n))) +
      geom_tile() +
      geom_text(aes(label = sprintf(&quot;%1.2f&quot;, n / sum(preds_p_df$n))), size = 10, vjust = 1) +
      scale_fill_gradient(limits = c(0, 1))

# Plot predictions vs original
preds_p_df = chose_ax.model.v %&gt;% predict() %&gt;% as_tibble()
p5 =
  preds_p_df %&gt;%
    add_column(chose_ax = chose_ax.model.v$data$chose_ax) %&gt;%
    ggplot(aes(x = Estimate, y = chose_ax, color = chose_ax)) +
      geom_point() +
      labs(x = &quot;Predicted values&quot;) +
      ylim(0, 1) +
      xlim(0, 1) +
      ggtitle(&quot;Values Model&quot;)

preds_p_df = plot_df %&gt;%
  add_predicted_draws(chose_ax.model.v) %&gt;%
  group_by(chose_ax, .prediction) %&gt;%
  count()
p6 =
  preds_p_df %&gt;%
    ggplot(aes(x = chose_ax, y = .prediction, fill = n / sum(preds_p_df$n))) +
      geom_tile() +
      geom_text(aes(label = sprintf(&quot;%1.2f&quot;, n / sum(preds_p_df$n))), size = 10, vjust = 1) +
      scale_fill_gradient(limits = c(0, 1))

grid.arrange(p1, p3, p5, p2, p4, p6, nrow = 2, ncol = 3)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-43-6.png' %}" width="1440" /></p>
</div>
<div id="inference-3" class="section level2">
<h2>Inference</h2>
<pre class="r"><code># Compute the bayes factor for each parameter value. Note: we compare
# here against the ROPE, and not the degenerate null hypothesis of 0
# Bayes-Factor values would be more significant in the latter case
# According to the Raftery, 1995 rules
# bf = 1 - 3: Weak
# bf = 3 - 20: Positive
# bf = 20 - 150: Strong
# bf &gt; 150: Very strong
print(bayesfactor_rope(chose_ax.model.null, null = c(-0.055, 0.055)), n = 30)
print(bayesfactor_rope(chose_ax.model.t, null = c(-0.055, 0.055)), n = 30)
print(bayesfactor_rope(chose_ax.model.v, null = c(-0.055, 0.055)), n = 30)</code></pre>
<pre class="r"><code>options(digits = 3)
# Check the significance of each hypothesis
h_df = as_tibble(insight::get_parameters(chose_ax.model.t)) %&gt;%
  transmute(
    Intercept = b_Intercept,
    has_dx_test = -2 * b_has_dx1,
    noise_levelL_test = b_noise_level.L,
    noise_levelQ_test = b_noise_level.Q
  )

hyp_results = test_hypotheses(h_df, rope_values = c(-0.055, 0.055))
print(hyp_results %&gt;% select(Parameter, pd, HDI_low, HDI_high, ROPE_Percentage, ROPE_Equivalence) %&gt;% as.matrix())</code></pre>
<pre><code>##      Parameter           pd      HDI_low  HDI_high ROPE_Percentage
## [1,] &quot;Intercept&quot;         &quot;0.901&quot; &quot;-1.132&quot; &quot;2.24&quot;   &quot;0.0415&quot;
## [2,] &quot;has_dx_test&quot;       &quot;0.860&quot; &quot;-0.796&quot; &quot;1.28&quot;   &quot;0.0965&quot;
## [3,] &quot;noise_levelL_test&quot; &quot;0.956&quot; &quot;-1.248&quot; &quot;0.43&quot;   &quot;0.0498&quot;
## [4,] &quot;noise_levelQ_test&quot; &quot;0.787&quot; &quot;-0.597&quot; &quot;1.01&quot;   &quot;0.1415&quot;
##      ROPE_Equivalence
## [1,] &quot;Undecided&quot;
## [2,] &quot;Undecided&quot;
## [3,] &quot;Undecided&quot;
## [4,] &quot;Undecided&quot;</code></pre>
<pre class="r"><code>h_df = as_tibble(insight::get_parameters(chose_ax.model.v)) %&gt;%
  transmute(
    Intercept = b_Intercept,
    has_dx_test = -2 * b_has_dx1,
    noise_level1_test = b_noise_level_f1,
    noise_level2_test = b_noise_level_f2,
    noise_level3_test = - b_noise_level_f1 - b_noise_level_f2
  )

hyp_results = test_hypotheses(h_df, rope_values = c(-0.055, 0.055))
print(hyp_results %&gt;% select(Parameter, pd, HDI_low, HDI_high, ROPE_Percentage, ROPE_Equivalence) %&gt;% as.matrix())</code></pre>
<pre><code>##      Parameter           pd      HDI_low  HDI_high ROPE_Percentage
## [1,] &quot;Intercept&quot;         &quot;0.902&quot; &quot;-0.853&quot; &quot;2.244&quot;  &quot;0.0385&quot;
## [2,] &quot;has_dx_test&quot;       &quot;0.858&quot; &quot;-0.836&quot; &quot;1.369&quot;  &quot;0.0885&quot;
## [3,] &quot;noise_level1_test&quot; &quot;0.968&quot; &quot;-0.287&quot; &quot;1.140&quot;  &quot;0.0408&quot;
## [4,] &quot;noise_level2_test&quot; &quot;0.799&quot; &quot;-0.864&quot; &quot;0.525&quot;  &quot;0.1565&quot;
## [5,] &quot;noise_level3_test&quot; &quot;0.883&quot; &quot;-0.777&quot; &quot;0.560&quot;  &quot;0.1308&quot;
##      ROPE_Equivalence
## [1,] &quot;Undecided&quot;
## [2,] &quot;Undecided&quot;
## [3,] &quot;Undecided&quot;
## [4,] &quot;Undecided&quot;
## [5,] &quot;Undecided&quot;</code></pre>
<pre class="r"><code>options(digits = 7)</code></pre>
<p><strong>Results</strong>:</p>
<ol style="list-style-type: decimal">
<li>Noise Level Linear
<ul>
<li>Negative (negative slope) pd = 95.6%</li>
<li>Median = -0.39, 89% CI [-0.75, -0.01]</li>
<li>Small, Std.Median = 0.23</li>
<li>4.98% in ROPE (n.s.)</li>
</ul></li>
</ol>
<p>(The ROPE is defined as [-0.055, 0.055]. It is the range is suggested in the literature for logistic models and it corresponds to a probability range of 0.11. Therefore, a probability change of less than 0.055 is considered no different from a probability change of 0)</p>
</div>
<div id="posterior-plots-3" class="section level2">
<h2>Posterior Plots</h2>
<pre class="r"><code># A visualization data frame
gg_df = plot_df %&gt;%
    mutate(noise_level = fct_rev(noise_level)) %&gt;%
    mutate(chose_ax = factor(chose_ax)) %&gt;%
    mutate(user_id = factor(user_id)) %&gt;%
    mutate(chose_ax = fct_recode(chose_ax, no_follow=&quot;0&quot;, follow = &quot;1&quot;)) %&gt;%
    mutate(.fill_column = paste(noise_level, !(study_condition == &#39;BASELINE&#39;), sep = &quot;:&quot;)) %&gt;%
    count(user_id, study_condition, .fill_column, chose_ax, num_actions, has_ax, has_dx, noise_level,
          name = &quot;num_follow_ax&quot;)

# Simulate predictions on new data
p_df =
  plot_df %&gt;%
  data_grid(chose_ax,
            has_dx,
            noise_level_f,
            gender,
            age_group,
            robot_experience,
            num_optimal,
            state_idx_rescaled = seq_range(state_idx_rescaled, n = 4),
            .model = chose_ax.model.v)

fits_p_df = p_df %&gt;% add_fitted_draws(chose_ax.model.v,
                                      re_formula = NA,
                                      seed = default_seed)
gc()</code></pre>
<pre><code>##             used   (Mb) gc trigger   (Mb)   max used    (Mb)
## Ncells   4492287  240.0    9278889  495.6   11598611   619.5
## Vcells 368928394 2814.8  589368862 4496.6 1458218023 11125.4</code></pre>
<pre class="r"><code># We don&#39;t want predicted draws because that&#39;s a raw count of ones and zeros; not the
# probability distribution
# preds_p_df = p_df %&gt;% add_predicted_draws(scenario_completed.model.v)

# Plot the posterior distributions
rope_value = gg_df %&gt;%
  filter(chose_ax == &#39;follow&#39;, has_dx == F) %&gt;%
  summarise(.value = median(num_follow_ax / num_actions)) %&gt;%
  pull(.value)
gg_df %&gt;%
  filter(chose_ax == &#39;follow&#39;) %&gt;%
  ggplot(aes(x = has_dx, y = num_follow_ax / num_actions)) +
    geom_rect(ymin = rope_value-0.055, ymax = rope_value+0.055,
              xmin = -Inf, xmax = Inf,
              color = &quot;transparent&quot;, fill = &quot;white&quot;) +
    annotate(&quot;text&quot;, x = 0.5, y = rope_value, label = &quot;ROPE&quot;, angle = 90) +
    stat_halfeye(aes(y = .value), data = fits_p_df, position = position_nudge(x = 0.1)) +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep = &quot;:&quot;)), size = 2) +
    stat_summary(fun.y = median, geom = &quot;point&quot;, shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(y = &quot;follow_ax&quot;, color = &quot;AX:DX:Noise&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-46-1.png' %}" width="672" /></p>
<pre class="r"><code>rope_value = gg_df %&gt;%
  filter(chose_ax == &#39;follow&#39;, noise_level == &quot;0.0&quot;) %&gt;%
  summarise(.value = median(num_follow_ax / num_actions)) %&gt;%
  pull(.value)
gg_df %&gt;%
  filter(chose_ax == &#39;follow&#39;) %&gt;%
  mutate(noise_level = fct_rev(noise_level)) %&gt;%
  ggplot(aes(x = noise_level, y = num_follow_ax / num_actions)) +
    geom_rect(ymin = rope_value-0.055, ymax = rope_value+0.055,
              xmin = -Inf, xmax = Inf,
              color = &quot;transparent&quot;, fill = &quot;white&quot;) +
    annotate(&quot;text&quot;, x = 0.5, y = rope_value, label = &quot;ROPE&quot;, angle = 90) +
    stat_halfeye(aes(y = .value, x = noise_level_f), data = fits_p_df, position = position_nudge(x = 0.1)) +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep = &quot;:&quot;)), size = 2) +
    stat_summary(fun.y = median, geom = &quot;point&quot;, shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(y = &quot;follow_ax&quot;, color = &quot;AX:DX:Noise&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-46-2.png' %}" width="672" /></p>
<pre class="r"><code># Remove the giant fits data frames
rm(fits_p_df, p_df, gg_df)</code></pre>
<pre class="r"><code>plot_df = actions %&gt;%
  filter(has_ax == T) %&gt;%
  select(X1, id, user_id, study_condition, start_condition, num_optimal,
         state_idx, state_idx_rescaled,
         age_group, gender, robot_experience,
         noise_level, noise_level_f, has_noise, has_dx, has_ax,
         scenario_completed, num_actions, chose_ax) %&gt;%
  mutate(scenario_completed = factor(scenario_completed)) %&gt;%
  mutate(scenario_completed = fct_recode(scenario_completed, Unresolved=&quot;0&quot;, Resolved = &quot;1&quot;),
         has_dx = fct_recode(has_dx, &quot;No DX&quot;=&quot;FALSE&quot;, &quot;DX&quot;=&quot;TRUE&quot;),
         noise_level = fct_recode(noise_level, &quot;Acc: 100%&quot;=&quot;0.0&quot;, &quot;Acc: 90%&quot;=&quot;1.0&quot;, &quot;Acc: 80%&quot;=&quot;2.0&quot;))

eff = fixef(chose_ax.model.v)

gg_df = plot_df %&gt;%
  mutate(.fill_column = paste(noise_level, !(study_condition == &#39;BASELINE&#39;), sep = &quot;:&quot;)) %&gt;%
  count(user_id, study_condition, .fill_column, chose_ax, num_actions, has_dx, noise_level,
        name = &quot;num_chose_ax&quot;) %&gt;%
  mutate(estimate = -1) %&gt;%
  mutate(estimate = if_else(noise_level == &quot;Acc: 100%&quot;, inv_logit_scaled(eff[&quot;noise_level_f1&quot;,&quot;Estimate&quot;] + eff[&quot;Intercept&quot;, &quot;Estimate&quot;]), estimate)) %&gt;%
  mutate(estimate = if_else(noise_level == &quot;Acc: 90%&quot;, inv_logit_scaled(eff[&quot;noise_level_f2&quot;,&quot;Estimate&quot;] + eff[&quot;Intercept&quot;, &quot;Estimate&quot;]), estimate)) %&gt;%
  mutate(estimate = if_else(noise_level == &quot;Acc: 80%&quot;, inv_logit_scaled(eff[&quot;Intercept&quot;, &quot;Estimate&quot;] - eff[&quot;noise_level_f1&quot;,&quot;Estimate&quot;] - eff[&quot;noise_level_f2&quot;,&quot;Estimate&quot;]), estimate)) %&gt;%
  mutate(noise_level = fct_rev(noise_level))

gg_df %&gt;%
  filter(chose_ax == &quot;1&quot;) %&gt;%
  ggplot(aes(noise_level, num_chose_ax / num_actions, group = noise_level, colour = noise_level)) +
    geom_hline(yintercept = inv_logit_scaled(eff[&quot;Intercept&quot;, &quot;Estimate&quot;]),
               size = .7, linetype = &quot;dashed&quot;, color = &quot;grey&quot;) +
    geom_count(aes(colour = NULL)) +
    geom_boxplot(aes(y = estimate)) +
    annotate(&quot;segment&quot;,
             x = 3,
             y = inv_logit_scaled(eff[&quot;noise_level_f1&quot;,&quot;Estimate&quot;] + eff[&quot;Intercept&quot;, &quot;Estimate&quot;]),
             xend = 2,
             yend = inv_logit_scaled(eff[&quot;noise_level_f2&quot;,&quot;Estimate&quot;] + eff[&quot;Intercept&quot;, &quot;Estimate&quot;]),
             colour = &quot;black&quot;) +
    annotate(&quot;segment&quot;,
             x = 1,
             y = inv_logit_scaled(eff[&quot;Intercept&quot;, &quot;Estimate&quot;] - eff[&quot;noise_level_f1&quot;,&quot;Estimate&quot;] - eff[&quot;noise_level_f2&quot;,&quot;Estimate&quot;]),
             xend = 2,
             yend = inv_logit_scaled(eff[&quot;noise_level_f2&quot;,&quot;Estimate&quot;] + eff[&quot;Intercept&quot;, &quot;Estimate&quot;]),
             color = &quot;black&quot;) +
    annotate(&quot;text&quot;,
             x = 3.5,
             y = inv_logit_scaled(eff[&quot;Intercept&quot;, &quot;Estimate&quot;] - 0.25),
             label = &quot;Mean of all levels&quot;,
             angle = 90,
             group = NA,
             colour = &quot;black&quot;) +
    scale_y_continuous(limits = c(0.0, 1.0), breaks = c(0.0, 0.5, 1.0)) +
    labs(y = &quot;Fraction of participants that resolved fault&quot;, x = NULL) +
    scale_colour_manual(values = c(&quot;#d62728&quot;, &quot;#2ca02c&quot;, &quot;#ff7f0e&quot;)) +
    scale_size_area(max_size = 1) +
    theme_economist_white(base_size = 13) +
    theme(plot.background = element_rect(fill = &quot;white&quot;),
          axis.title.y = element_blank()) +
    legend_none()</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-47-1.png' %}" width="672" /></p>
</div>
</div>
<div id="cdx-compliance-with-dx-suggestions" class="section level1">
<h1>CDX: Compliance with DX Suggestions</h1>
<p><strong>Did the user take an follow the DX suggestions that were provided to them?</strong></p>
<p>In the code, the variable might be referred to as <code>chose_dx</code>, <code>follow_dx</code>, etc. depending on the version of the codebase</p>
<pre class="r"><code>plot_df = actions %&gt;%
  filter(has_dx == T) %&gt;%
  select(X1, id, user_id, study_condition, start_condition, num_optimal,
         state_idx, state_idx_rescaled,
         age_group, gender, robot_experience,
         noise_level, noise_level_f, has_noise, has_dx, has_ax,
         scenario_completed, num_actions, chose_dx)

text_short(report(plot_df))</code></pre>
<pre><code>## Warning in report.numeric(model[[col]], median = median, centrality =
## centrality, : Variable `model[[col]]` contains only two different values.
## Consider converting it to a factor.

## Warning in report.numeric(model[[col]], median = median, centrality =
## centrality, : Variable `model[[col]]` contains only two different values.
## Consider converting it to a factor.</code></pre>
<pre><code>## The data contains 1322 observations of the following variables:
##   - X1: Mean = 1137.18, SD = 633.99, range = [0, 2125], 0 missing
##   - id: Mean = 1138.91, SD = 634.68, range = [1, 2126], 0 missing
##   - user_id: Mean = 109.46, SD = 59.09, range = [1, 200], 0 missing
##   - study_condition: 10 levels: DX_100, n = 247; AX_100, n = 0; DXAX_100, n = 193; DX_90, n = 254; AX_90, n = 0; DXAX_90, n = 183; DX_80, n = 256; AX_80, n = 0; DXAX_80, n = 189 and BASELINE, n = 0
##   - start_condition: 4 entries: kc.dt.occluding.above_mug.default.empty.dt, n = 402; kc.kc.occluding.default.default.empty.dt, n = 349; dt.kc.default.default.default.empty.kc, n = 317 and 1 other
##   - num_optimal: Mean = 4.72, SD = 1.55, range = [3, 7], 0 missing
##   - state_idx: Mean = 89.96, SD = 59.50, range = [1, 171], 0 missing
##   - state_idx_rescaled: Mean = 0.52, SD = 0.35, range = [0, 1], 0 missing
##   - age_group: 7 levels: 2, n = 112; 3, n = 318; 4, n = 321; 5, n = 169; 6, n = 137; 7, n = 87 and 8, n = 178
##   - gender: 2 levels: F, n = 544 and M, n = 778
##   - robot_experience: 5 levels: 0, n = 840; 1, n = 190; 2, n = 107; 3, n = 69 and 4, n = 116
##   - noise_level: 3 levels: 0.0, n = 440; 1.0, n = 437 and 2.0, n = 445
##   - noise_level_f: 3 levels: 0.0, n = 440; 1.0, n = 437 and 2.0, n = 445
##   - has_noise: 2 levels: FALSE, n = 440 and TRUE, n = 882
##   - has_dx: 2 levels: FALSE, n = 0 and TRUE, n = 1322
##   - has_ax: 2 levels: FALSE, n = 757 and TRUE, n = 565
##   - scenario_completed: Mean = 0.58, SD = 0.49, range = [0, 1], 0 missing
##   - num_actions: Mean = 14.20, SD = 5.87, range = [3, 20], 0 missing
##   - chose_dx: Mean = 0.65, SD = 0.48, range = [0, 1], 0 missing</code></pre>
<div id="data-4" class="section level2">
<h2>Data</h2>
<pre class="r"><code>gg_df =
  plot_df %&gt;%
    mutate(noise_level = fct_rev(noise_level)) %&gt;%
    mutate(chose_dx = factor(chose_dx)) %&gt;%
    mutate(user_id = factor(user_id)) %&gt;%
    mutate(chose_dx = fct_recode(chose_dx, no_follow=&quot;0&quot;, follow = &quot;1&quot;)) %&gt;%
    mutate(.fill_column = paste(noise_level, !(study_condition == &#39;BASELINE&#39;), sep = &quot;:&quot;)) %&gt;%
    count(user_id, study_condition, .fill_column, chose_dx, num_actions, has_ax, has_dx, noise_level,
          name = &quot;num_follow_dx&quot;)

# Plot by the study condition
gg_df %&gt;%
  ggplot(aes(user_id, num_follow_dx / num_actions, fill=chose_dx)) +
    geom_bar(stat=&quot;identity&quot;) +
    geom_hline(yintercept = 0.5) +
    facet_wrap(vars(study_condition), nrow = 1, scales = &quot;free_y&quot;) +
    labs(y = &quot;Fraction followed DX&quot;) +
    coord_flip() +
    scale_fill_economist()</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-50-1.png' %}" width="1440" /></p>
<pre class="r"><code>gg_df %&gt;%
  filter(chose_dx == &quot;follow&quot;) %&gt;%
  ggplot(aes(study_condition, num_follow_dx / num_actions, fill=.fill_column)) +
    geom_violin() +
    geom_boxplot(width = 0.1) +
    labs(y = &quot;Fraction followed DX&quot;, fill = &quot;noise:has_suggestions&quot;) +
    scale_fill_economist()</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-50-2.png' %}" width="1440" /></p>
<pre class="r"><code># Visualize the data facetted by the variables that we care about
p1 = gg_df %&gt;%
  filter(chose_dx == &quot;follow&quot;) %&gt;%
  ggplot(aes(has_ax, num_follow_dx / num_actions, fill=.fill_column)) +
    geom_boxplot() +
    geom_count() +
    facet_grid(rows = vars(noise_level), labeller = label_both) +
    labs(y = &quot;Fraction followed DX&quot;) +
    scale_fill_economist() +
    theme(legend.position = &quot;right&quot;)

p3 = gg_df %&gt;%
  filter(chose_dx == &quot;follow&quot;) %&gt;%
  mutate(noise_level = fct_rev(noise_level)) %&gt;%
  ggplot(aes(noise_level, num_follow_dx / num_actions, fill=.fill_column)) +
    geom_boxplot() +
    geom_count() +
    facet_grid(cols = vars(has_ax), labeller = label_both) +
    labs(y = &quot;Fraction followed DX&quot;) +
    scale_fill_economist() +
    legend_none()

grid.arrange(p1, p3, ncol = 2)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-50-3.png' %}" width="1440" /></p>
<pre class="r"><code># Visualize also the variation in each of the three levels that we care about
p1 = gg_df %&gt;%
  filter(chose_dx == &#39;follow&#39;) %&gt;%
  ggplot(aes(has_ax, num_follow_dx/num_actions)) +
    geom_violin() +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep=&quot;:&quot;), size = 2)) +
    stat_summary(fun.y = median, geom=&quot;point&quot;, shape=23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    theme(legend.position = &quot;right&quot;)

p3 = gg_df %&gt;%
  filter(chose_dx == &#39;follow&#39;) %&gt;%
  ggplot(aes(noise_level, num_follow_dx/num_actions)) +
    geom_violin() +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep=&quot;:&quot;), size = 2)) +
    stat_summary(fun.y = median, geom=&quot;point&quot;, shape=23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    scale_x_discrete(limits = rev(levels(gg_df$noise_level))) +
    legend_none()

grid.arrange(p1, p3, ncol = 2)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-50-4.png' %}" width="1440" /></p>
</div>
<div id="model-4" class="section level2">
<h2>Model</h2>
<p>Based on the data, we assume the following structural model:</p>
<p><span class="math display">\[cdx_{ij} = Bernoulli(p_{ij})\]</span> <span class="math display">\[\begin{aligned}
logit^{-1}(p_{ij}) &amp;= \beta_0 + \beta_{0i} + \beta_{ax}\text{ax}_i + \beta_{noise}\text{noise}_i + \beta_{ax:noise}\text{ax}_i\text{noise}_i + \\
&amp;\beta_{no}\text{no}_i + \mathbf{\beta_{demo}X_{demo,i}} + \beta_{state} \text{state}_{ij}
\end{aligned}\]</span> <span class="math display">\[\begin{aligned}
\beta_{.} &amp;\sim Normal(0, 10) \\
\beta_{0i} &amp;\sim Normal(0, \sigma_i) \\
\sigma_i &amp;\sim HalfStudent(3, 0, 10)
\end{aligned}\]</span></p>
<p>The prior for the <span class="math inline">\(\sigma_i\)</span> parameter is the default used in <code>brms</code>; I see no need to change it.</p>
<p>Model fitting:</p>
<pre class="r"><code># The null model
chose_dx.model.null = brm(
  chose_dx ~ 0 + Intercept + (1 | user_id),
  family = &quot;bernoulli&quot;,
  prior = set_prior(&quot;normal(0, 10)&quot;, class = &quot;b&quot;),
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
chose_dx.model.null = add_criterion(chose_dx.model.null, &quot;waic&quot;)
chose_dx.model.null = add_criterion(chose_dx.model.null, &quot;loo&quot;, reloo = T)
saveModel(chose_dx.model.null)

# The trend model to see if there is a trend in the noise level variable
chose_dx.model.t = brm(
  chose_dx ~ 0 + Intercept + (has_ax * noise_level) + (age_group + robot_experience + gender) + num_optimal + state_idx_rescaled + (1 | user_id),
  family = bernoulli,
  prior = c(set_prior(&quot;normal(0, 10)&quot;, class = &quot;b&quot;),
            set_prior(&quot;student_t(3, 0, 10)&quot;, class = &quot;sd&quot;)),
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
chose_dx.model.t = add_criterion(chose_dx.model.t, &quot;waic&quot;)
chose_dx.model.t = add_criterion(chose_dx.model.t, &quot;loo&quot;, reloo = T)
saveModel(chose_dx.model.t)

# The values model, to see if specific values of the noise level variable are significant
chose_dx.model.v = brm(
  chose_dx ~ 0 + Intercept + (has_ax * noise_level_f) + (age_group + robot_experience + gender) + num_optimal + state_idx_rescaled + (1 | user_id),
  family = bernoulli,
  prior = c(set_prior(&quot;normal(0, 10)&quot;, class = &quot;b&quot;),
            set_prior(&quot;student_t(3, 0, 10)&quot;, class = &quot;sd&quot;)),
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
chose_dx.model.v = add_criterion(chose_dx.model.v, &quot;waic&quot;)
chose_dx.model.v = add_criterion(chose_dx.model.v, &quot;loo&quot;, reloo = T)
saveModel(chose_dx.model.v)</code></pre>
<pre class="r"><code># Load the models
chose_dx.model.null = loadModel(&quot;chose_dx.model.null&quot;)
chose_dx.model.t = loadModel(&quot;chose_dx.model.t&quot;)
chose_dx.model.v = loadModel(&quot;chose_dx.model.v&quot;)</code></pre>
<p>Model fitting results:</p>
<pre class="r"><code># print(tidy_stan(chose_ax.model.null))
print(performance::r2(chose_dx.model.null))</code></pre>
<pre><code>## # Bayesian R2 with Standard Error
##
##   Conditional R2: 0.244 [0.018]
##      Marginal R2: 0.000 [0.000]</code></pre>
<pre class="r"><code># Print the parameters, and some initial diagnostics
print(tidy_stan(chose_dx.model.t, effects = &quot;fixed&quot;))</code></pre>
<pre><code>## Warning: &#39;tidy_stan&#39; is deprecated.
## Use &#39;parameters::model_parameters()&#39; instead.
## See help(&quot;Deprecated&quot;)</code></pre>
<pre><code>##
## Summary Statistics of Stan-Model
##
## # Fixed effects
##
##              Parameter Estimate Std.Error       HDI(89%)    pd  ESS Rhat MCSE
##            (Intercept)     0.83      0.57 [-0.00,  1.79] 93.8% 2340 1.00 0.01
##                has_ax1    -0.02      0.16 [-0.27,  0.23] 55.0% 2588 1.00 0.00
##          noise_level.L    -0.68      0.27 [-1.09, -0.22] 99.4% 2659 1.00 0.01
##          noise_level.Q     0.06      0.26 [-0.37,  0.51] 58.5% 2444 1.00 0.01
##             age_group1     0.04      0.51 [-0.73,  0.89] 53.3% 2746 1.00 0.01
##             age_group2    -0.14      0.32 [-0.67,  0.40] 66.9% 2597 1.00 0.01
##             age_group3    -0.43      0.33 [-0.95,  0.09] 90.3% 2121 1.00 0.01
##             age_group4     0.19      0.38 [-0.42,  0.80] 69.9% 2761 1.00 0.01
##             age_group5    -0.22      0.43 [-0.98,  0.44] 69.7% 2718 1.00 0.01
##             age_group6     0.97      0.60 [ 0.07,  1.95] 95.5% 3129 1.00 0.01
##      robot_experience1     0.40      0.28 [-0.06,  0.87] 91.8% 2576 1.00 0.01
##      robot_experience2    -0.18      0.38 [-0.76,  0.44] 67.8% 2785 1.00 0.01
##      robot_experience3    -0.69      0.56 [-1.59,  0.18] 89.0% 2593 1.00 0.01
##      robot_experience4     0.79      0.59 [-0.20,  1.70] 90.9% 2735 1.00 0.01
##                gender1     0.00      0.16 [-0.28,  0.25] 51.2% 2600 1.00 0.00
##            num_optimal     0.01      0.10 [-0.15,  0.17] 53.2% 2606 1.00 0.00
##     state_idx_rescaled     0.35      0.22 [ 0.00,  0.73] 93.7% 4355 1.00 0.00
##  has_ax1.noise_level.L    -0.31      0.27 [-0.74,  0.14] 87.3% 2345 1.00 0.01
##  has_ax1.noise_level.Q     0.03      0.28 [-0.38,  0.50] 54.4% 2775 1.00 0.01</code></pre>
<pre class="r"><code>print(performance::r2(chose_dx.model.t))</code></pre>
<pre><code>## # Bayesian R2 with Standard Error
##
##   Conditional R2: 0.262 [0.016]
##      Marginal R2: 0.108 [0.027]</code></pre>
<pre class="r"><code>print(tidy_stan(chose_dx.model.v, effects = &quot;fixed&quot;))</code></pre>
<pre><code>## Warning: &#39;tidy_stan&#39; is deprecated.
## Use &#39;parameters::model_parameters()&#39; instead.
## See help(&quot;Deprecated&quot;)</code></pre>
<pre><code>##
## Summary Statistics of Stan-Model
##
## # Fixed effects
##
##               Parameter Estimate Std.Error      HDI(89%)    pd  ESS Rhat MCSE
##             (Intercept)     0.83      0.55 [-0.08, 1.72] 93.7% 2573 1.00 0.01
##                 has_ax1    -0.01      0.17 [-0.29, 0.24] 53.8% 2877 1.00 0.00
##          noise_level_f1     0.50      0.22 [ 0.15, 0.85] 99.0% 2780 1.00 0.00
##          noise_level_f2    -0.05      0.22 [-0.38, 0.31] 58.5% 2927 1.00 0.00
##              age_group1     0.05      0.50 [-0.77, 0.88] 54.1% 3057 1.00 0.01
##              age_group2    -0.14      0.32 [-0.65, 0.35] 66.9% 3376 1.00 0.01
##              age_group3    -0.42      0.32 [-0.93, 0.11] 89.8% 3026 1.00 0.01
##              age_group4     0.17      0.37 [-0.39, 0.79] 68.4% 3173 1.00 0.01
##              age_group5    -0.24      0.43 [-0.94, 0.48] 71.2% 3425 1.00 0.01
##              age_group6     0.99      0.58 [ 0.09, 1.94] 95.9% 3648 1.00 0.01
##       robot_experience1     0.39      0.29 [-0.05, 0.88] 91.2% 3060 1.00 0.01
##       robot_experience2    -0.18      0.39 [-0.75, 0.48] 67.6% 2848 1.00 0.01
##       robot_experience3    -0.68      0.57 [-1.59, 0.25] 89.7% 2692 1.00 0.01
##       robot_experience4     0.78      0.58 [-0.11, 1.81] 91.0% 3337 1.00 0.01
##                 gender1    -0.00      0.16 [-0.24, 0.29] 50.8% 2930 1.00 0.00
##             num_optimal     0.01      0.10 [-0.15, 0.18] 53.1% 2472 1.00 0.00
##      state_idx_rescaled     0.36      0.23 [-0.02, 0.71] 93.8% 7025 1.00 0.00
##  has_ax1:noise_level_f1     0.24      0.23 [-0.15, 0.59] 84.6% 3036 1.00 0.00
##  has_ax1:noise_level_f2    -0.02      0.22 [-0.39, 0.35] 54.1% 2848 1.00 0.00</code></pre>
<pre class="r"><code>print(performance::r2(chose_dx.model.v))</code></pre>
<pre><code>## # Bayesian R2 with Standard Error
##
##   Conditional R2: 0.263 [0.016]
##      Marginal R2: 0.107 [0.027]</code></pre>
</div>
<div id="diagnostic-4" class="section level2">
<h2>Diagnostic</h2>
<pre class="r"><code># Alternative diagnostics: diagnostic_posterior(scenario_completed.model.v)

# LOO cross-validation results
print(chose_dx.model.t$criteria$loo$estimates)</code></pre>
<pre><code>##            Estimate        SE
## elpd_loo -729.53444 18.389647
## p_loo      84.16831  3.617458
## looic    1459.06888 36.779294</code></pre>
<pre class="r"><code>print(chose_dx.model.v$criteria$loo$estimates)</code></pre>
<pre><code>##            Estimate        SE
## elpd_loo -728.81145 18.395286
## p_loo      83.66685  3.579335
## looic    1457.62291 36.790571</code></pre>
<pre class="r"><code>print(loo_compare(chose_dx.model.null, chose_dx.model.t, chose_dx.model.v))</code></pre>
<pre><code>##                     elpd_diff se_diff
## chose_dx.model.null  0.0       0.0
## chose_dx.model.v    -2.7       3.0
## chose_dx.model.t    -3.4       3.1</code></pre>
<pre class="r"><code># Diagnostics of the sampling process
par(mfrow=c(1,2))
plot(chose_dx.model.t$criteria$loo, main = &quot;Trend Model&quot;)
plot(chose_dx.model.v$criteria$loo, main = &quot;Values Model&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-54-1.png' %}" width="1440" /></p>
<pre class="r"><code># Include posterior predictive checks of the sampling (are sampling according
# to the data that we actually have?)
p1 = pp_check(chose_dx.model.t, type = &quot;bars_grouped&quot;, group = &quot;has_ax&quot;) + ggtitle(&quot;t:has_ax&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars_grouped&#39; by default.</code></pre>
<pre class="r"><code>p3 = pp_check(chose_dx.model.t, type = &quot;bars_grouped&quot;, group = &quot;noise_level&quot;) + ggtitle(&quot;t:noise_level&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars_grouped&#39; by default.</code></pre>
<pre class="r"><code>grid.arrange(p1, p3, ncol = 2)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-54-2.png' %}" width="1440" /></p>
<pre class="r"><code>mcmc_intervals(as.matrix(chose_dx.model.t), regex_pars = &quot;b_|^shape&quot;, prob = 0.90, prob_outer = 0.95) +
  ggtitle(&quot;Trends Model&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-54-3.png' %}" width="1440" /></p>
<pre class="r"><code>p1 = pp_check(chose_dx.model.v, type = &quot;bars_grouped&quot;, group = &quot;has_ax&quot;) + ggtitle(&quot;v:has_ax&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars_grouped&#39; by default.</code></pre>
<pre class="r"><code>p3 = pp_check(chose_dx.model.v, type = &quot;bars_grouped&quot;, group = &quot;noise_level_f&quot;) + ggtitle(&quot;v:noise_level&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;bars_grouped&#39; by default.</code></pre>
<pre class="r"><code>grid.arrange(p1, p3, nrow = 2)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-54-4.png' %}" width="1440" /></p>
<pre class="r"><code>mcmc_intervals(as.matrix(chose_dx.model.v), regex_pars = &quot;b_|^shape&quot;, prob = 0.90, prob_outer = 0.95) +
  ggtitle(&quot;Values Model&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-54-5.png' %}" width="1440" /></p>
<pre class="r"><code># Plot predictions vs original
preds_p_df = chose_dx.model.null %&gt;% predict() %&gt;% as_tibble()
p1 =
  preds_p_df %&gt;%
    add_column(chose_dx = chose_dx.model.null$data$chose_dx) %&gt;%
    ggplot(aes(x = Estimate, y = chose_dx, color = chose_dx)) +
      geom_point() +
      labs(x = &quot;Predicted values&quot;) +
      ylim(0, 1) +
      xlim(0, 1) +
      ggtitle(&quot;Null Model&quot;)

preds_p_df = plot_df %&gt;%
  add_predicted_draws(chose_dx.model.null) %&gt;%
  group_by(chose_dx, .prediction) %&gt;%
  count()
p2 =
  preds_p_df %&gt;%
    ggplot(aes(x = chose_dx, y = .prediction, fill = n / sum(preds_p_df$n))) +
      geom_tile() +
      geom_text(aes(label = sprintf(&quot;%1.2f&quot;, n / sum(preds_p_df$n))), size = 10, vjust = 1) +
      scale_fill_gradient(limits = c(0, 1))

preds_p_df = chose_dx.model.t %&gt;% predict() %&gt;% as_tibble()
p3 =
  preds_p_df %&gt;%
    add_column(chose_dx = chose_dx.model.t$data$chose_dx) %&gt;%
    ggplot(aes(x = Estimate, y = chose_dx, color = chose_dx)) +
      geom_point() +
      labs(x = &quot;Predicted values&quot;) +
      ylim(0, 1) +
      xlim(0, 1) +
      ggtitle(&quot;Trends Model&quot;)

preds_p_df = plot_df %&gt;%
  add_predicted_draws(chose_dx.model.t) %&gt;%
  group_by(chose_dx, .prediction) %&gt;%
  count()
p4 =
  preds_p_df %&gt;%
    ggplot(aes(x = chose_dx, y = .prediction, fill = n / sum(preds_p_df$n))) +
      geom_tile() +
      geom_text(aes(label = sprintf(&quot;%1.2f&quot;, n / sum(preds_p_df$n))), size = 10, vjust = 1) +
      scale_fill_gradient(limits = c(0, 1))

# Plot predictions vs original
preds_p_df = chose_dx.model.v %&gt;% predict() %&gt;% as_tibble()
p5 =
  preds_p_df %&gt;%
    add_column(chose_dx = chose_dx.model.v$data$chose_dx) %&gt;%
    ggplot(aes(x = Estimate, y = chose_dx, color = chose_dx)) +
      geom_point() +
      labs(x = &quot;Predicted values&quot;) +
      ylim(0, 1) +
      xlim(0, 1) +
      ggtitle(&quot;Values Model&quot;)

preds_p_df = plot_df %&gt;%
  add_predicted_draws(chose_dx.model.v) %&gt;%
  group_by(chose_dx, .prediction) %&gt;%
  count()
p6 =
  preds_p_df %&gt;%
    ggplot(aes(x = chose_dx, y = .prediction, fill = n / sum(preds_p_df$n))) +
      geom_tile() +
      geom_text(aes(label = sprintf(&quot;%1.2f&quot;, n / sum(preds_p_df$n))), size = 10, vjust = 1) +
      scale_fill_gradient(limits = c(0, 1))

grid.arrange(p1, p3, p5, p2, p4, p6, nrow = 2, ncol = 3)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-54-6.png' %}" width="1440" /></p>
</div>
<div id="inference-4" class="section level2">
<h2>Inference</h2>
<pre class="r"><code># Compute the bayes factor for each parameter value. Note: we compare
# here against the ROPE, and not the degenerate null hypothesis of 0
# Bayes-Factor values would be more significant in the latter case
# According to the Raftery, 1995 rules
# bf = 1 - 3: Weak
# bf = 3 - 20: Positive
# bf = 20 - 150: Strong
# bf &gt; 150: Very strong
print(bayesfactor_rope(chose_dx.model.null, null = c(-0.055, 0.055)), n = 30)
print(bayesfactor_rope(chose_dx.model.t, null = c(-0.055, 0.055)), n = 30)
print(bayesfactor_rope(chose_dx.model.v, null = c(-0.055, 0.055)), n = 30)</code></pre>
<pre class="r"><code>options(digits = 3)
# Check the significance of each hypothesis
h_df = as_tibble(insight::get_parameters(chose_dx.model.t)) %&gt;%
  transmute(
    Intercept = b_Intercept,
    has_ax_test = -2 * b_has_ax1,
    noise_levelL_test = b_noise_level.L,
    noise_levelQ_test = b_noise_level.Q
  )

hyp_results = test_hypotheses(h_df, rope_values = c(-0.055, 0.055))
print(hyp_results %&gt;% select(Parameter, pd, HDI_low, HDI_high, ROPE_Percentage, ROPE_Equivalence) %&gt;% as.matrix())</code></pre>
<pre><code>##      Parameter           pd      HDI_low  HDI_high ROPE_Percentage
## [1,] &quot;Intercept&quot;         &quot;0.938&quot; &quot;-0.935&quot; &quot;3.219&quot;  &quot;0.02475&quot;
## [2,] &quot;has_ax_test&quot;       &quot;0.550&quot; &quot;-1.148&quot; &quot;1.131&quot;  &quot;0.13375&quot;
## [3,] &quot;noise_levelL_test&quot; &quot;0.994&quot; &quot;-1.696&quot; &quot;0.296&quot;  &quot;0.00625&quot;
## [4,] &quot;noise_levelQ_test&quot; &quot;0.585&quot; &quot;-0.936&quot; &quot;0.965&quot;  &quot;0.16250&quot;
##      ROPE_Equivalence
## [1,] &quot;Rejected&quot;
## [2,] &quot;Undecided&quot;
## [3,] &quot;Rejected&quot;
## [4,] &quot;Undecided&quot;</code></pre>
<pre class="r"><code>h_df = as_tibble(insight::get_parameters(chose_dx.model.v)) %&gt;%
  transmute(
    Intercept = b_Intercept,
    has_ax_test = -2 * b_has_ax1,
    noise_level1_test = b_noise_level_f1,
    noise_level2_test = b_noise_level_f2,
    noise_level3_test = - b_noise_level_f1 - b_noise_level_f2
  )

hyp_results = test_hypotheses(h_df, rope_values = c(-0.055, 0.055))
print(hyp_results %&gt;% select(Parameter, pd, HDI_low, HDI_high, ROPE_Percentage, ROPE_Equivalence) %&gt;% as.matrix())</code></pre>
<pre><code>##      Parameter           pd      HDI_low  HDI_high ROPE_Percentage
## [1,] &quot;Intercept&quot;         &quot;0.936&quot; &quot;-1.115&quot; &quot;2.969&quot;  &quot;0.0227&quot;
## [2,] &quot;has_ax_test&quot;       &quot;0.538&quot; &quot;-1.381&quot; &quot;1.177&quot;  &quot;0.1212&quot;
## [3,] &quot;noise_level1_test&quot; &quot;0.990&quot; &quot;-0.312&quot; &quot;1.503&quot;  &quot;0.0105&quot;
## [4,] &quot;noise_level2_test&quot; &quot;0.585&quot; &quot;-0.957&quot; &quot;0.956&quot;  &quot;0.1815&quot;
## [5,] &quot;noise_level3_test&quot; &quot;0.976&quot; &quot;-1.490&quot; &quot;0.493&quot;  &quot;0.0253&quot;
##      ROPE_Equivalence
## [1,] &quot;Rejected&quot;
## [2,] &quot;Undecided&quot;
## [3,] &quot;Rejected&quot;
## [4,] &quot;Undecided&quot;
## [5,] &quot;Undecided&quot;</code></pre>
<pre class="r"><code>options(digits = 7)</code></pre>
<p><strong>Results</strong>:</p>
<ol style="list-style-type: decimal">
<li>Noise Level Linear
<ul>
<li>Negative (negative slope) pd = 99.4%</li>
<li>Median = -0.68, 89% CI [-1.09, -0.22]</li>
<li>Small, Std.Median = 0.27</li>
<li>0.63% in ROPE</li>
</ul></li>
</ol>
<p>(The ROPE is defined as [-0.055, 0.055]. It is the range is suggested in the literature for logistic models and it corresponds to a probability range of 0.11. Therefore, a probability change of less than 0.055 is considered no different from a probability change of 0)</p>
</div>
<div id="posterior-plots-4" class="section level2">
<h2>Posterior Plots</h2>
<pre class="r"><code># A visualization data frame
gg_df = plot_df %&gt;%
    mutate(noise_level = fct_rev(noise_level)) %&gt;%
    mutate(chose_dx = factor(chose_dx)) %&gt;%
    mutate(user_id = factor(user_id)) %&gt;%
    mutate(chose_dx = fct_recode(chose_dx, no_follow=&quot;0&quot;, follow = &quot;1&quot;)) %&gt;%
    mutate(.fill_column = paste(noise_level, !(study_condition == &#39;BASELINE&#39;), sep = &quot;:&quot;)) %&gt;%
    count(user_id, study_condition, .fill_column, chose_dx, num_actions, has_ax, has_dx, noise_level,
          name = &quot;num_follow_dx&quot;)

# Simulate predictions on new data
p_df =
  plot_df %&gt;%
  data_grid(chose_dx,
            has_ax,
            noise_level_f,
            gender,
            age_group,
            robot_experience,
            num_optimal,
            state_idx_rescaled = seq_range(state_idx_rescaled, n = 4),
            .model = chose_dx.model.v)

fits_p_df = p_df %&gt;% add_fitted_draws(chose_dx.model.v,
                                      re_formula = NA,
                                      n = 100,
                                      seed = default_seed)
gc()</code></pre>
<pre><code>##            used  (Mb) gc trigger   (Mb)   max used    (Mb)
## Ncells  4921779 262.9    9278889  495.6   11598611   619.5
## Vcells 35287994 269.3  672742432 5132.7 1458218023 11125.4</code></pre>
<pre class="r"><code>pars_p_df = chose_dx.model.t %&gt;%
  extract_draws(newdata = p_df %&gt;% rename(noise_level = noise_level_f),
                re_formula = NA,
                nsamples = 100)
gc()</code></pre>
<pre><code>##            used  (Mb) gc trigger   (Mb)   max used    (Mb)
## Ncells  4921868 262.9    9278889  495.6   11598611   619.5
## Vcells 35496748 270.9  538193946 4106.1 1458218023 11125.4</code></pre>
<pre class="r"><code># We don&#39;t want predicted draws because that&#39;s a raw count of ones and zeros; not the
# probability distribution
# preds_p_df = p_df %&gt;% add_predicted_draws(scenario_completed.model.v)

# Plot the posterior distributions
rope_value = gg_df %&gt;%
  filter(chose_dx == &#39;follow&#39;, has_ax == F) %&gt;%
  summarise(.value = median(num_follow_dx / num_actions)) %&gt;%
  pull(.value)
gg_df %&gt;%
  filter(chose_dx == &#39;follow&#39;) %&gt;%
  ggplot(aes(x = has_ax, y = num_follow_dx / num_actions)) +
    geom_rect(ymin = rope_value-0.055, ymax = rope_value+0.055,
              xmin = -Inf, xmax = Inf,
              color = &quot;transparent&quot;, fill = &quot;white&quot;) +
    annotate(&quot;text&quot;, x = 0.5, y = rope_value, label = &quot;ROPE&quot;, angle = 90) +
    stat_halfeye(aes(y = .value), data = fits_p_df, position = position_nudge(x = 0.1)) +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep = &quot;:&quot;)), size = 2) +
    stat_summary(fun.y = median, geom = &quot;point&quot;, shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(y = &quot;follow_dx&quot;, color = &quot;AX:DX:Noise&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-57-1.png' %}" width="672" /></p>
<pre class="r"><code>rope_value = gg_df %&gt;%
  filter(chose_dx == &#39;follow&#39;, noise_level == &quot;0.0&quot;) %&gt;%
  summarise(.value = median(num_follow_dx / num_actions)) %&gt;%
  pull(.value)
gg_df %&gt;%
  filter(chose_dx == &#39;follow&#39;) %&gt;%
  mutate(noise_level = fct_rev(noise_level)) %&gt;%
  ggplot(aes(x = noise_level, y = num_follow_dx / num_actions)) +
    geom_rect(ymin = rope_value-0.055, ymax = rope_value+0.055,
              xmin = -Inf, xmax = Inf,
              color = &quot;transparent&quot;, fill = &quot;white&quot;) +
    annotate(&quot;text&quot;, x = 0.5, y = rope_value, label = &quot;ROPE&quot;, angle = 90) +
    stat_halfeye(aes(y = .value, x = noise_level_f), data = fits_p_df, position = position_nudge(x = 0.1)) +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep = &quot;:&quot;)), size = 2) +
    stat_summary(fun.y = median, geom = &quot;point&quot;, shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(y = &quot;follow_dx&quot;, color = &quot;AX:DX:Noise&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-57-2.png' %}" width="672" /></p>
<pre class="r"><code># Create a plot to show the trend
pars_p_df_X = pars_p_df$dpars$mu$fe$X[,c(&quot;Intercept&quot;, &quot;noise_level.L&quot;)]
pars_p_df_b = pars_p_df$dpars$mu$fe$b[,c(&quot;b_Intercept&quot;, &quot;b_noise_level.L&quot;)]
pars_p_df_y = pars_p_df_X %*% t(pars_p_df_b)
pars_p_df_y = inv_logit_scaled(pars_p_df_y)
p_df =
  p_df %&gt;%
  bind_cols(as_tibble(pars_p_df_y)) %&gt;%
  gather(key = &quot;.sample&quot;, value = &quot;.value&quot;, V1:V100) %&gt;%
  rename(noise_level = noise_level_f)

rope_value = gg_df %&gt;%
  filter(chose_dx == &#39;follow&#39;, noise_level == &quot;0.0&quot;) %&gt;%
  summarise(.value = median(num_follow_dx / num_actions)) %&gt;%
  pull(.value)
gg_df %&gt;%
  filter(chose_dx == &#39;follow&#39;) %&gt;%
  mutate(noise_level = fct_rev(noise_level)) %&gt;%
  ggplot(aes(x = noise_level, y = num_follow_dx / num_actions)) +
    geom_rect(ymin = rope_value-0.055, ymax = rope_value+0.055,
              xmin = -Inf, xmax = Inf,
              color = &quot;transparent&quot;, fill = &quot;white&quot;) +
    annotate(&quot;text&quot;, x = 0.5, y = rope_value, label = &quot;ROPE&quot;, angle = 90) +
    stat_halfeye(aes(y = .value, x = noise_level_f), data = fits_p_df, position = position_nudge(x = 0.1)) +
    geom_line(aes(y = .value, group = .sample, alpha = 0.3), data = p_df) +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep = &quot;:&quot;)), size = 2) +
    stat_summary(fun.y = median, geom = &quot;point&quot;, shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(y = &quot;scenario_completed&quot;, color = &quot;AX:DX:Noise&quot;) +
    guides(alpha = F)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-57-3.png' %}" width="672" /></p>
<pre class="r"><code># Remove the giant fits data frames
rm(fits_p_df, pars_p_df, pars_p_df_b, pars_p_df_X, pars_p_df_y, p_df, gg_df)</code></pre>
<pre class="r"><code>plot_df = actions %&gt;%
  filter(has_dx == T) %&gt;%
  select(X1, id, user_id, study_condition, start_condition, num_optimal,
         state_idx, state_idx_rescaled,
         age_group, gender, robot_experience,
         noise_level, noise_level_f, has_noise, has_dx, has_ax,
         scenario_completed, num_actions, chose_dx) %&gt;%
  mutate(scenario_completed = factor(scenario_completed)) %&gt;%
  mutate(scenario_completed = fct_recode(scenario_completed, Unresolved=&quot;0&quot;, Resolved = &quot;1&quot;),
         has_ax = fct_recode(has_ax, &quot;No AX&quot;=&quot;FALSE&quot;, &quot;AX&quot;=&quot;TRUE&quot;),
         noise_level = fct_recode(noise_level, &quot;Acc: 100%&quot;=&quot;0.0&quot;, &quot;Acc: 90%&quot;=&quot;1.0&quot;, &quot;Acc: 80%&quot;=&quot;2.0&quot;))

eff = fixef(chose_dx.model.v)

gg_df = plot_df %&gt;%
  mutate(.fill_column = paste(noise_level, !(study_condition == &#39;BASELINE&#39;), sep = &quot;:&quot;)) %&gt;%
  count(user_id, study_condition, .fill_column, chose_dx, num_actions, has_ax, noise_level,
        name = &quot;num_chose_dx&quot;) %&gt;%
  mutate(estimate = -1) %&gt;%
  mutate(estimate = if_else(noise_level == &quot;Acc: 100%&quot;, inv_logit_scaled(eff[&quot;noise_level_f1&quot;,&quot;Estimate&quot;] + eff[&quot;Intercept&quot;, &quot;Estimate&quot;]), estimate)) %&gt;%
  mutate(estimate = if_else(noise_level == &quot;Acc: 90%&quot;, inv_logit_scaled(eff[&quot;noise_level_f2&quot;,&quot;Estimate&quot;] + eff[&quot;Intercept&quot;, &quot;Estimate&quot;]), estimate)) %&gt;%
  mutate(estimate = if_else(noise_level == &quot;Acc: 80%&quot;, inv_logit_scaled(eff[&quot;Intercept&quot;, &quot;Estimate&quot;] - eff[&quot;noise_level_f1&quot;,&quot;Estimate&quot;] - eff[&quot;noise_level_f2&quot;,&quot;Estimate&quot;]), estimate)) %&gt;%
  mutate(noise_level = fct_rev(noise_level))

gg_df %&gt;%
  filter(chose_dx == &quot;1&quot;) %&gt;%
  ggplot(aes(noise_level, num_chose_dx / num_actions, group = noise_level, colour = noise_level)) +
    geom_hline(yintercept = inv_logit_scaled(eff[&quot;Intercept&quot;, &quot;Estimate&quot;]),
               size = .7, linetype = &quot;dashed&quot;, color = &quot;grey&quot;) +
    geom_count(aes(colour = NULL)) +
    geom_boxplot(aes(y = estimate)) +
    annotate(&quot;segment&quot;,
             x = 3,
             y = inv_logit_scaled(eff[&quot;noise_level_f1&quot;,&quot;Estimate&quot;] + eff[&quot;Intercept&quot;, &quot;Estimate&quot;]),
             xend = 2,
             yend = inv_logit_scaled(eff[&quot;noise_level_f2&quot;,&quot;Estimate&quot;] + eff[&quot;Intercept&quot;, &quot;Estimate&quot;]),
             colour = &quot;black&quot;) +
    annotate(&quot;segment&quot;,
             x = 1,
             y = inv_logit_scaled(eff[&quot;Intercept&quot;, &quot;Estimate&quot;] - eff[&quot;noise_level_f1&quot;,&quot;Estimate&quot;] - eff[&quot;noise_level_f2&quot;,&quot;Estimate&quot;]),
             xend = 2,
             yend = inv_logit_scaled(eff[&quot;noise_level_f2&quot;,&quot;Estimate&quot;] + eff[&quot;Intercept&quot;, &quot;Estimate&quot;]),
             color = &quot;black&quot;) +
    annotate(&quot;text&quot;,
             x = 3.5,
             y = inv_logit_scaled(eff[&quot;Intercept&quot;, &quot;Estimate&quot;] - 0.25),
             label = &quot;Mean of all levels&quot;,
             angle = 90,
             group = NA,
             colour = &quot;black&quot;) +
    scale_y_continuous(limits = c(0.0, 1.0), breaks = c(0.0, 0.5, 1.0)) +
    labs(y = &quot;Fraction of participants that resolved fault&quot;, x = NULL) +
    scale_colour_manual(values = c(&quot;#d62728&quot;, &quot;#2ca02c&quot;, &quot;#ff7f0e&quot;)) +
    scale_size_area(max_size = 1) +
    theme_economist_white(base_size = 13) +
    theme(plot.background = element_rect(fill = &quot;white&quot;),
          axis.title.y = element_blank()) +
    legend_none()</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-58-1.png' %}" width="672" /></p>
</div>
</div>
<div id="sus-system-usability-scale" class="section level1">
<h1>SUS: System Usability Scale</h1>
<pre class="r"><code>plot_df = users %&gt;%
  select(X1, id, study_condition, start_condition, num_optimal,
         age_group, gender, robot_experience,
         noise_level, noise_level_f, has_noise, has_dx, has_ax,
         sus, scenario_completed) %&gt;%
  mutate(scenario_completed = factor(scenario_completed))

text_short(report(plot_df))</code></pre>
<pre><code>## The data contains 200 observations of the following variables:
##   - X1: Mean = 99.50, SD = 57.88, range = [0, 199], 0 missing
##   - id: Mean = 341.38, SD = 159.33, range = [75, 599], 0 missing
##   - study_condition: 10 levels: DX_100, n = 20; AX_100, n = 20; DXAX_100, n = 20; DX_90, n = 20; AX_90, n = 20; DXAX_90, n = 20; DX_80, n = 20; AX_80, n = 20; DXAX_80, n = 20 and BASELINE, n = 20
##   - start_condition: 4 entries: dt.kc.default.default.default.empty.kc, n = 50; kc.dt.default.default.default.empty.dt, n = 50; kc.dt.occluding.above_mug.default.empty.dt, n = 50 and 1 other
##   - num_optimal: Mean = 4.50, SD = 1.50, range = [3, 7], 0 missing
##   - age_group: 7 levels: 2, n = 21; 3, n = 47; 4, n = 44; 5, n = 33; 6, n = 19; 7, n = 16 and 8, n = 20
##   - gender: 2 levels: F, n = 73 and M, n = 127
##   - robot_experience: 5 levels: 0, n = 133; 1, n = 33; 2, n = 12; 3, n = 12 and 4, n = 10
##   - noise_level: 3 levels: 0.0, n = 80; 1.0, n = 60 and 2.0, n = 60
##   - noise_level_f: 3 levels: 0.0, n = 80; 1.0, n = 60 and 2.0, n = 60
##   - has_noise: 2 levels: FALSE, n = 80 and TRUE, n = 120
##   - has_dx: 2 levels: FALSE, n = 80 and TRUE, n = 120
##   - has_ax: 2 levels: FALSE, n = 80 and TRUE, n = 120
##   - sus: Mean = 67.05, SD = 22.86, range = [0, 100], 0 missing
##   - scenario_completed: 2 levels: 0, n = 38 and 1, n = 162</code></pre>
<div id="data-5" class="section level2">
<h2>Data</h2>
<pre class="r"><code># Plot by the study condition
plot_df %&gt;%
  mutate(.fill_column = paste(noise_level, !(study_condition == &#39;BASELINE&#39;), sep = &quot;:&quot;)) %&gt;%
  ggplot(aes(study_condition, sus, fill = .fill_column)) +
    geom_boxplot() +
    geom_count() +
    labs(y = &quot;Number unnecessary actions&quot;, fill = &quot;noise:has_suggestions&quot;) +
    scale_fill_economist()</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-61-1.png' %}" width="1440" /></p>
<pre class="r"><code># Visualize the data by the three variables that we care about
gg_df = plot_df %&gt;%
  mutate(.fill_column = paste(noise_level, !(study_condition == &#39;BASELINE&#39;), sep = &quot;:&quot;)) %&gt;%
  mutate(noise_level = fct_rev(noise_level))

p1 = gg_df %&gt;%
  ggplot(aes(has_ax, sus, fill=.fill_column)) +
    geom_boxplot() +
    geom_count() +
    facet_grid(rows = vars(noise_level), cols = vars(has_dx), labeller = label_both) +
    labs(y = &quot;Number unnecessary actions&quot;) +
    scale_fill_economist()

p2 = gg_df %&gt;%
  ggplot(aes(has_dx, sus, fill=.fill_column)) +
    geom_boxplot() +
    geom_count() +
    facet_grid(rows = vars(noise_level), cols = vars(has_ax), labeller = label_both) +
    labs(y = &quot;Number unnecessary actions&quot;) +
    scale_fill_economist()

p3 = gg_df %&gt;%
  mutate(noise_level = fct_rev(noise_level)) %&gt;%
  mutate(has_ax = factor(has_ax)) %&gt;%
  mutate(has_ax = fct_rev(has_ax)) %&gt;%
  ggplot(aes(noise_level, sus, fill=.fill_column)) +
    geom_boxplot() +
    geom_count() +
    facet_grid(rows = vars(has_ax), cols = vars(has_dx), labeller = label_both) +
    labs(y = &quot;Number unnecessary actions&quot;) +
    scale_fill_economist()

grid.arrange(p1, p2, p3, ncol = 3)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-61-2.png' %}" width="1440" /></p>
<pre class="r"><code># Visualize also the variation in each of the three levels that we care about
p1 = gg_df %&gt;%
  ggplot(aes(sus)) +
    geom_histogram(binwidth = 10) +
    facet_grid(rows = vars(has_ax)) +
    ylim(0, 30) +
    scale_fill_economist() +
    legend_none()

p2 = gg_df %&gt;%
  ggplot(aes(sus)) +
    geom_histogram(binwidth = 10) +
    facet_grid(rows = vars(has_dx)) +
    ylim(0, 30) +
    scale_fill_economist() +
    theme(legend.position = &quot;bottom&quot;)

p3 = gg_df %&gt;%
  ggplot(aes(sus)) +
    geom_histogram(binwidth = 10) +
    facet_grid(rows = vars(noise_level)) +
    ylim(0, 30) +
    scale_fill_economist() +
    legend_none()

grid.arrange(p1, p2, p3, ncol = 3)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-61-3.png' %}" width="1440" /></p>
</div>
<div id="model-5" class="section level2">
<h2>Model</h2>
<p>Based on the data, we assume a skew-normal linear model:</p>
<p><span class="math display">\[sus_i = SkewNormal(\mu_i, \sigma, \alpha)\]</span> <span class="math display">\[\begin{aligned}
\mu_i &amp;= \beta_0 + \beta_{ax}\text{ax}_i + \beta_{dx}\text{dx}_i + \beta_{noise}\text{noise}_i +\\ &amp;\beta_{ax:noise}\text{ax}_i\text{noise}_i + \beta_{dx:noise}\text{dx}_i\text{noise}_i + \\
&amp;\beta_{no}\text{no}_i + \mathbf{\beta_{demo}X_{demo,i}}
\end{aligned}\]</span> <span class="math display">\[\begin{aligned}
\beta_{.} &amp;\sim Normal(0, 10) \\
\sigma &amp;\sim HalfStudent(3, 0, 22) \\
\alpha &amp;\sim Normal(0, 4)
\end{aligned}\]</span></p>
<p>The prior for the <span class="math inline">\(\sigma, \alpha\)</span> parameters are the default used in <code>brms</code>; I see no need to change it.</p>
<p>Model fitting:</p>
<pre class="r"><code># A null model to compare against
sus.model.null = brm(
  sus ~ 0 + Intercept,
  family = &quot;skew_normal&quot;,
  prior = c(set_prior(&quot;normal(0, 10)&quot;, class = &quot;b&quot;),
            set_prior(&quot;student_t(3, 0, 22)&quot;, class = &quot;sigma&quot;),
            set_prior(&quot;normal(0, 4)&quot;, class = &quot;alpha&quot;)),
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
sus.model.null = add_criterion(sus.model.null, &quot;waic&quot;)
sus.model.null = add_criterion(sus.model.null, &quot;loo&quot;, reloo = T)
saveModel(sus.model.null)

# The trend model to see if there is a trend in the noise level variable
sus.model.t = brm(
  sus ~ 0 + Intercept + ((has_dx + has_ax) * noise_level) + (gender + age_group + robot_experience) + num_optimal,
  family = &quot;skew_normal&quot;,
  prior = c(set_prior(&quot;normal(0, 10)&quot;, class = &quot;b&quot;),
            set_prior(&quot;student_t(3, 0, 22)&quot;, class = &quot;sigma&quot;),
            set_prior(&quot;normal(0, 4)&quot;, class = &quot;alpha&quot;)),
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
sus.model.t = add_criterion(sus.model.t, &quot;waic&quot;)
sus.model.t = add_criterion(sus.model.t, &quot;loo&quot;, reloo = T)
saveModel(sus.model.t)

# The values model, to see if specific values of the noise level variable are significant
sus.model.v = brm(
  sus ~ 0 + Intercept + ((has_dx + has_ax) * noise_level_f) + (gender + age_group + robot_experience) + num_optimal,
  family = &quot;skew_normal&quot;,
  prior = c(set_prior(&quot;normal(0, 10)&quot;, class = &quot;b&quot;),
            set_prior(&quot;student_t(3, 0, 22)&quot;, class = &quot;sigma&quot;),
            set_prior(&quot;normal(0, 4)&quot;, class = &quot;alpha&quot;)),
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
sus.model.v = add_criterion(sus.model.v, &quot;waic&quot;)
sus.model.v = add_criterion(sus.model.v, &quot;loo&quot;, reloo = T)
saveModel(sus.model.v)
# More model plots are available at:
# https://bookdown.org/ajkurz/Statistical_Rethinking_recoded/counting-and-classification.html

# Attempted RStanarm models: The estimate contrasts doesn&#39;t seem to be as helpful?
# scenario_completed.model.v = stan_glm(
#   scenario_completed ~ ((has_dx + has_ax) * noise_level) + (gender + age_group + robot_experience) + num_optimal,
#   family = binomial(link = &quot;logit&quot;),
#   prior = normal(0, 10),
#   prior_intercept = normal(0, 10),
#   data = plot_df,
#   QR = T,
#   seed = default_seed
# )</code></pre>
<pre class="r"><code># Load the models
sus.model.null = loadModel(&quot;sus.model.null&quot;)
sus.model.t = loadModel(&quot;sus.model.t&quot;)
sus.model.v = loadModel(&quot;sus.model.v&quot;)</code></pre>
<p>Model fitting results:</p>
<pre class="r"><code># print(summary(sus.model.null))
print(performance::r2(sus.model.null))</code></pre>
<pre><code>## # Bayesian R2 with Standard Error
##
##   Conditional R2: 0.000 [0.000]</code></pre>
<pre class="r"><code># Print the parameters, and some initial diagnostics
print(tidy_stan(sus.model.t))</code></pre>
<pre><code>## Warning: &#39;tidy_stan&#39; is deprecated.
## Use &#39;parameters::model_parameters()&#39; instead.
## See help(&quot;Deprecated&quot;)</code></pre>
<pre><code>##
## Summary Statistics of Stan-Model
##
## # Fixed effects
##
##              Parameter Estimate Std.Error        HDI(89%)     pd  ESS Rhat MCSE
##            (Intercept)    55.40      4.52 [ 48.17, 62.75] 100.0% 2876 1.00 0.09
##                has_dx1     0.25      1.55 [ -2.21,  2.82]  56.2% 3357 1.00 0.03
##                has_ax1    -1.67      1.56 [ -4.09,  0.83]  85.9% 3077 1.00 0.03
##          noise_level.L     0.27      2.52 [ -4.01,  4.12]  54.0% 3038 1.00 0.05
##          noise_level.Q     1.28      2.60 [ -2.90,  5.29]  68.5% 3672 1.00 0.04
##                gender1     1.15      1.39 [ -0.95,  3.57]  79.3% 3965 1.00 0.02
##             age_group1    -1.52      3.44 [ -6.69,  4.00]  66.8% 4481 1.00 0.05
##             age_group2     4.12      2.60 [  0.12,  8.29]  94.7% 3827 1.00 0.04
##             age_group3     3.31      2.71 [ -0.81,  7.92]  89.6% 3806 1.00 0.05
##             age_group4     3.40      3.24 [ -1.68,  8.42]  86.7% 4287 1.00 0.05
##             age_group5    -0.95      3.63 [ -6.55,  4.87]  60.5% 3382 1.00 0.06
##             age_group6    -0.63      3.70 [ -6.59,  5.60]  56.8% 3666 1.00 0.06
##      robot_experience1     3.62      2.45 [ -0.14,  7.54]  93.5% 3650 1.00 0.04
##      robot_experience2     2.71      3.29 [ -2.81,  7.77]  80.0% 3100 1.00 0.06
##      robot_experience3     0.90      4.29 [ -6.20,  7.57]  58.3% 3606 1.00 0.07
##      robot_experience4    -8.48      3.99 [-14.24, -1.12]  96.8% 3716 1.00 0.07
##            num_optimal     1.42      0.85 [  0.00,  2.75]  95.8% 3388 1.00 0.02
##  has_dx1.noise_level.L     2.22      2.62 [ -1.93,  6.08]  80.5% 3162 1.00 0.05
##  has_dx1.noise_level.Q     0.64      2.71 [ -3.72,  4.70]  58.7% 3782 1.00 0.04
##  has_ax1.noise_level.L    -0.72      2.60 [ -5.04,  3.15]  61.1% 3620 1.00 0.04
##  has_ax1.noise_level.Q     3.08      2.75 [ -1.35,  7.54]  86.5% 3230 1.00 0.05</code></pre>
<pre class="r"><code>print(performance::r2(sus.model.t))</code></pre>
<pre><code>## # Bayesian R2 with Standard Error
##
##   Conditional R2: 0.104 [0.025]</code></pre>
<pre class="r"><code>print(tidy_stan(sus.model.v))</code></pre>
<pre><code>## Warning: &#39;tidy_stan&#39; is deprecated.
## Use &#39;parameters::model_parameters()&#39; instead.
## See help(&quot;Deprecated&quot;)</code></pre>
<pre><code>##
## Summary Statistics of Stan-Model
##
## # Fixed effects
##
##               Parameter Estimate Std.Error        HDI(89%)     pd  ESS Rhat
##             (Intercept)    55.31      4.47 [ 48.21, 62.58] 100.0% 2757 1.00
##                 has_dx1     0.21      1.54 [ -2.15,  2.80]  55.5% 4058 1.00
##                 has_ax1    -1.69      1.57 [ -4.19,  0.88]  85.9% 3567 1.00
##          noise_level_f1     0.30      1.91 [ -2.78,  3.33]  56.8% 3428 1.00
##          noise_level_f2    -1.12      2.11 [ -4.66,  2.09]  70.5% 3943 1.00
##                 gender1     1.17      1.35 [ -0.94,  3.35]  80.6% 4383 1.00
##              age_group1    -1.37      3.57 [ -7.03,  4.06]  65.5% 4538 1.00
##              age_group2     4.15      2.64 [ -0.02,  8.30]  94.5% 4370 1.00
##              age_group3     3.24      2.78 [ -1.14,  7.74]  88.2% 4170 1.00
##              age_group4     3.36      3.06 [ -1.75,  8.07]  87.3% 4608 1.00
##              age_group5    -0.95      3.51 [ -6.76,  4.46]  61.0% 4567 1.00
##              age_group6    -0.59      3.85 [ -6.54,  5.58]  56.4% 4424 1.00
##       robot_experience1     3.65      2.30 [ -0.11,  7.51]  94.4% 4039 1.00
##       robot_experience2     2.84      3.19 [ -2.28,  7.93]  81.2% 4103 1.00
##       robot_experience3     0.91      4.34 [ -5.66,  7.84]  57.7% 4559 1.00
##       robot_experience4    -8.45      3.84 [-14.71, -1.78]  96.9% 4047 1.00
##             num_optimal     1.44      0.86 [  0.11,  2.92]  95.7% 3118 1.00
##  has_dx1:noise_level_f1    -1.44      1.90 [ -4.39,  1.76]  76.5% 3137 1.00
##  has_dx1:noise_level_f2    -0.56      2.17 [ -4.08,  2.74]  60.7% 3561 1.00
##  has_ax1:noise_level_f1     1.81      2.00 [ -1.69,  4.87]  81.3% 3477 1.00
##  has_ax1:noise_level_f2    -2.58      2.33 [ -6.25,  1.06]  87.2% 3311 1.00
##  MCSE
##  0.09
##  0.02
##  0.03
##  0.03
##  0.03
##  0.02
##  0.05
##  0.04
##  0.04
##  0.05
##  0.05
##  0.06
##  0.04
##  0.05
##  0.06
##  0.06
##  0.02
##  0.03
##  0.04
##  0.03
##  0.04</code></pre>
<pre class="r"><code>print(performance::r2(sus.model.v))</code></pre>
<pre><code>## # Bayesian R2 with Standard Error
##
##   Conditional R2: 0.105 [0.026]</code></pre>
</div>
<div id="diagnostic-5" class="section level2">
<h2>Diagnostic</h2>
<pre class="r"><code># Alternative diagnostics: diagnostic_posterior(scenario_completed.model.v)

# LOO cross-validation results
print(sus.model.t$criteria$loo$estimates)</code></pre>
<pre><code>##            Estimate        SE
## elpd_loo -904.72965  9.322567
## p_loo      18.79304  2.482107
## looic    1809.45929 18.645135</code></pre>
<pre class="r"><code>print(sus.model.v$criteria$loo$estimates)</code></pre>
<pre><code>##            Estimate        SE
## elpd_loo -904.42745  9.311701
## p_loo      18.53389  2.419375
## looic    1808.85490 18.623402</code></pre>
<pre class="r"><code>print(loo_compare(sus.model.null,
                  sus.model.t,
                  sus.model.v))</code></pre>
<pre><code>##                elpd_diff se_diff
## sus.model.null   0.0       0.0
## sus.model.v    -13.2       4.2
## sus.model.t    -13.5       4.2</code></pre>
<pre class="r"><code># Diagnostics of the sampling process
par(mfrow=c(1,2))
plot(sus.model.t$criteria$loo, main = &quot;Trend Model&quot;)
plot(sus.model.v$criteria$loo, main = &quot;Values Model&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-65-1.png' %}" width="1440" /></p>
<pre class="r"><code># Include posterior predictive checks of the sampling (are sampling according
# to the data that we actually have?)
mcmc_intervals(as.matrix(sus.model.null), regex_pars = &quot;b_&quot;, prob = 0.90, prob_outer = 0.95) +
  ggtitle(&quot;Null Model&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-65-2.png' %}" width="1440" /></p>
<pre class="r"><code>p1 = pp_check(sus.model.t, group = &quot;has_ax&quot;, stat = &quot;median&quot;) + ggtitle(&quot;t:has_ax&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;dens_overlay&#39; by default.</code></pre>
<pre><code>## Warning: The following arguments were unrecognized and ignored: stat, group</code></pre>
<pre class="r"><code>p2 = pp_check(sus.model.t, group = &quot;has_dx&quot;) + ggtitle(&quot;t:has_dx&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;dens_overlay&#39; by default.</code></pre>
<pre><code>## Warning: The following arguments were unrecognized and ignored: group</code></pre>
<pre class="r"><code>p3 = pp_check(sus.model.t, group = &quot;noise_level&quot;) + ggtitle(&quot;t:noise_level&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;dens_overlay&#39; by default.</code></pre>
<pre><code>## Warning: The following arguments were unrecognized and ignored: group</code></pre>
<pre class="r"><code>grid.arrange(p1, p2, p3, nrow = 3)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-65-3.png' %}" width="1440" /></p>
<pre class="r"><code>mcmc_intervals(as.matrix(sus.model.t), regex_pars = &quot;b_&quot;, prob = 0.90, prob_outer = 0.95) +
  ggtitle(&quot;Trends Model&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-65-4.png' %}" width="1440" /></p>
<pre class="r"><code>p1 = pp_check(sus.model.v, group = &quot;has_ax&quot;) + ggtitle(&quot;v:has_ax&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;dens_overlay&#39; by default.</code></pre>
<pre><code>## Warning: The following arguments were unrecognized and ignored: group</code></pre>
<pre class="r"><code>p2 = pp_check(sus.model.v, group = &quot;has_dx&quot;) + ggtitle(&quot;v:has_dx&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;dens_overlay&#39; by default.</code></pre>
<pre><code>## Warning: The following arguments were unrecognized and ignored: group</code></pre>
<pre class="r"><code>p3 = pp_check(sus.model.v, group = &quot;noise_level_f&quot;) + ggtitle(&quot;v:noise_level&quot;)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;dens_overlay&#39; by default.</code></pre>
<pre><code>## Warning: The following arguments were unrecognized and ignored: group</code></pre>
<pre class="r"><code>grid.arrange(p1, p2, p3, nrow = 3)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-65-5.png' %}" width="1440" /></p>
<pre class="r"><code>mcmc_intervals(as.matrix(sus.model.v), regex_pars = &quot;b_&quot;, prob = 0.90, prob_outer = 0.95) +
  ggtitle(&quot;Values Model&quot;)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-65-6.png' %}" width="1440" /></p>
<pre class="r"><code># Plot predictions vs original
preds_p_df = sus.model.null %&gt;% predict() %&gt;% as_tibble()
p1 =
  preds_p_df %&gt;%
    add_column(sus = sus.model.null$data$sus) %&gt;%
    ggplot(aes(x = sus, y = Estimate)) +
      geom_point() +
      labs(y = &quot;Predicted values&quot;) +
      ggtitle(&quot;Null Model&quot;)

preds_p_df = sus.model.t %&gt;% predict() %&gt;% as_tibble()
p3 =
  preds_p_df %&gt;%
    add_column(sus = sus.model.t$data$sus) %&gt;%
    ggplot(aes(x = sus, y = Estimate)) +
      geom_point() +
      labs(y = &quot;Predicted values&quot;) +
      ggtitle(&quot;Trends Model&quot;)

preds_p_df = sus.model.v %&gt;% predict() %&gt;% as_tibble()
p5 =
  preds_p_df %&gt;%
    add_column(sus = sus.model.v$data$sus) %&gt;%
    ggplot(aes(x = sus, y = Estimate)) +
      geom_point() +
      labs(y = &quot;Predicted values&quot;) +
      ggtitle(&quot;Values Model&quot;)

grid.arrange(p1, p3, p5, nrow = 1, ncol = 3)</code></pre>
<p><img src="{% static 'roman2020_files/figure-html/unnamed-chunk-65-7.png' %}" width="1440" /></p>
</div>
<div id="inference-5" class="section level2">
<h2>Inference</h2>
<pre class="r"><code># Compute the bayes factor for each parameter value. Note: we compare
# here against the ROPE, and not the degenerate null hypothesis of 0
# Bayes-Factor values would be more significant in the latter case
# According to the Raftery, 1995 rules
# bf = 1 - 3: Weak
# bf = 3 - 20: Positive
# bf = 20 - 150: Strong
# bf &gt; 150: Very strong
print(bayesfactor_rope(sus.model.null), n = 30)
print(bayesfactor_rope(sus.model.t), n = 30)
print(bayesfactor_rope(sus.model.v), n = 30)</code></pre>
<pre class="r"><code># Compare the models. Note: we cannot do this because we don&#39;t have enough samples.
# The default is 4000; apparently these functions are only meaningful with 40000
# comparison = bayesfactor_models(scenario_completed.model.t, scenario_completed.model.v,
#                                 denominator = scenario_completed.model.null)
# print(comparison)
# print(bayesfactor_inclusion(comparison))

# # If we&#39;re using the brms functions, then use the following (make sure to update!)
# common_hyp_to_test = c(&quot;-2 * has_ax1 = 0&quot;, &quot;-2 * has_dx1 = 0&quot;)
# noise_levels_hyp_to_test = c(
#   &quot;Intercept-noise_level_f1 = 0&quot;, &quot;Intercept-noise_level_f2 = 0&quot;, &quot;Intercept-noise_level_f1-noise_level_f2 = 0&quot;,
#   &quot;Intercept-has_ax1:noise_level_f1 = 0&quot;, &quot;Intercept-has_dx1:noise_level_f1 = 0&quot;,
#   &quot;Intercept-has_ax1:noise_level_f2 = 0&quot;, &quot;Intercept-has_dx1:noise_level_f2 = 0&quot;,
#   &quot;Intercept-has_ax1:noise_level_f1-has_ax1:noise_level_f2 = 0&quot;,
#   &quot;Intercept-has_dx1:noise_level_f1-has_dx1:noise_level_f2 = 0&quot;
# )
# hyp_results = test_hypotheses(hypotheses_list = c(common_hyp_to_test, noise_levels_hyp_to_test), model = scenario_completed.model.v)
# print(hyp_results$hypothesis)
# plot(hyp_results, ask=F)

options(digits = 3)
# Check the significance of each hypothesis
h_df = as_tibble(insight::get_parameters(sus.model.t)) %&gt;%
  transmute(
    Intercept = b_Intercept,
    has_ax_test = -2 * b_has_ax1,
    has_dx_test = -2 * b_has_dx1,
    noise_levelL_test = b_noise_level.L,
    noise_levelQ_test = b_noise_level.Q
  )

hyp_results = test_hypotheses(h_df, rope_values = c(-2.3, 2.3))
print(hyp_results %&gt;% select(Parameter, pd, HDI_low, HDI_high, ROPE_Percentage, ROPE_Equivalence) %&gt;% as.matrix())</code></pre>
<pre><code>##      Parameter           pd      HDI_low  HDI_high ROPE_Percentage
## [1,] &quot;Intercept&quot;         &quot;1.000&quot; &quot; 37.29&quot; &quot;69.93&quot;  &quot;0.000&quot;
## [2,] &quot;has_ax_test&quot;       &quot;0.859&quot; &quot; -7.66&quot; &quot;14.86&quot;  &quot;0.344&quot;
## [3,] &quot;has_dx_test&quot;       &quot;0.562&quot; &quot;-10.85&quot; &quot;12.70&quot;  &quot;0.537&quot;
## [4,] &quot;noise_levelL_test&quot; &quot;0.540&quot; &quot; -7.60&quot; &quot;10.48&quot;  &quot;0.631&quot;
## [5,] &quot;noise_levelQ_test&quot; &quot;0.685&quot; &quot; -7.58&quot; &quot; 9.27&quot;  &quot;0.568&quot;
##      ROPE_Equivalence
## [1,] &quot;Rejected&quot;
## [2,] &quot;Undecided&quot;
## [3,] &quot;Undecided&quot;
## [4,] &quot;Undecided&quot;
## [5,] &quot;Undecided&quot;</code></pre>
<pre class="r"><code>h_df = as_tibble(insight::get_parameters(sus.model.v)) %&gt;%
  transmute(
    Intercept = b_Intercept,
    has_ax_test = -2 * b_has_ax1,
    has_dx_test = -2 * b_has_dx1,
    noise_level1_test = b_noise_level_f1,
    noise_level2_test = b_noise_level_f2,
    noise_level3_test = - b_noise_level_f1 - b_noise_level_f2
  )

hyp_results = test_hypotheses(h_df, rope_values = c(-2.3, 2.3))
print(hyp_results %&gt;% select(Parameter, pd, HDI_low, HDI_high, ROPE_Percentage, ROPE_Equivalence) %&gt;% as.matrix())</code></pre>
<pre><code>##      Parameter           pd      HDI_low  HDI_high ROPE_Percentage
## [1,] &quot;Intercept&quot;         &quot;1.000&quot; &quot; 35.85&quot; &quot;70.37&quot;  &quot;0.000&quot;
## [2,] &quot;has_ax_test&quot;       &quot;0.859&quot; &quot; -8.06&quot; &quot;15.04&quot;  &quot;0.326&quot;
## [3,] &quot;has_dx_test&quot;       &quot;0.554&quot; &quot;-13.01&quot; &quot;11.60&quot;  &quot;0.554&quot;
## [4,] &quot;noise_level1_test&quot; &quot;0.568&quot; &quot; -6.28&quot; &quot; 6.46&quot;  &quot;0.762&quot;
## [5,] &quot;noise_level2_test&quot; &quot;0.705&quot; &quot; -8.89&quot; &quot; 5.83&quot;  &quot;0.654&quot;
## [6,] &quot;noise_level3_test&quot; &quot;0.625&quot; &quot; -6.52&quot; &quot; 7.92&quot;  &quot;0.669&quot;
##      ROPE_Equivalence
## [1,] &quot;Rejected&quot;
## [2,] &quot;Undecided&quot;
## [3,] &quot;Undecided&quot;
## [4,] &quot;Undecided&quot;
## [5,] &quot;Undecided&quot;
## [6,] &quot;Undecided&quot;</code></pre>
<pre class="r"><code>options(digits = 7)</code></pre>
<p><strong>Results</strong>:</p>
<p>There is no significant effect of any of the suggestions parameters on the SUS.</p>
<p>(The ROPE is defined as [-2.3, 2.3]. It corresponds to <code>0.1 * SD</code> of the output)</p>
<p>There are no posterior plots to show (all effects are supposedly non-existent).</p>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->
<script>
$(document).ready(function () {
  window.initializeCodeFolding("hide" === "show");
});
</script>

<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3,h4",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
