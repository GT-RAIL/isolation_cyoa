---
title: "IROS 2020"
output:
  html_document:
    theme: readable
    code_folding: hide
    df_print: kable
    toc: true
    toc_depth: 2
    toc_float: true
---

TODO: Create better response plots such as the ones seen in:

1. [Example plots](https://mjskay.github.io/tidybayes/articles/tidy-brms.html#posterior-predictions-kruschke-style)
1. [Responses for multilevel models](https://bookdown.org/ajkurz/Statistical_Rethinking_recoded/multilevel-models.html)
1. [Responses for count/binary models](https://bookdown.org/ajkurz/Statistical_Rethinking_recoded/counting-and-classification.html#binomial-regression)

```{r message=FALSE, warning=FALSE}
library(readr)
library(ggplot2)
library(tidyr)
library(broom)
library(car)
library(stats)
library(ggsignif)
library(ggthemes)
library(dplyr)
library(tibble)
library(brms)
library(jtools)
library(modelr)
library(bayesplot)
library(tidybayes)
library(stringr)

# Setup for multiprocessing
# library(future)
# plan(multiprocess)
```

```{r}
# Script execution globals
train_models = F
default_seed = 0x1331
data_folder = "~/Documents/GT/Research/Data/arbitration/2019-12-09/results"
```

```{r, message=FALSE, warning=FALSE}
# Helper functions copied from http://www.cookbook-r.com/Graphs/Plotting_means_and_error_bars_(ggplot2)/
## Gives count, mean, standard deviation, standard error of the mean, and confidence interval (default 95%).
##   data: a data frame.
##   measurevar: the name of a column that contains the variable to be summariezed
##   groupvars: a vector containing names of columns that contain grouping variables
##   na.rm: a boolean that indicates whether to ignore NA's
##   conf.interval: the percent range of the confidence interval (default is 95%)
summarySE <- function(data=NULL, measurevar, groupvars=NULL, na.rm=FALSE,
                      conf.interval=.95, .drop=TRUE) {
    library(plyr)

    # New version of length which can handle NA's: if na.rm==T, don't count them
    length2 <- function (x, na.rm=FALSE) {
        if (na.rm) sum(!is.na(x))
        else       length(x)
    }

    # This does the summary. For each group's data frame, return a vector with
    # N, mean, and sd
    datac <- ddply(data, groupvars, .drop=.drop,
      .fun = function(xx, col) {
        c(N    = length2(xx[[col]], na.rm=na.rm),
          mean = mean   (xx[[col]], na.rm=na.rm),
          sd   = sd     (xx[[col]], na.rm=na.rm)
        )
      },
      measurevar
    )

    # Rename the "mean" column    
    datac <- rename(datac, c("mean" = measurevar))

    datac$se <- datac$sd / sqrt(datac$N)  # Calculate standard error of the mean

    # Confidence interval multiplier for standard error
    # Calculate t-statistic for confidence interval: 
    # e.g., if conf.interval is .95, use .975 (above/below), and use df=N-1
    ciMult <- qt(conf.interval/2 + .5, datac$N-1)
    datac$ci <- datac$se * ciMult

    return(datac)
}

# Create a simple coding contrasts matrix
contr.deviation = function(nlevels) {
  x1 = contr.treatment(nlevels)
  x2 = matrix(rep(1/nlevels, (nlevels-1) * nlevels), ncol = (nlevels-1))
  return(x1-x2)
}

# NOTE: For now we're ignoring this advice
# # Use a simple coding contrasts matrix to add our own contrasts columns
# # Explained in: http://talklab.psy.gla.ac.uk/tvw/catpred/
# create_simple_contrasts = function(d, column_name) {
#  factor_column = d %>% pull(column_name)
#  contrast_matrix = contr.deviation(length(levels(factor_column)))
# }

# Save a model
saveModel = function(model, folder = data_folder) {
  saveRDS(model, file = file.path(folder, paste(substitute(model), ".RDS", sep = '')))
}

# Load a model
loadModel = function(model_name, folder = data_folder) {
  return(readRDS(file.path(folder, paste(model_name, '.RDS', sep = ''))))
}
```

```{r}
# Load the CSV files
users = read_csv(
  file.path(data_folder, "users.csv"),
  col_types = cols(
    study_condition = col_factor(),
    noise_level = col_factor(ordered = T),
    age_group = col_factor(levels = seq(from = 0, to = 8)),
    robot_experience = col_factor(levels = seq(from = 0, to = 4))
  )
)

actions = read_csv(
  file.path(data_folder, "actions.csv"),
  col_types = cols(
    study_condition = col_factor(),
    noise_level = col_factor(ordered = T),
    age_group = col_factor(levels = seq(from = 0, to = 8)),
    robot_experience = col_factor(levels = seq(from = 0, to = 4))
  )
)

# Relabel the factors
users$study_condition = mapvalues(users$study_condition,
                                  from = seq(from = 1, to = 10),
                                  to = c("1"="BASELINE",
                                         "2"="DX_100", "3"="AX_100", "4"="DXAX_100",
                                         "5"="DX_90", "6"="AX_90", "7"="DXAX_90",
                                         "8"="DX_80", "9"="AX_80", "10"="DXAX_80"))
actions$study_condition = mapvalues(actions$study_condition,
                                    from = seq(from = 1, to = 10),
                                    to = c("1"="BASELINE",
                                           "2"="DX_100", "3"="AX_100", "4"="DXAX_100",
                                           "5"="DX_90", "6"="AX_90", "7"="DXAX_90",
                                           "8"="DX_80", "9"="AX_80", "10"="DXAX_80"))

# Relevel the non-binary age
users$gender[users$gender == 'U'] = 'M'
actions$gender[actions$gender == 'U'] = 'M'

# Change binary responses to integers
users$scenario_completed = as.integer(users$scenario_completed)
actions$scenario_completed = as.integer(actions$scenario_completed)

actions$optimal_ax = as.integer(actions$optimal_ax)
actions$chose_ax = as.integer(actions$chose_ax)

actions$optimal_dx = as.integer(actions$optimal_dx)
actions$chose_dx = as.integer(actions$optimal_dx)

# Relabel user ids to be in the range 1-200. Otherwise, we're using DB ids
# The user ID column in the users table that we can now join on is X1
actions$user_id = actions %>% group_indices(user_id)

# Relevel the age group factor to remove unused levels. Also, predict the
# value of the 1 unknown age from the other indicators of demographics
# NOTE: If we retrain the model and the output doesn't match the hardcoded
# values, then update those values
if (train_models) {
  plot_df = subset(users, users$age_group != 0)
  plot_df$age_group = factor(plot_df$age_group, levels = seq(from = 0, to = 8), ordered = T)
  plot_df$age_group = droplevels(plot_df$age_group)
  age_group_model = brm(age_group, gender + robot_experience, data = plot_df, family = "cumulative")
  data_to_predict = users %>% filter(age_group == 0) %>% select(c("robot_experience", "gender"))
  as_tibble(predict(age_group_model, data_to_predict))
# # A tibble: 1 x 7
#   `P(Y = 2)` `P(Y = 3)` `P(Y = 4)` `P(Y = 5)` `P(Y = 6)` `P(Y = 7)` `P(Y = 8)`
#        <dbl>      <dbl>      <dbl>      <dbl>      <dbl>      <dbl>      <dbl>
# 1     0.0707      0.230      0.218      0.174      0.105       0.09      0.113
  rm(age_group_model)
}
users[users$age_group == 0,]$age_group = 3
actions[actions$age_group == 0,]$age_group = 3

users$age_group = droplevels(users$age_group)
actions$age_group = droplevels(actions$age_group)

# Code the factor contrasts
contrasts(users$age_group) = contr.deviation(length(levels(users$age_group)))
contrasts(actions$age_group) = contr.deviation(length(levels(actions$age_group)))
contrasts(users$robot_experience) = contr.deviation(length(levels(users$robot_experience)))
contrasts(actions$robot_experience) = contr.deviation(length(levels(actions$robot_experience)))

# Create a non-polynomial noise level factor column
users$noise_level_f = factor(users$noise_level, ordered = F)
contrasts(users$noise_level_f) = contr.deviation(length(levels(users$noise_level_f)))
actions$noise_level_f = factor(actions$noise_level, ordered = F)
contrasts(actions$noise_level_f) = contr.deviation(length(levels(actions$noise_level_f)))
```

## User Level Metrics {.tabset}

### Scenario Completed?

We assume the following structural model

$scenario\_completed \sim ((has\_dx + has\_ax) * noise\_level) + (gender + age\_group + robot\_experience) + num\_optimal$

We use the family of Bernoulli models based on the results in the other notebook where we focused on finding the right distribution for the data.

```{r}
plot_df = users;
```
```{r, eval=train_models}
# scenario_completed.model = glm(
#   scenario_completed ~ ((has_dx + has_ax) * noise_level) + (gender + age_group + robot_experience) + num_optimal,
#   data = plot_df,
#   family = binomial()
# )
# saveModel(scenario_completed.model)

# Create the null model and the model that we're interested in for Bernoulli
# We should not be doing model selection on parameters; model selection on the distributions
# is OK though, I think
scenario_completed.model.1 = brm(
  scenario_completed ~ 1,
  family = "bernoulli",
  data = plot_df,
  cores = 4,
  seed = default_seed,
  save_all_pars = T
)
scenario_completed.model.1 = add_criterion(scenario_completed.model.1, "waic")
scenario_completed.model.1 = add_criterion(scenario_completed.model.1, "loo")
saveModel(scenario_completed.model.1)

scenario_completed.model.2 = brm(
  scenario_completed ~ ((has_dx + has_ax) * noise_level) + (gender + age_group + robot_experience) + num_optimal,
  family = "bernoulli",
  data = plot_df,
  cores = 4,
  seed = default_seed,
  save_all_pars = T
)
scenario_completed.model.2 = add_criterion(scenario_completed.model.2, "waic")
scenario_completed.model.2 = add_criterion(scenario_completed.model.2, "loo")
saveModel(scenario_completed.model.2)

# Print the summary and the comparison of the models
summary(scenario_completed.model.1)
summary(scenario_completed.model.2)
scenario_completed.model.comparison = loo_compare(scenario_completed.model.1,
                                                  scenario_completed.model.2)
scenario_completed.model.comparison %>%
  data.frame() %>%
  rownames_to_column(var = "model") %>%
  ggplot() +
    geom_pointrange(aes(x = reorder(model, -looic), y = looic,
                        ymin = looic - se_looic,
                        ymax = looic + se_looic,
                        color = model),
                    shape = 16) +
    coord_flip() +
    labs(x = NULL, y = NULL, title = "LOO") +
    theme(axis.ticks.y = element_blank(), legend.position = "none")
```
```{r}
ppa =
  pp_average(scenario_completed.model.2, weights = "loo", method = "fitted") %>%
  as_tibble() %>%
  bind_cols(scenario_completed.model.2$data) %>%
  distinct(Estimate, Q2.5, Q97.5, has_ax, noise_level) %>%
  mutate(x_axis = str_c(has_ax, noise_level, sep = "/")) %>%
  mutate(x_axis = factor(x_axis)) %>%
  rename(scenario_completed = Estimate)

d_plot =
  plot_df %>%
  group_by(has_ax, noise_level, id) %>%
  summarise(scenario_completed = mean(scenario_completed)) %>%
  mutate(x_axis = str_c(has_ax, noise_level, sep = "/")) %>%
  mutate(x_axis = factor(x_axis))

ppa %>%
  ggplot(aes(x = x_axis)) +
  geom_smooth(aes(y = scenario_completed, ymin = Q2.5, ymax = Q97.5, group = 0),
              stat = "identity",
              color = "black",
              alpha = 1, size = 1/2) +
  geom_line(data = d_plot,
            aes(y = scenario_completed, group = id),
            size = 1/3) +
  scale_x_discrete(expand = c(.03, .03)) +
  coord_cartesian(ylim = 0:1) +
  labs(x = "has_ax/noise_level", y = "proportion completed") +
  theme(axis.ticks.x = element_blank())

mcmc_pairs(x = posterior_samples(scenario_completed.model.2),
           pars = c("b_Intercept", "b_has_axTRUE", "b_has_axTRUE:noise_level.L", "b_has_axTRUE:noise_level.Q",
                    "b_has_dxTRUE", "b_has_dxTRUE:noise_level.L", "b_has_dxTRUE:noise_level.Q"),
           off_diag_args = list(size = 1/10, alpha = 1/6),
           diag_fun = "dens")
```

```{r, eval=!train_models}
scenario_completed.model = loadModel("scenario_completed.model")
```
```{r}
print(with(scenario_completed.model, pchisq(null.deviance - deviance, df.null - df.residual, lower.tail = FALSE)))
print(summ(scenario_completed.model))
```

Based on the data, only the effect of the ax variable matters

```{r, eval=train_models}
scenario_completed.model.sparse = glm(
  scenario_completed ~ has_ax,
  data = plot_df,
  family = binomial()
)
saveModel(scenario_completed.model.sparse)
```
```{r, eval=!train_models}
scenario_completed.model.sparse = loadModel("scenario_completed.model.sparse")
```
```{r}
print(with(scenario_completed.model.sparse, pchisq(null.deviance - deviance, df.null - df.residual, lower.tail = FALSE)))
print(summ(scenario_completed.model.sparse))
```

So now, we can plot the effects:

```{r}
plot_summs(scenario_completed.model, scenario_completed.model.sparse,
           omit.coefs = c("age_group.L", "age_group.Q", "age_group.C", "age_group^4",
                          "age_group^5", "age_group^6", "age_group^7", "(Intercept)"))
plot_summs(scenario_completed.model.sparse, omit.coefs = NULL, plot.distributions = T)
# ggPredict(scenario_completed.model.sparse, se = T)
```

Then plot the data itself:

```{r}
gg_df = plot_df
gg_df$study_condition = factor(plot_df$study_condition, levels = c("DX_100", "AX_100", "DXAX_100", "DX_90", "AX_90", "DXAX_90", "DX_80", "AX_80", "DXAX_80", "BASELINE"))
gg_df$scenario_completed = factor(plot_df$scenario_completed)
levels(gg_df$scenario_completed) = c("incomplete", "complete")
ggplot(gg_df %>% count(study_condition, scenario_completed),
       aes(study_condition, n, fill=scenario_completed)) +
  geom_bar(stat="identity") +
  theme(axis.text.x = element_text(angle = 45))
```
```{r, message=F, warning=F}
rm(scenario_completed.model)
rm(scenario_completed.model.sparse)
gc()
```


### Num Actions Diff {.tabset}

We assume a structural model where we remove the effect that the action suggestions might have on the number of actions over the number of optimal actions taken by using the prediction of action completion based on the action suggestions

```
num_actions_diff ~ ((has_dx + has_ax) * noise_level) + (gender + age_group + robot_experience) + num_optimal
# Optionally
num_actions_diff ~ ((has_dx + has_ax) * noise_level) + (gender + age_group + robot_experience) + num_optimal + scenario_completed
```

We use a negative binomial distribution to estimate the params, based on tests in the other notebook. First, let's examine without a control for scenario completed.

#### Naive Model

```{r}
plot_df = users;
plot_df$scenario_completed = factor(plot_df$scenario_completed)
```
```{r, eval=train_models}
actions_diff.model = glm.nb(
  num_actions_diff ~ ((has_dx + has_ax) * noise_level) + (gender + age_group + robot_experience) + num_optimal,
  data = plot_df
)
saveModel(actions_diff.model)
```
```{r, eval=!train_models}
actions_diff.model = loadModel("actions_diff.model")
```
```{r}
print(with(actions_diff.model, pchisq(null.deviance - deviance, df.null - df.residual, lower.tail = FALSE)))
print(summ(actions_diff.model))
```

There is a high effect of robot experience and of action suggestions. How does a reduced model fare?

```{r, eval=train_models}
actions_diff.model.sparse = glm.nb(
  num_actions_diff ~ robot_experience + has_ax,
  data = plot_df
)
saveModel(actions_diff.model.sparse)
```
```{r, eval=!train_models}
actions_diff.model.sparse = loadModel("actions_diff.model.sparse")
```
```{r}
print(with(actions_diff.model.sparse, pchisq(null.deviance - deviance, df.null - df.residual, lower.tail = FALSE)))
print(summ(actions_diff.model.sparse))

# Also show comparisons (remove the age factors because there's massive variation in those)
# Turns out that we can't plot the non-sparse model for some reason.
# plot_summs(actions_diff.model, actions_diff.model.sparse,
#            omit.coefs = c("age_group.L", "age_group.Q", "age_group.C", "age_group^4",
#                           "age_group^5", "age_group^6", "age_group^7", "(Intercept)"))
plot_summs(actions_diff.model.sparse, omit.coefs = NULL, plot.distributions = T)
# Cannot plot binomial responses?
plot_df$fitted = actions_diff.model.sparse$fitted.values
ggplot(plot_df, aes(x = robot_experience, y = num_actions_diff, color = has_ax, fill = has_ax)) +
  geom_boxplot(aes(alpha = 0.3)) +
  geom_jitter() +
  geom_line(aes(y = fitted, x = robot_experience, color = has_ax, group = has_ax, size = 2))
```

Unfortunately, while AIC is lower, the R^2 has also gone lower. Let's look at the data.

```{r}
gg_df = plot_df
gg_df$study_condition = factor(plot_df$study_condition, levels = c("DX_100", "AX_100", "DXAX_100", "DX_90", "AX_90", "DXAX_90", "DX_80", "AX_80", "DXAX_80", "BASELINE"))
ggplot(gg_df, aes(x = study_condition, y = num_actions_diff, fill = noise_level)) +
  geom_boxplot(aes(alpha = 0.3)) +
  geom_jitter(aes(color = noise_level)) +
  theme(axis.text.x = element_text(angle = 45))

# Normalize by the number of optimal actions...?
ggplot(gg_df, aes(x = study_condition, y = frac_actions_diff, fill = noise_level)) +
  geom_boxplot(aes(alpha = 0.3)) +
  geom_jitter(aes(color = noise_level)) +
  theme(axis.text.x = element_text(angle = 45))
```

So according to the second plot, there is a significant difference of whether a participant completed the scenario or not (fraction of people at max is its own distribution). So we're going to add that as a factor


#### Augmented Model

This time, we plot the data first:

```{r}
gg_df = subset(plot_df, scenario_completed == "1")
gg_df$study_condition = factor(gg_df$study_condition, levels = c("DX_100", "AX_100", "DXAX_100", "DX_90", "AX_90", "DXAX_90", "DX_80", "AX_80", "DXAX_80", "BASELINE"))
ggplot(gg_df, aes(x = study_condition, y = num_actions_diff, fill = noise_level)) +
  geom_boxplot(aes(alpha = 0.3)) +
  geom_jitter(aes(color = noise_level)) +
  theme(axis.text.x = element_text(angle = 45))
```

```{r}
plot_df = users;
plot_df$scenario_completed = factor(plot_df$scenario_completed)
```
```{r, eval=train_models}
actions_diff.model.augmented = glm.nb(
  num_actions_diff ~ ((has_dx + has_ax) * noise_level) + (gender + age_group + robot_experience) + num_optimal + scenario_completed,
  data = plot_df
)
saveModel(actions_diff.model.augmented)
```
```{r, eval=!train_models}
actions_diff.model.augmented = loadModel("actions_diff.model.augmented")
```
```{r}
print(with(actions_diff.model.augmented, pchisq(null.deviance - deviance, df.null - df.residual, lower.tail = FALSE)))
print(summ(actions_diff.model.augmented))
```

There is indeed a large effect of the scenario completed; and controlling for it seems to negate the effect of whether action suggestions are present. Creating the sparser model from this full(er) model based on the factors that are deemed significant

```{r, eval=train_models}
actions_diff.model.augmented.sparse = glm.nb(
  num_actions_diff ~ gender + robot_experience + scenario_completed + has_dx*noise_level,
  data = plot_df
)
saveModel(actions_diff.model.augmented.sparse)
```
```{r, eval=!train_models}
actions_diff.model.augmented.sparse = loadModel("actions_diff.model.augmented.sparse")
```
```{r}
print(with(actions_diff.model.augmented.sparse, pchisq(null.deviance - deviance, df.null - df.residual, lower.tail = FALSE)))
print(summ(actions_diff.model.augmented.sparse))

# Plot the coefficients. For some unknown reason, we cannot plot this model either
plot_summs(actions_diff.model.augmented.sparse, plot.distributions = T)

# Plot the DX and noise fit
plot_df$fitted = actions_diff.model.augmented.sparse$fitted.values
plot_df$scenario_completed = factor(plot_df$scenario_completed)
ggplot(plot_df, aes(x = gender, y = num_actions_diff, color = scenario_completed, fill = scenario_completed)) +
  geom_boxplot(aes(alpha = 0.3)) +
  geom_jitter() +
  geom_line(aes(y = fitted, x = gender, color = scenario_completed, group = scenario_completed, size = 2))
```

```{r, message=F, warning=F}
rm(actions_diff.model, actions_diff.model.sparse, actions_diff.model.augmented, actions_diff.model.augmented.sparse)
gc()
```


## Action Level Metrics {.tabset}

### Taking Optimal Actions

One can almost think of this as an inverse of the "reliance" metric mentioned by Jessie Yang et al. We're going to need brms to fit all the models here.

```{r}
gg_df = users
gg_df$study_condition = factor(gg_df$study_condition, levels = c("DX_100", "AX_100", "DXAX_100", "DX_90", "AX_90", "DXAX_90", "DX_80", "AX_80", "DXAX_80", "BASELINE"))
ggplot(gg_df, aes(x = study_condition, y = num_ax_optimal, fill = noise_level)) +
  geom_boxplot(aes(alpha = 0.3)) +
  # geom_jitter(aes(color = noise_level)) +
  theme(axis.text.x = element_text(angle = 45))

# This is technically what the binomial regression is checking
ggplot(gg_df, aes(x = study_condition, y = frac_ax_optimal, fill = noise_level)) +
  geom_boxplot(aes(alpha = 0.3)) +
  # geom_jitter(aes(color = noise_level)) +
  theme(axis.text.x = element_text(angle = 45))
```

The choice of the model family is informed by the tests in the second markdown file.

```{r}
plot_df = actions
```
```{r, eval=train_models}
# Fit the main model
optimal_action.model = brm(
  optimal_ax | trials(num_actions) ~ ((has_dx + has_ax) * noise_level) + (age_group + robot_experience + gender) + state_idx + (1 | user_id),
  data = plot_df,
  family = "binomial",
  cores = 4
)
optimal_action.model = add_criterion(optimal_action.model, "waic")
optimal_action.model = add_criterion(optimal_action.model, "loo")
saveModel(optimal_action.model)
```
```{r, eval=!train_models}
optimal_action.model = loadModel("optimal_action.model")
```
```{r}
print(summary(optimal_action.model))
print(optimal_action.model$criteria$loo$estimates)
```

```{r}
# Plot the effects of the condition (the stuff we care about)
hyp_to_test = c("has_axTRUE = 0", "has_dxTRUE = 0",
                "noise_level.Q = 0", "noise_level.L = 0",
                "has_axTRUE:noise_level.Q = 0", "has_axTRUE:noise_level.L = 0",
                "has_dxTRUE:noise_level.Q = 0", "has_dxTRUE:noise_level.L = 0")
hyp_results = hypothesis(optimal_action.model, hypothesis = hyp_to_test)
print(as.matrix(hyp_results$hypothesis))
plot(hyp_results, ask = F)

mcmc_areas(as.matrix(optimal_action.model), regex_pars = "has_|noise_", prob = 0.95, prob_outer = 1)
mcmc_intervals(as.matrix(optimal_action.model), regex_pars = "b_", prob = 0.90, prob_outer = 0.95)
```

Based on the plots, it seems that a reduced model for this model might include state_idx, robot_experience, age_group, and has_ax.

```{r, eval=train_models}
optimal_action.model.sparse = brm(
  optimal_ax | trials(num_actions) ~ has_ax + age_group + robot_experience + state_idx + (1 | user_id),
  data = plot_df,
  family = "binomial",
  cores = 4,
)
optimal_action.model.sparse = add_criterion(optimal_action.model.sparse, "waic")
optimal_action.model.sparse = add_criterion(optimal_action.model.sparse, "loo")
saveModel(optimal_action.model.sparse)
```
```{r, eval=!train_models}
optimal_action.model.sparse = loadModel("optimal_action.model.sparse")
```
```{r}
print(summary(optimal_action.model.sparse))
print(optimal_action.model.sparse$criteria$loo$estimates)
print(loo_compare(optimal_action.model, optimal_action.model.sparse))

# Plot the effects of the condition (the stuff we care about)
hyp_to_test = c("has_axTRUE = 0")
hyp_results = hypothesis(optimal_action.model.sparse, hypothesis = hyp_to_test)
print(as.matrix(hyp_results$hypothesis))
plot(hyp_results, ask = F)

mcmc_areas(as.matrix(optimal_action.model.sparse), regex_pars = "has_|noise_", prob = 0.95, prob_outer = 1)
mcmc_intervals(as.matrix(optimal_action.model.sparse), regex_pars = "b_", prob = 0.90, prob_outer = 0.95)
```

So there are three independent effects; get the conditional plots for those

```{r}
plot(conditional_effects(optimal_action.model.sparse, effects = "has_ax"), ask = F, points = T)
plot(conditional_effects(optimal_action.model.sparse, effects = "robot_experience"), ask = F, points = T)
plot(conditional_effects(optimal_action.model.sparse, effects = "age_group"), ask = F, points = T)
```
```{r}
# We want to create our own conditional sample, but don't want to kill our machine
plot_df$state_idx = mean(plot_df$state_idx)
g = subset(plot_df, age_group == 0 | age_group == 8) %>%
  data_grid(num_actions, has_ax, age_group, robot_experience, state_idx, user_id)
fits = g %>% add_fitted_draws(optimal_action.model.sparse, n=10)
preds = g %>% add_predicted_draws(optimal_action.model.sparse, n=10)

# Binary responses are obvious
plot_df %>%
  ggplot(aes(y = has_ax, x = optimal_ax / num_actions)) +
  stat_halfeyeh(aes(x = .value / num_actions), position = position_nudge(y = 0.1), data = fits) +
  stat_intervalh(aes(x = .prediction / num_actions), data = preds) +
  geom_point(data = plot_df, color = 'gray')

# For multi-level responses, we should do a pairwise comparison
plot_df %>%
  ggplot(aes(y = robot_experience, x = optimal_ax / num_actions)) +
  stat_halfeyeh(aes(x = .value / num_actions), position = position_nudge(y = 0.1), data = fits) +
  stat_intervalh(aes(x = .prediction / num_actions), data = preds) +
  geom_point(data = plot_df, color = 'gray')

# I don't understand why there are more than 2 age groups... is it because of the sampling process?
plot_df %>%
  ggplot(aes(y = age_group, x = optimal_ax / num_actions)) +
  stat_halfeyeh(aes(x = .value / num_actions), position = position_nudge(y = 0.1), data = fits) +
  stat_intervalh(aes(x = .prediction / num_actions), data = preds) +
  geom_point(data = plot_df, color = 'gray')
```

```{r, message=F, warning=F}
rm(optimal_action.model, optimal_action.model.sparse)
gc()
```


### Following Suggestions of Actions


```{r}
gg_df = subset(users, has_ax)
gg_df$study_condition = factor(gg_df$study_condition, levels = c("DX_100", "AX_100", "DXAX_100", "DX_90", "AX_90", "DXAX_90", "DX_80", "AX_80", "DXAX_80", "BASELINE"))
ggplot(gg_df, aes(x = study_condition, y = num_ax_followed, fill = noise_level)) +
  geom_boxplot(aes(alpha = 0.3)) +
  # geom_jitter(aes(color = noise_level)) +
  theme(axis.text.x = element_text(angle = 45))

# This is technically what the binomial regression is checking
ggplot(gg_df, aes(x = study_condition, y = frac_ax_followed, fill = noise_level)) +
  geom_boxplot(aes(alpha = 0.3)) +
  # geom_jitter(aes(color = noise_level)) +
  theme(axis.text.x = element_text(angle = 45))
```

The choice of the model family is informed by the tests in the second markdown file.

```{r}
plot_df = subset(actions, has_ax)
```
```{r, eval=train_models}
# Fit the main model
follow_action.model = brm(
  chose_ax | trials(num_actions) ~ (has_dx * noise_level) + (age_group + robot_experience + gender) + state_idx + (1 | user_id),
  data = plot_df,
  family = "binomial",
  cores = 4
)
follow_action.model = add_criterion(follow_action.model, "waic")
follow_action.model = add_criterion(follow_action.model, "loo")
saveModel(follow_action.model)
```
```{r, eval=!train_models}
follow_action.model = loadModel("follow_action.model")
```
```{r}
print(summary(follow_action.model))
print(follow_action.model$criteria$loo$estimates)
```

```{r}
# Plot the effects of the condition (the stuff we care about)
hyp_to_test = c("has_dxTRUE = 0",
                "noise_level.Q = 0", "noise_level.L = 0",
                "has_dxTRUE:noise_level.Q = 0", "has_dxTRUE:noise_level.L = 0")
hyp_results = hypothesis(follow_action.model, hypothesis = hyp_to_test)
print(as.matrix(hyp_results$hypothesis))
plot(hyp_results, ask = F)

mcmc_areas(as.matrix(follow_action.model), regex_pars = "has_|noise_", prob = 0.95, prob_outer = 1)
mcmc_intervals(as.matrix(follow_action.model), regex_pars = "b_", prob = 0.90, prob_outer = 0.95)
```

Based on the plots, it seems that a reduced model for this model might include state_idx, robot_experience, age_group, and gender

```{r, eval=train_models}
follow_action.model.sparse = brm(
  chose_ax | trials(num_actions) ~ gender + age_group + robot_experience + state_idx + (1 | user_id),
  data = plot_df,
  family = "binomial",
  cores = 4,
)
follow_action.model.sparse = add_criterion(follow_action.model.sparse, "waic")
follow_action.model.sparse = add_criterion(follow_action.model.sparse, "loo")
saveModel(follow_action.model.sparse)
```
```{r, eval=!train_models}
follow_action.model.sparse = loadModel("follow_action.model.sparse")
```
```{r}
print(summary(follow_action.model.sparse))
print(follow_action.model.sparse$criteria$loo$estimates)
print(loo_compare(follow_action.model, follow_action.model.sparse))

# Our IV is not part of this condition
# hyp_to_test = c("has_axTRUE = 0")
# hyp_results = hypothesis(follow_action.model.sparse, hypothesis = hyp_to_test)
# print(as.matrix(hyp_results$hypothesis))
# plot(hyp_results, ask = F)

# mcmc_areas(as.matrix(follow_action.model.sparse), regex_pars = "has_|noise_", prob = 0.95, prob_outer = 1)
mcmc_intervals(as.matrix(follow_action.model.sparse), regex_pars = "b_", prob = 0.90, prob_outer = 0.95)
```

So there are three independent effects; get the conditional plots for those

```{r}
plot(conditional_effects(follow_action.model.sparse, effects = "gender"), ask = F, points = T)
plot(conditional_effects(follow_action.model.sparse, effects = "robot_experience"), ask = F, points = T)
plot(conditional_effects(follow_action.model.sparse, effects = "age_group"), ask = F, points = T)
```
```{r}
# We want to create our own conditional sample, but don't want to kill our machine
plot_df$state_idx = mean(plot_df$state_idx)
g = subset(plot_df, age_group == 0 | age_group == 8) %>%
  data_grid(num_actions, gender, age_group, robot_experience, state_idx, user_id)
fits = g %>% add_fitted_draws(follow_action.model.sparse, n=10)
preds = g %>% add_predicted_draws(follow_action.model.sparse, n=10)

# Binary responses are obvious
plot_df %>%
  ggplot(aes(y = gender, x = chose_ax / num_actions)) +
  stat_halfeyeh(aes(x = .value / num_actions), position = position_nudge(y = 0.1), data = fits) +
  stat_intervalh(aes(x = .prediction / num_actions), data = preds) +
  geom_point(data = plot_df, color = 'gray')

# For multi-level responses, we should do a pairwise comparison
plot_df %>%
  ggplot(aes(y = robot_experience, x = chose_ax / num_actions)) +
  stat_halfeyeh(aes(x = .value / num_actions), position = position_nudge(y = 0.1), data = fits) +
  stat_intervalh(aes(x = .prediction / num_actions), data = preds) +
  geom_point(data = plot_df, color = 'gray')

# I don't understand why there are more than 2 age groups... is it because of the sampling process?
plot_df %>%
  ggplot(aes(y = age_group, x = chose_ax / num_actions)) +
  stat_halfeyeh(aes(x = .value / num_actions), position = position_nudge(y = 0.1), data = fits) +
  stat_intervalh(aes(x = .prediction / num_actions), data = preds) +
  geom_point(data = plot_df, color = 'gray')
```

```{r, message=F, warning=F}
rm(follow_action.model, follow_action.model.sparse)
gc()
```
