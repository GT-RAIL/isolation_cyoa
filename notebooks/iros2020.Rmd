---
title: "RO-MAN 2020"
output:
  html_document:
    # theme: readable
    code_folding: hide
    df_print: tibble
    toc: true
    toc_depth: 2
    number_sections: true
---

```{r, message=F, warning=F}
library(car)
library(stats)
library(grid)
library(gridExtra)
library(fitdistrplus)
# library(dplyr)
# library(forcats)
# library(tibble)
# library(tidyr)
# library(readr)
# library(ggplot2)
# library(stringr)
library(tidyverse)
library(ggsignif)
library(ggthemes)
library(jtools)
library(broom)
library(modelr)
library(loo)
# library(rstanarm)
library(brms)
library(bayesplot)
library(GGally)
library(tidybayes)
library(bayestestR)
library(sjstats)
library(report)
library(see)

# Setup for multiprocessing
options(mc.cores = 4)
# library(future)
# plan(multiprocess)

# Make the default coding of contrasts sum-coding; just in case
options(contrasts = c("contr.sum", "contr.poly"))
```

Global options:

```{r}
# Script execution globals
train_models = F     # Retrain the MCMC models
plot_posteriors = T  # Plot the posterior distributions
plot_diagnostics = F # Plot the diagnostics
report_bayesfactors = F  # Report the Bayes Factor scores of model parameters
default_seed = 0x1331
data_folder = "~/Documents/GT/Research/Data/arbitration/2019-12-09/results"
```

Helper functions:

```{r, message=FALSE, warning=FALSE}
# Create a simple coding contrasts matrix
contr.deviation = function(nlevels) {
  x1 = contr.treatment(nlevels)
  x2 = matrix(rep(1/nlevels, (nlevels-1) * nlevels), ncol = (nlevels-1))
  return(x1-x2)
}

# NOTE: For now we're ignoring this advice
# # Use a simple coding contrasts matrix to add our own contrasts columns
# # Explained in: http://talklab.psy.gla.ac.uk/tvw/catpred/
# create_simple_contrasts = function(d, column_name) {
#  factor_column = d %>% pull(column_name)
#  contrast_matrix = contr.deviation(length(levels(factor_column)))
# }

# Save a model
saveModel = function(model, folder = data_folder) {
  saveRDS(model, file = file.path(folder, paste(substitute(model), ".rds", sep = '')))
}

# Load a model
loadModel = function(model_name, folder = data_folder) {
  return(readRDS(file.path(folder, paste(model_name, '.rds', sep = ''))))
}

# Test hypotheses. The NULL arguments need to be provided in this case
test_hypotheses = function(h_df = NULL, rope_values = NULL, hypotheses_list = NULL, model = NULL) {
  # Option 1: Use the hypothesis function in brms
  if (!is.null(hypotheses_list)) {
    hyp_results = hypothesis(model, hypothesis = hypotheses_list, seed = default_seed)
    return(hyp_results)
  }

  # Option 2: Use the ROPE & Overlap amount
  else {
    if (is.null(rope_values)) {
      rope_values = c(-0.1, 0.1)
    }
    hyp_results =
      h_df %>%
        equivalence_test(range = rope_values, ci = 1.0) %>%
        as_tibble() %>%
        bind_cols(h_df %>% pd() %>% select(pd))
    return(hyp_results)
  }
}

# Fit distributions to vectors and print the results. Also return the fit, if necessary
fit_and_print_dist = function(vec, distr, string, use_mass = F) {
  if (!use_mass) {
    f = fitdist(vec, distr)
  } else {
    f = MASS::fitdistr(vec, distr)
  }
  print(paste(string, f$loglik, f$aic))
  return(f)
}
```

Data loading:

```{r, message=F, warning=F}
# Load the CSV files. If the age_group fill model is run again and we get a different output,
# then remember to update the value here
loadCSV = function(filename, age_group_fill, contrast_func) {
  dat = read_csv(
    file.path(data_folder, filename),
    col_types = cols(
      study_condition = col_factor(),
      noise_level = col_factor(ordered = T),
      gender = col_factor(levels = c("F", "M", "U")),
      age_group = col_factor(levels = seq(from = 0, to = 8)),
      robot_experience = col_factor(levels = seq(from = 0, to = 4))
    )
  )

  # Relabel the factors
  dat = dat %>%
    mutate(study_condition = fct_recode(study_condition,
                                        BASELINE="1",
                                        DX_100="2", AX_100="3", DXAX_100="4",
                                        DX_90="5", AX_90="6", DXAX_90="7",
                                        DX_80="8", AX_80="9", DXAX_80="10")) %>%
    mutate(study_condition = fct_relevel(study_condition, c("DX_100", "AX_100", "DXAX_100",
                                                            "DX_90", "AX_90", "DXAX_90",
                                                            "DX_80", "AX_80", "DXAX_80")))

  # Relevel the non-binary gender
  dat$gender[dat$gender == 'U'] = 'M'

  # Change binary responses to integers
  dat$scenario_completed = as.integer(dat$scenario_completed)
  
  # Relevel age_group
  dat[dat$age_group == 0,]$age_group = age_group_fill
  
  # Create an unordered noise_level. Also drop unused levels
  dat$age_group = droplevels(dat$age_group)
  dat$gender = droplevels(dat$gender)
  dat$noise_level_f = factor(dat$noise_level, ordered = F)

  # Set the contrasts for everything
  contrasts(dat$age_group) = contrast_func(length(levels(dat$age_group)))
  contrasts(dat$robot_experience) = contrast_func(length(levels(dat$robot_experience)))
  contrasts(dat$gender) = contrast_func(length(levels(dat$gender)))

  contrasts(dat$has_ax) = contrast_func(2)
  contrasts(dat$has_dx) = contrast_func(2)
  contrasts(dat$has_noise) = contrast_func(2)
  contrasts(dat$has_dxax) = contrast_func(2)

  contrasts(dat$noise_level_f) = contrast_func(length(levels(dat$noise_level_f)))
  
  # Return the data frame
  return(dat)
}

# Get the users df and the actions df
users = loadCSV("users.csv", 3, contr.sum)
actions = loadCSV("actions.csv", 3, contr.sum)

# Change more binary responses to integers
actions$optimal_ax = as.integer(actions$optimal_ax)
actions$chose_ax = as.integer(actions$chose_ax)
actions$optimal_dx = as.integer(actions$optimal_dx)
actions$chose_dx = as.integer(actions$optimal_dx)

# Relabel user ids to be in the range 1-200. Otherwise, we're using DB ids
# The user ID column in the users table that we can now join on is X1
actions$user_id = actions %>% group_indices(user_id)

# Rescale state ids
actions$state_idx_rescaled = scales::rescale(actions$state_idx)

# Code to relevel the age group factor to remove unused levels. Also, predict the
# value of the 1 unknown age from the other indicators of demographics
# NOTE: If we retrain the model and the output doesn't match the hardcoded
# values, then update those values
if (train_models) {
  plot_df = subset(users, users$age_group != 0)
  plot_df$age_group = factor(plot_df$age_group, levels = seq(from = 0, to = 8), ordered = T)
  plot_df$age_group = droplevels(plot_df$age_group)
  age_group_model = brm(age_group ~ gender + robot_experience, data = plot_df, family = "cumulative")
  data_to_predict = users %>% filter(age_group == 0) %>% select(c("robot_experience", "gender"))
  as_tibble(predict(age_group_model, data_to_predict))
# # A tibble: 1 x 7
#   `P(Y = 2)` `P(Y = 3)` `P(Y = 4)` `P(Y = 5)` `P(Y = 6)` `P(Y = 7)` `P(Y = 8)`
#        <dbl>      <dbl>      <dbl>      <dbl>      <dbl>      <dbl>      <dbl>
# 1     0.0707      0.230      0.218      0.174      0.105       0.09      0.113
  rm(age_group_model)
}
```

For each of our dependent variables, there are 2 models:

1. With the noise variable as an ordered factor, so that we can make inferences on the trends in the variable
1. With the noise variable as an unordered factor, so that we can make inferences on the values of the variable

We use Deviation / Sum coding in the coding of non-ordered factors so that we can test the influence of individual factor levels over and above the grand mean.

In all our models:

- $\beta_0$ is assumed to be the intercept. In a mixed effects model, this is indexed by $i$, the user; i.e. $\beta_{0i}$.
- $\mathbf{X_{demo,i}}$ is a vector of demographic information
- $\text{ax}_i$ denotes if sample $i$ received AX suggestions or not
- $\text{dx}_i$ denotes if sample $i$ received DX suggestions or not
- $\text{noise}_i$ denotes the level of noise in the suggestions that sample $i$ received. 0 if none was present.
- $\text{no}_i$ denotes the number of optimal actions for the scenario present in sample $i$. This is a proxy for a factor encoding of the start scenario
- $\text{scenario_completed}_i$ denotes whether sample $i$ completed the scenario or not. When used as a predictor, this is a binary factor; when used as a dependent variable, this is a probability of the user completing the scenario.
- $\text{state}_{ij}$ denotes the state the user $i$ visited on action number $j$. The sample, in this case, is indexed by $j$. The states are indexed according to the frequency of users visits (0 = most visited state), and then all the indices are rescaled into the range 0-1.

We test the following hypotheses (the explanations are a statement of the null hypotheses; the coefficients are from the expected regression parameters, given sum coding):

- $(\beta_0 - \beta_{ax_0}) - (\beta_0 + \beta_{ax_0}) = -2\beta_{ax_0} = 0 \Rightarrow \beta_{ax_0} = 0$: The main difference in effects from having action suggestions vs. not is not negligible (ceterus paribus)
- $(\beta_0 - \beta_{dx_0}) - (\beta_0 + \beta_{dx_0}) = -2\beta_{dx_0} = 0 \Rightarrow \beta_{dx_0} = 0$: The main difference in effects from having diagnosis suggestions vs. not is not neglible (ceterus paribus)
- $\beta_{noise_0} = 0$: The main effect of no noise in suggestions is not different from averaging the effects of noise.
- $\beta_{noise_1} = 0$: The main effect of level 1 noise in suggestions is not different from averaging the effects of noise.
- $- \beta_{noise_0} - \beta_{noise_1} = 0$: The main effect of level 2 noise in suggestions is not different from averaging the effect of noise.
- $\beta_{noise_L} = 0; \beta_{noise_Q} = 0$: Noise does not have a linear / quadratic effect on the outcome.

We do not test the interaction effects (because it's hard to make sense of what those mean).

The method of reporting and testing is based on the following papers:

1. [A protocol for conducting and presenting results of regression‐type analyses](https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.12577)
1. [Indices of Effect Existence and Significance in the Bayesian Framework](https://www.frontiersin.org/articles/10.3389/fpsyg.2019.02767/full). The paper is associated with [this post](https://easystats.github.io/bayestestR/articles/guidelines.html) on how to present results, and [this post](https://easystats.github.io/bayestestR/articles/region_of_practical_equivalence.html) giving a quick overview of terms (the posts are part of a package I'm using heavily in these analyses)

Note, that unlike the previous version of this HTML page, and some of the references, we are NOT going to perform model-selection here. Based on what I've read, we're doing confirmatory hypothesis testing, which is not where one should use model selection paradigms.


# User Level Metrics {.tabset}

These are metrics where an individual unit of analysis is the user.

## Scenario Completed?

**Did the person complete the scenario or not?**

```{r}
plot_df = users %>%
  select(X1, id, study_condition, start_condition, num_optimal,
         age_group, gender, robot_experience,
         noise_level, noise_level_f, has_noise, has_dx, has_ax,
         scenario_completed)
text_short(report(plot_df))
```

```{r fig.height=7, fig.width=15}
# Plot by the study condition
plot_df %>%
  mutate(scenario_completed = factor(scenario_completed)) %>%
  mutate(scenario_completed = fct_recode(scenario_completed, incomplete="0", complete = "1")) %>%
  count(study_condition, scenario_completed) %>%
  ggplot(aes(study_condition, n / 20, fill=scenario_completed)) +
    geom_bar(stat="identity") +
    labs(y = "Fraction completed") +
    scale_fill_economist()

# Visualize the data by the three variables that we care about
gg_df = plot_df %>%
  mutate(scenario_completed = factor(scenario_completed)) %>%
  mutate(scenario_completed = fct_recode(scenario_completed, incomplete="0", complete = "1")) %>%
  mutate(noise_level = fct_rev(noise_level)) %>%
  count(has_ax, has_dx, noise_level, scenario_completed)

p1 = gg_df %>%
  ggplot(aes(has_ax, n / 20, fill=scenario_completed)) +
    geom_bar(stat = "identity") +
    facet_grid(rows = vars(noise_level), cols = vars(has_dx), labeller = label_both) +
    labs(y = "Fraction completed") +
    legend_none() +
    scale_fill_economist()

p2 = gg_df %>%
  ggplot(aes(has_dx, n / 20, fill=scenario_completed)) +
    geom_bar(stat = "identity") +
    facet_grid(rows = vars(noise_level), cols = vars(has_ax), labeller = label_both) +
    labs(y = "Fraction completed") +
    theme(legend.position = "bottom") +
    scale_fill_economist()

p3 = gg_df %>%
  mutate(noise_level = fct_rev(noise_level)) %>%
  mutate(has_ax = factor(has_ax)) %>%
  mutate(has_ax = fct_rev(has_ax)) %>%
  ggplot(aes(noise_level, n / 20, fill=scenario_completed)) +
    geom_bar(stat = "identity") +
    facet_grid(rows = vars(has_ax), cols = vars(has_dx), labeller = label_both) +
    labs(y = "Fraction completed") +
    legend_none() +
    scale_fill_economist()

grid.arrange(p1, p2, p3, ncol = 3)

# Visualize also the variation in each of the three levels that we care about
p1 = gg_df %>%
  filter(scenario_completed == 'complete') %>%
  ggplot(aes(has_ax, n/20)) +
    geom_violin() +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep=":"), size = 2)) +
    stat_summary(fun.y = median, geom="point", shape=23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(color = "AX:DX:Noise") +
    theme(legend.position = "left") +
    guides(size = F)

p2 = gg_df %>%
  filter(scenario_completed == 'complete') %>%
  ggplot(aes(has_dx, n/20)) +
    geom_violin() +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep=":"), size = 2)) +
    stat_summary(fun.y = median, geom="point", shape=23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    legend_none()

p3 = gg_df %>%
  filter(scenario_completed == 'complete') %>%
  ggplot(aes(noise_level, n/20)) +
    geom_violin() +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep=":"), size = 2)) +
    stat_summary(fun.y = median, geom="point", shape=23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    scale_x_discrete(limits = rev(levels(gg_df$noise_level))) +
    legend_none()

grid.arrange(p1, p2, p3, ncol = 3)
```

Based on the data, we assume the following structural model:

$$scenario\_completed_i = Bernoulli(p_i)$$
$$\begin{aligned}
logit(p_i) &= \beta_0 + \beta_{ax}\text{ax}_i + \beta_{dx}\text{dx}_i + \beta_{noise}\text{noise}_i +\\ &\beta_{ax:noise}\text{ax}_i\text{noise}_i + \beta_{dx:noise}\text{dx}_i\text{noise}_i + \\
&\beta_{no}\text{no}_i + \mathbf{\beta_{demo}X_{demo,i}}
\end{aligned}$$
$$\beta_{.} \sim Normal(0, 10)$$

Model fitting:

```{r, eval=train_models}
# A null model to compare against
scenario_completed.model.null = brm(
  scenario_completed ~ 0 + Intercept,
  family = "bernoulli",
  prior = set_prior("normal(0, 10)", class = "b"),
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
scenario_completed.model.null = add_criterion(scenario_completed.model.null, "waic")
scenario_completed.model.null = add_criterion(scenario_completed.model.null, "loo", reloo = T)
saveModel(scenario_completed.model.null)

# The trend model to see if there is a trend in the noise level variable
scenario_completed.model.t = brm(
  scenario_completed ~ 0 + Intercept + ((has_dx + has_ax) * noise_level) + (gender + age_group + robot_experience) + num_optimal,
  family = "bernoulli",
  prior = set_prior("normal(0, 10)", class = "b"),
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
scenario_completed.model.t = add_criterion(scenario_completed.model.t, "waic")
scenario_completed.model.t = add_criterion(scenario_completed.model.t, "loo", reloo = T)
saveModel(scenario_completed.model.t)

# The values model, to see if specific values of the noise level variable are significant
scenario_completed.model.v = brm(
  scenario_completed ~ 0 + Intercept + ((has_dx + has_ax) * noise_level_f) + (gender + age_group + robot_experience) + num_optimal,
  family = "bernoulli",
  prior = set_prior("normal(0, 10)", class = "b"),
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
scenario_completed.model.v = add_criterion(scenario_completed.model.v, "waic")
scenario_completed.model.v = add_criterion(scenario_completed.model.v, "loo", reloo = T)
saveModel(scenario_completed.model.v)
# More model plots are available at:
# https://bookdown.org/ajkurz/Statistical_Rethinking_recoded/counting-and-classification.html

# Attempted RStanarm models: The estimate contrasts doesn't seem to be as helpful?
# scenario_completed.model.v = stan_glm(
#   scenario_completed ~ ((has_dx + has_ax) * noise_level) + (gender + age_group + robot_experience) + num_optimal,
#   family = binomial(link = "logit"),
#   prior = normal(0, 10),
#   prior_intercept = normal(0, 10),
#   data = plot_df,
#   QR = T,
#   seed = default_seed
# )
```
```{r, eval=!train_models}
# Load the models
scenario_completed.model.null = loadModel("scenario_completed.model.null")
scenario_completed.model.t = loadModel("scenario_completed.model.t")
scenario_completed.model.v = loadModel("scenario_completed.model.v")
```

Model fitting results:

```{r}
# print(summary(scenario_completed.model.null))
print(performance::r2(scenario_completed.model.null))

# Print the parameters, and some initial diagnostics
print(tidy_stan(scenario_completed.model.t))
print(performance::r2(scenario_completed.model.t))

print(tidy_stan(scenario_completed.model.v))
print(performance::r2(scenario_completed.model.v))
```

Diagnostics of the fit:

```{r fig.height=7, fig.width=15, eval=plot_diagnostics}
# Alternative diagnostics: diagnostic_posterior(scenario_completed.model.v)

# LOO cross-validation results
print(scenario_completed.model.null$criteria$loo$estimates)
print(scenario_completed.model.t$criteria$loo$estimates)
print(scenario_completed.model.v$criteria$loo$estimates)
print(loo_compare(scenario_completed.model.null,
                  scenario_completed.model.t,
                  scenario_completed.model.v))

# Diagnostics of the sampling process
par(mfrow=c(1,2))
plot(scenario_completed.model.t$criteria$loo, main = "Trend Model")
plot(scenario_completed.model.v$criteria$loo, main = "Values Model")

# Include posterior predictive checks of the sampling (are sampling according
# to the data that we actually have?)
mcmc_intervals(as.matrix(scenario_completed.model.null), regex_pars = "b_", prob = 0.90, prob_outer = 0.95) +
  ggtitle("Null Model")

p1 = pp_check(scenario_completed.model.t, type = "bars_grouped", group = "has_ax", stat = "median") + ggtitle("t:has_ax")
p2 = pp_check(scenario_completed.model.t, type = "bars_grouped", group = "has_dx") + ggtitle("t:has_dx")
p3 = pp_check(scenario_completed.model.t, type = "bars_grouped", group = "noise_level") + ggtitle("t:noise_level")
grid.arrange(p1, p2, p3, nrow = 3)
mcmc_intervals(as.matrix(scenario_completed.model.t), regex_pars = "b_", prob = 0.90, prob_outer = 0.95) +
  ggtitle("Trends Model")

p1 = pp_check(scenario_completed.model.v, type = "bars_grouped", group = "has_ax") + ggtitle("v:has_ax")
p2 = pp_check(scenario_completed.model.v, type = "bars_grouped", group = "has_dx") + ggtitle("v:has_dx")
p3 = pp_check(scenario_completed.model.v, type = "bars_grouped", group = "noise_level_f") + ggtitle("v:noise_level")
grid.arrange(p1, p2, p3, nrow = 3)
mcmc_intervals(as.matrix(scenario_completed.model.v), regex_pars = "b_", prob = 0.90, prob_outer = 0.95) +
  ggtitle("Values Model")

# Plot predictions vs original
preds_p_df = scenario_completed.model.null %>% predict() %>% as_tibble()
p1 =
  preds_p_df %>%
    add_column(scenario_completed = scenario_completed.model.null$data$scenario_completed) %>%
    ggplot(aes(x = Estimate, y = scenario_completed, color = scenario_completed)) +
      geom_point() +
      labs(x = "Predicted values") +
      ylim(0, 1) +
      xlim(0, 1) +
      ggtitle("Null Model")

preds_p_df = plot_df %>%
  add_predicted_draws(scenario_completed.model.null) %>%
  group_by(scenario_completed, .prediction) %>%
  count()
p2 =
  preds_p_df %>%
    ggplot(aes(x = scenario_completed, y = .prediction, fill = n / sum(preds_p_df$n))) +
      geom_tile() +
      geom_text(aes(label = sprintf("%1.2f", n / sum(preds_p_df$n))), size = 10, vjust = 1) +
      scale_fill_gradient(limits = c(0, 1))

preds_p_df = scenario_completed.model.t %>% predict() %>% as_tibble()
p3 =
  preds_p_df %>%
    add_column(scenario_completed = scenario_completed.model.t$data$scenario_completed) %>%
    ggplot(aes(x = Estimate, y = scenario_completed, color = scenario_completed)) +
      geom_point() +
      labs(x = "Predicted values") +
      ylim(0, 1) +
      xlim(0, 1) +
      ggtitle("Trends Model")

preds_p_df = plot_df %>%
  add_predicted_draws(scenario_completed.model.t) %>%
  group_by(scenario_completed, .prediction) %>%
  count()
p4 =
  preds_p_df %>%
    ggplot(aes(x = scenario_completed, y = .prediction, fill = n / sum(preds_p_df$n))) +
      geom_tile() +
      geom_text(aes(label = sprintf("%1.2f", n / sum(preds_p_df$n))), size = 10, vjust = 1) +
      scale_fill_gradient(limits = c(0, 1))

# Plot predictions vs original
preds_p_df = scenario_completed.model.v %>% predict() %>% as_tibble()
p5 =
  preds_p_df %>%
    add_column(scenario_completed = scenario_completed.model.v$data$scenario_completed) %>%
    ggplot(aes(x = Estimate, y = scenario_completed, color = scenario_completed)) +
      geom_point() +
      labs(x = "Predicted values") +
      ylim(0, 1) +
      xlim(0, 1) +
      ggtitle("Values Model")

preds_p_df = plot_df %>%
  add_predicted_draws(scenario_completed.model.v) %>%
  group_by(scenario_completed, .prediction) %>%
  count()
p6 =
  preds_p_df %>%
    ggplot(aes(x = scenario_completed, y = .prediction, fill = n / sum(preds_p_df$n))) +
      geom_tile() +
      geom_text(aes(label = sprintf("%1.2f", n / sum(preds_p_df$n))), size = 10, vjust = 1) +
      scale_fill_gradient(limits = c(0, 1))

grid.arrange(p1, p3, p5, p2, p4, p6, nrow = 2, ncol = 3)
```

Inferences from the fitting:

```{r, eval=report_bayesfactors}
# Compute the bayes factor for each parameter value. Note: we compare
# here against the ROPE, and not the degenerate null hypothesis of 0
# Bayes-Factor values would be more significant in the latter case
# According to the Raftery, 1995 rules
# bf = 1 - 3: Weak
# bf = 3 - 20: Positive
# bf = 20 - 150: Strong
# bf > 150: Very strong
print(bayesfactor_rope(scenario_completed.model.null, null = c(-0.055, 0.055)), n = 30)
print(bayesfactor_rope(scenario_completed.model.t, null = c(-0.055, 0.055)), n = 30)
print(bayesfactor_rope(scenario_completed.model.v, null = c(-0.055, 0.055)), n = 30)
```
```{r}
# Compare the models. Note: we cannot do this because we don't have enough samples.
# The default is 4000; apparently these functions are only meaningful with 40000
# comparison = bayesfactor_models(scenario_completed.model.t, scenario_completed.model.v,
#                                 denominator = scenario_completed.model.null)
# print(comparison)
# print(bayesfactor_inclusion(comparison))

# # If we're using the brms functions, then use the following (make sure to update!)
# common_hyp_to_test = c("-2 * has_ax1 = 0", "-2 * has_dx1 = 0")
# noise_levels_hyp_to_test = c(
#   "Intercept-noise_level_f1 = 0", "Intercept-noise_level_f2 = 0", "Intercept-noise_level_f1-noise_level_f2 = 0",
#   "Intercept-has_ax1:noise_level_f1 = 0", "Intercept-has_dx1:noise_level_f1 = 0",
#   "Intercept-has_ax1:noise_level_f2 = 0", "Intercept-has_dx1:noise_level_f2 = 0",
#   "Intercept-has_ax1:noise_level_f1-has_ax1:noise_level_f2 = 0",
#   "Intercept-has_dx1:noise_level_f1-has_dx1:noise_level_f2 = 0"
# )
# hyp_results = test_hypotheses(hypotheses_list = c(common_hyp_to_test, noise_levels_hyp_to_test), model = scenario_completed.model.v)
# print(hyp_results$hypothesis)
# plot(hyp_results, ask=F)

options(digits = 3)
# Check the significance of each hypothesis
h_df = as_tibble(insight::get_parameters(scenario_completed.model.t)) %>%
  transmute(
    Intercept = b_Intercept,
    has_ax_test = -2 * b_has_ax1,
    has_dx_test = -2 * b_has_dx1,
    noise_levelL_test = b_noise_level.L,
    noise_levelQ_test = b_noise_level.Q
  )

hyp_results = test_hypotheses(h_df, rope_values = c(-0.055, 0.055))
print(hyp_results %>% select(Parameter, pd, HDI_low, HDI_high, ROPE_Percentage, ROPE_Equivalence) %>% as.matrix())

h_df = as_tibble(insight::get_parameters(scenario_completed.model.v)) %>%
  transmute(
    Intercept = b_Intercept,
    has_ax_test = -2 * b_has_ax1,
    has_dx_test = -2 * b_has_dx1,
    noise_level1_test = b_noise_level_f1,
    noise_level2_test = b_noise_level_f2,
    noise_level3_test = - b_noise_level_f1 - b_noise_level_f2
  )

hyp_results = test_hypotheses(h_df, rope_values = c(-0.055, 0.055))
print(hyp_results %>% select(Parameter, pd, HDI_low, HDI_high, ROPE_Percentage, ROPE_Equivalence) %>% as.matrix())
options(digits = 7)
```

**Results**:

1. The effect of incorporating action suggestions (AX) has a probability 1.0 of being positive (Median = 2.36, 89% CI [1.43, 3.33]) and can be considered as significant  (0.0% in ROPE).
1. The noise level has a quadratic relationship to the probability of scenario completed with a positive effect size (convex-shape) with probability of 0.99 (Median = 1.37, 89% CI [0.63, 2.24]) and can be considered significant (0.0% in ROPE).
1. The effect of 10% noise (90% accurate suggestions) has a probability 0.99 of a negative effect on the probability of completing the scenario (Median = -1.14, 89% CI [-1.81, -0.49]) and can be considered significant (0.0% in ROPE).

(The ROPE is defined as [-0.055, 0.055]. It is the range is suggested in the literature for logistic models and it corresponds to a probability range of 0.11. Therefore, a probability change of less than 0.055 is considered no different from a probability change of 0)

So here are the following plots:

```{r, eval=plot_posteriors}
# A visualization data frame
gg_df = plot_df %>%
  mutate(scenario_completed = factor(scenario_completed)) %>%
  mutate(scenario_completed = fct_recode(scenario_completed, incomplete="0", complete = "1")) %>%
  mutate(noise_level = fct_rev(noise_level)) %>%
  count(has_ax, has_dx, noise_level, scenario_completed)

# Simulate predictions on new data
p_df =
  plot_df %>%
  data_grid(scenario_completed,
            has_ax,
            has_dx,
            noise_level_f,
            gender,
            age_group,
            robot_experience,
            num_optimal,
            .model = scenario_completed.model.v)

fits_p_df = p_df %>% add_fitted_draws(scenario_completed.model.v)
pars_p_df = scenario_completed.model.t %>% extract_draws(newdata = p_df %>% rename(noise_level = noise_level_f))
# We don't want predicted draws because that's a raw count of ones and zeros; not the
# probability distribution
# preds_p_df = p_df %>% add_predicted_draws(scenario_completed.model.v)

# Plot the posterior distributions
# rope_value = inv_logit_scaled(fixef(scenario_completed.model.null)["Intercept","Estimate"])
rope_value = gg_df %>%
  filter(scenario_completed == 'complete', has_ax == F) %>%
  summarise(.value = median(n) / 20) %>%
  pull(.value)
gg_df %>%
  filter(scenario_completed == 'complete') %>%
  ggplot(aes(x = has_ax, y = n / 20)) +
    geom_rect(ymin = rope_value-0.055, ymax = rope_value+0.055,
              xmin = -Inf, xmax = Inf,
              color = "transparent", fill = "white") +
    annotate("text", x = 0.5, y = rope_value, label = "ROPE", angle = 90) +
    stat_halfeye(aes(y = .value), data = fits_p_df, position = position_nudge(x = 0.1)) +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep = ":")), size = 2) +
    stat_summary(fun.y = median, geom = "point", shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(y = "scenario_completed", color = "AX:DX:Noise")

rope_value = gg_df %>%
  filter(scenario_completed == 'complete', has_dx == F) %>%
  summarise(.value = median(n) / 20) %>%
  pull(.value)
gg_df %>%
  filter(scenario_completed == 'complete') %>%
  ggplot(aes(x = has_dx, y = n / 20)) +
    geom_rect(ymin = rope_value-0.055, ymax = rope_value+0.055,
              xmin = -Inf, xmax = Inf,
              color = "transparent", fill = "white") +
    annotate("text", x = 0.5, y = rope_value, label = "ROPE", angle = 90) +
    stat_halfeye(aes(y = .value), data = fits_p_df, position = position_nudge(x = 0.1)) +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep = ":")), size = 2) +
    stat_summary(fun.y = median, geom = "point", shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(y = "scenario_completed", color = "AX:DX:Noise")

rope_value = gg_df %>%
  filter(scenario_completed == 'complete', noise_level == "0.0") %>%
  summarise(.value = median(n) / 20) %>%
  pull(.value)
gg_df %>%
  filter(scenario_completed == 'complete') %>%
  mutate(noise_level = fct_rev(noise_level)) %>%
  ggplot(aes(x = noise_level, y = n / 20)) +
    geom_rect(ymin = rope_value-0.055, ymax = rope_value+0.055,
              xmin = -Inf, xmax = Inf,
              color = "transparent", fill = "white") +
    annotate("text", x = 0.5, y = rope_value, label = "ROPE", angle = 90) +
    stat_halfeye(aes(y = .value, x = noise_level_f), data = fits_p_df, position = position_nudge(x = 0.1)) +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep = ":")), size = 2) +
    stat_summary(fun.y = median, geom = "point", shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(y = "scenario_completed", color = "AX:DX:Noise")

# Create a plot to show the trend
pars_p_df_X = pars_p_df$dpars$mu$fe$X[,c("Intercept", "noise_level.Q")]
pars_p_df_b = pars_p_df$dpars$mu$fe$b[,c("b_Intercept", "b_noise_level.Q")]
pars_p_df_y = pars_p_df_X %*% t(pars_p_df_b)[,as.integer(seq(from = 1, to = 4000, length.out = 100))]
pars_p_df_y = inv_logit_scaled(pars_p_df_y)
p_df =
  p_df %>%
  bind_cols(as_tibble(pars_p_df_y)) %>%
  gather(key = ".sample", value = ".value", V1:V100) %>%
  rename(noise_level = noise_level_f)

rope_value = gg_df %>%
  filter(scenario_completed == 'complete', noise_level == "0.0") %>%
  summarise(.value = median(n) / 20) %>%
  pull(.value)
gg_df %>%
  filter(scenario_completed == 'complete') %>%
  mutate(noise_level = fct_rev(noise_level)) %>%
  ggplot(aes(x = noise_level, y = n / 20)) +
    geom_rect(ymin = rope_value-0.055, ymax = rope_value+0.055,
              xmin = -Inf, xmax = Inf,
              color = "transparent", fill = "white") +
    annotate("text", x = 0.5, y = rope_value, label = "ROPE", angle = 90) +
    stat_halfeye(aes(y = .value, x = noise_level_f), data = fits_p_df, position = position_nudge(x = 0.1)) +
    geom_line(aes(y = .value, group = .sample, alpha = 0.3), data = p_df) +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep = ":")), size = 2) +
    stat_summary(fun.y = median, geom = "point", shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(y = "scenario_completed", color = "AX:DX:Noise") +
    guides(alpha = F)

# Remove the giant fits data frames
rm(fits_p_df, pars_p_df, pars_p_df_b, pars_p_df_X, pars_p_df_y, p_df, gg_df)
```

```{r, echo=F, results=F, message=F, warning=F}
rm(scenario_completed.model.null, scenario_completed.model.t, scenario_completed.model.v)
gc()
```


## Num Actions Diff

**The number of unnecessary actions that a person took; given the scenario they were assigned**

```{r}
# Optional try using predicted values of scenario completed
# scenario_completed.model.v = loadModel("scenario_completed.model.v")
# preds_p_df = users %>%
#   add_predicted_draws(scenario_completed.model.v, n=1) %>%
#   ungroup() %>%
#   rename(scenario_completed_pred = .prediction)
# 
# plot_df = preds_p_df %>%
#   select(X1, id, study_condition, start_condition, num_optimal,
#          age_group, gender, robot_experience,
#          noise_level, noise_level_f, has_noise, has_dx, has_ax,
#          scenario_completed, num_actions, num_actions_diff, frac_actions_diff,
#          scenario_completed_pred) %>%
#   mutate(scenario_completed = factor(scenario_completed),
#          scenario_completed_pred = factor(scenario_completed_pred))
# 
# rm(scenario_completed.model.v, preds_p_df)

plot_df = users %>%
  select(X1, id, study_condition, start_condition, num_optimal,
         age_group, gender, robot_experience,
         noise_level, noise_level_f, has_noise, has_dx, has_ax,
         scenario_completed, num_actions, num_actions_diff, frac_actions_diff) %>%
  mutate(scenario_completed = factor(scenario_completed))

text_short(report(plot_df))
```

```{r fig.height=7, fig.width=15}
# Plot by the study condition
plot_df %>%
  mutate(.fill_column = paste(noise_level, !(study_condition == 'BASELINE'), sep = ":")) %>%
  ggplot(aes(study_condition, num_actions_diff, fill = .fill_column)) +
    geom_boxplot() +
    geom_count() +
    labs(y = "Number unnecessary actions", fill = "noise:has_suggestions") +
    scale_fill_economist()

# Visualize the data by the three variables that we care about
gg_df = plot_df %>%
  mutate(.fill_column = paste(noise_level, !(study_condition == 'BASELINE'), sep = ":")) %>%
  mutate(noise_level = fct_rev(noise_level))

p1 = gg_df %>%
  ggplot(aes(has_ax, num_actions_diff, fill=.fill_column)) +
    geom_boxplot() +
    geom_count() +
    facet_grid(rows = vars(noise_level), cols = vars(has_dx), labeller = label_both) +
    labs(y = "Number unnecessary actions") +
    scale_fill_economist()

p2 = gg_df %>%
  ggplot(aes(has_dx, num_actions_diff, fill=.fill_column)) +
    geom_boxplot() +
    geom_count() +
    facet_grid(rows = vars(noise_level), cols = vars(has_ax), labeller = label_both) +
    labs(y = "Number unnecessary actions") +
    scale_fill_economist()

p3 = gg_df %>%
  mutate(noise_level = fct_rev(noise_level)) %>%
  mutate(has_ax = factor(has_ax)) %>%
  mutate(has_ax = fct_rev(has_ax)) %>%
  ggplot(aes(noise_level, num_actions_diff, fill=.fill_column)) +
    geom_boxplot() +
    geom_count() +
    facet_grid(rows = vars(has_ax), cols = vars(has_dx), labeller = label_both) +
    labs(y = "Number unnecessary actions") +
    scale_fill_economist()

grid.arrange(p1, p2, p3, ncol = 3)

# Visualize also the variation in each of the three levels that we care about
p1 = gg_df %>%
  ggplot(aes(num_actions_diff, group = scenario_completed, fill = scenario_completed)) +
    geom_histogram(binwidth = 1) +
    facet_grid(rows = vars(has_ax)) +
    ylim(0, 30) +
    scale_fill_economist() +
    legend_none()

p2 = gg_df %>%
  ggplot(aes(num_actions_diff, group = scenario_completed, fill = scenario_completed)) +
    geom_histogram(binwidth = 1) +
    facet_grid(rows = vars(has_dx)) +
    ylim(0, 30) +
    scale_fill_economist() +
    theme(legend.position = "bottom")

p3 = gg_df %>%
  ggplot(aes(num_actions_diff, group = scenario_completed, fill = scenario_completed)) +
    geom_histogram(binwidth = 1) +
    facet_grid(rows = vars(noise_level)) +
    ylim(0, 30) +
    scale_fill_economist() +
    legend_none()

grid.arrange(p1, p2, p3, ncol = 3)
```

Checking some probability distributions of the action counts (given the above plots):

```{r}
# Run a few simple distribution fitting tests using the action
x = fit_and_print_dist(plot_df %>% filter(has_ax == T, scenario_completed == 1) %>% pull(num_actions_diff),
                       "pois", "AX = T, SC = T; pois:")
x = fit_and_print_dist(plot_df %>% filter(has_ax == T, scenario_completed == 1) %>% pull(num_actions_diff),
                       "nbinom", "AX = T, SC = T; nbinom:")
x = fit_and_print_dist(plot_df %>% filter(has_ax == T) %>% pull(num_actions_diff),
                       "pois", "AX = T, NSC; pois:")
x = fit_and_print_dist(plot_df %>% filter(has_ax == T) %>% pull(num_actions_diff),
                       "nbinom", "AX = T, NSC; nbinom:")

# Try a couple of other predictor variables
x = fit_and_print_dist(plot_df %>% filter(has_dx == T, scenario_completed == 1) %>% pull(num_actions_diff),
                       "nbinom", "DX = T, SC == T; nbinom:")
x = fit_and_print_dist(plot_df %>% filter(noise_level == "0.0", scenario_completed == 1) %>% pull(num_actions_diff),
                       "nbinom", "Noise = 0, SC == T; nbinom:")
x = fit_and_print_dist(plot_df %>% filter(noise_level == "1.0", scenario_completed == 1) %>% pull(num_actions_diff),
                       "nbinom", "Noise = 1, SC == T; nbinom:")
x = fit_and_print_dist(plot_df %>% filter(noise_level == "2.0", scenario_completed == 1) %>% pull(num_actions_diff),
                       "nbinom", "Noise = 2, SC == T; nbinom:")
x = fit_and_print_dist(plot_df %>% filter(has_dx == T, scenario_completed == 1) %>% pull(num_actions_diff),
                       "pois", "DX = T, SC == T; pois:")
x = fit_and_print_dist(plot_df %>% filter(noise_level == "0.0", scenario_completed == 1) %>% pull(num_actions_diff),
                       "pois", "Noise = 0, SC == T; pois:")
x = fit_and_print_dist(plot_df %>% filter(noise_level == "1.0", scenario_completed == 1) %>% pull(num_actions_diff),
                       "pois", "Noise = 1, SC == T; pois:")
x = fit_and_print_dist(plot_df %>% filter(noise_level == "2.0", scenario_completed == 1) %>% pull(num_actions_diff),
                       "pois", "Noise = 2, SC == T; pois:")
rm(x)
```

From this, we can take away that:

1. The distribution of counts is more akin to a Negative Binomial distribution than a Poisson distribution
1. The truncation-effect of whether a person completed the scenario affects the distribution of counts, and therefore, that should be considered as a filter before our regression

Therefore, we assume the following structural model:

$$num\_actions\_diff_i = NegBinomial(\mu_i, \phi)$$
$$\begin{aligned}
log(\mu_i) &= \beta_0 + \beta_{ax}\text{ax}_i + \beta_{dx}\text{dx}_i + \beta_{noise}\text{noise}_i +\\ &\beta_{ax:noise}\text{ax}_i\text{noise}_i + \beta_{dx:noise}\text{dx}_i\text{noise}_i + \\
&\beta_{no}\text{no}_i + \mathbf{\beta_{demo}X_{demo,i}}
\end{aligned}$$
$$\begin{aligned}
\beta_{.} &\sim Normal(0, 10)\\
\phi &\sim Gamma(0.01, 0.01)
\end{aligned}$$

The prior for the $\phi$ parameter is the default used in `brms`; I see no need to change it.

Model fitting:

```{r, eval=train_models}
# A null model to compare against
num_actions_diff.model.null = brm(
  num_actions_diff ~ 0 + Intercept,
  family = negbinomial,
  prior = c(set_prior("normal(0, 10)", class = "b"),
            set_prior("gamma(0.01, 0.01)", class = "shape")),
  data = plot_df %>% filter(scenario_completed == 1),
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
num_actions_diff.model.null = add_criterion(num_actions_diff.model.null, "waic")
num_actions_diff.model.null = add_criterion(num_actions_diff.model.null, "loo", reloo = T)
saveModel(num_actions_diff.model.null)

# The trend model to see if there is a trend in the noise level variable
num_actions_diff.model.t = brm(
  num_actions_diff ~ 0 + Intercept + ((has_dx + has_ax) * noise_level) + (gender + age_group + robot_experience) + num_optimal,
  family = negbinomial,
  prior = c(set_prior("normal(0, 10)", class = "b"),
            set_prior("gamma(0.01, 0.01)", class = "shape")),
  data = plot_df %>% filter(scenario_completed == 1),
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
num_actions_diff.model.t = add_criterion(num_actions_diff.model.t, "waic")
num_actions_diff.model.t = add_criterion(num_actions_diff.model.t, "loo", reloo = T)
saveModel(num_actions_diff.model.t)

# The values model, to see if specific values of the noise level variable are significant
num_actions_diff.model.v = brm(
  num_actions_diff ~ 0 + Intercept + ((has_dx + has_ax) * noise_level_f) + (gender + age_group + robot_experience) + num_optimal,
  family = negbinomial,
  prior = c(set_prior("normal(0, 10)", class = "b"),
            set_prior("gamma(0.01, 0.01)", class = "shape")),
  data = plot_df %>% filter(scenario_completed == 1),
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
num_actions_diff.model.v = add_criterion(num_actions_diff.model.v, "waic")
num_actions_diff.model.v = add_criterion(num_actions_diff.model.v, "loo", reloo = T)
saveModel(num_actions_diff.model.v)

# The number of data points that we're actually using per condition:
# A tibble: 10 x 3
# Groups:   study_condition, scenario_completed [10]
#    study_condition scenario_completed     n
#    <fct>           <fct>              <int>
#  1 DX_100          1                     14
#  2 AX_100          1                     20
#  3 DXAX_100        1                     18
#  4 DX_90           1                     11
#  5 AX_90           1                     16
#  6 DXAX_90         1                     18
#  7 DX_80           1                     13
#  8 AX_80           1                     19
#  9 DXAX_80         1                     17
# 10 BASELINE        1                     15

# One alternative is to include data where scenario_completed is true, but
# it is included as a predictor in the regression. I don't think that makes
# sense because scenario_completed and num_optimal can together determine
# the num_actions_diff. Also, NOTE: the significance results don't change
# if we consider only those points where scenario is completed
# num_actions_diff.model.v = brm(
#   num_actions_diff ~ 0 + Intercept + ((has_dx + has_ax) * noise_level_f) + (gender + age_group + robot_experience) + num_optimal + scenario_completed,
#   family = negbinomial,
#   prior = c(set_prior("normal(0, 10)", class = "b"),
#             set_prior("gamma(0.01, 0.01)", class = "shape")),
#   data = plot_df,
#   seed = default_seed,
#   save_all_pars = T,
#   sample_prior = T
# )
# num_actions_diff.model.v = add_criterion(num_actions_diff.model.v, "waic")
# num_actions_diff.model.v = add_criterion(num_actions_diff.model.v, "loo", reloo = T)

# Another option is to remove the effect of the baseline condition. This is because Baseline
# possibly confounds with noise - on the one hand, baseline has no noise and could be
# considered as a 0 noise condition; on the other hand, it has 100% noise. Again; there
# doesn't seem to be a significant effect of ignoring baseline
# num_actions_diff.model.v = brm(
#   num_actions_diff ~ 0 + Intercept + ((has_dx + has_ax) * noise_level_f) + (gender + age_group + robot_experience) + num_optimal,
#   family = negbinomial,
#   prior = c(set_prior("normal(0, 10)", class = "b"),
#             set_prior("gamma(0.01, 0.01)", class = "shape")),
#   data = plot_df %>% filter(scenario_completed == 1, study_condition != "BASELINE"),
#   seed = default_seed,
#   save_all_pars = T,
#   sample_prior = T
# )
# num_actions_diff.model.v = add_criterion(num_actions_diff.model.v, "waic")
# num_actions_diff.model.v = add_criterion(num_actions_diff.model.v, "loo", reloo = T)

# Yet another option is to use our prediction of the scenario being completed as a metric
# for filtering. This probably will not work as well because our predictions can be quite
# iffy. There is a significant effect of adding the predictions, but not enough to really
# differentiate us in the ROPE
# num_actions_diff.model.v = brm(
#   num_actions_diff ~ 0 + Intercept + ((has_dx + has_ax) * noise_level_f) + (gender + age_group + robot_experience) + num_optimal + scenario_completed_pred,
#   family = negbinomial,
#   prior = c(set_prior("normal(0, 10)", class = "b"),
#             set_prior("gamma(0.01, 0.01)", class = "shape")),
#   data = plot_df,
#   seed = default_seed,
#   save_all_pars = T,
#   sample_prior = T
# )
# num_actions_diff.model.v = add_criterion(num_actions_diff.model.v, "waic")
# num_actions_diff.model.v = add_criterion(num_actions_diff.model.v, "loo", reloo = T)

# The misspecified model alternative (not listed above) is to neither filter by the
# scenario_completed predictor nor include that variable in the regression
```
```{r, eval=!train_models}
# Load the models
num_actions_diff.model.null = loadModel("num_actions_diff.model.null")
num_actions_diff.model.t = loadModel("num_actions_diff.model.t")
num_actions_diff.model.v = loadModel("num_actions_diff.model.v")
```

Model fitting results:

```{r}
# print(tidy_stan(num_actions_diff.model.null))
print(performance::r2(num_actions_diff.model.null))

# Print the parameters, and some initial diagnostics
print(tidy_stan(num_actions_diff.model.t))
print(performance::r2(num_actions_diff.model.t))

print(tidy_stan(num_actions_diff.model.v))
print(performance::r2(num_actions_diff.model.v))
```

Diagnostics of the fit:

```{r fig.height=7, fig.width=15, eval=plot_diagnostics}
# Alternative diagnostics: diagnostic_posterior(scenario_completed.model.v)

# LOO cross-validation results
print(num_actions_diff.model.null$criteria$loo$estimates)
print(num_actions_diff.model.t$criteria$loo$estimates)
print(num_actions_diff.model.v$criteria$loo$estimates)
print(loo_compare(num_actions_diff.model.null,
                  num_actions_diff.model.t,
                  num_actions_diff.model.v))

# Diagnostics of the sampling process
par(mfrow=c(1,2))
plot(num_actions_diff.model.t$criteria$loo, main = "Trend Model")
plot(num_actions_diff.model.v$criteria$loo, main = "Values Model")

# Include posterior predictive checks of the sampling (are sampling according
# to the data that we actually have?)
mcmc_intervals(as.matrix(num_actions_diff.model.null), regex_pars = "b_", prob = 0.90, prob_outer = 0.95) +
  ggtitle("Null Model")

p1 = pp_check(num_actions_diff.model.t, type = "bars_grouped", group = "has_ax", stat = "median") + ggtitle("t:has_ax")
p2 = pp_check(num_actions_diff.model.t, type = "bars_grouped", group = "has_dx") + ggtitle("t:has_dx")
p3 = pp_check(num_actions_diff.model.t, type = "bars_grouped", group = "noise_level") + ggtitle("t:noise_level")
grid.arrange(p1, p2, p3, nrow = 3)
mcmc_intervals(as.matrix(num_actions_diff.model.t), regex_pars = "b_|^shape", prob = 0.90, prob_outer = 0.95) +
  ggtitle("Trends Model")

p1 = pp_check(num_actions_diff.model.v, type = "bars_grouped", group = "has_ax") + ggtitle("v:has_ax")
p2 = pp_check(num_actions_diff.model.v, type = "bars_grouped", group = "has_dx") + ggtitle("v:has_dx")
p3 = pp_check(num_actions_diff.model.v, type = "bars_grouped", group = "noise_level_f") + ggtitle("v:noise_level")
grid.arrange(p1, p2, p3, nrow = 3)
mcmc_intervals(as.matrix(num_actions_diff.model.v), regex_pars = "b_|^shape", prob = 0.90, prob_outer = 0.95) +
  ggtitle("Values Model")

# Plot predictions vs original
preds_p_df = num_actions_diff.model.null %>% predict() %>% as_tibble()
p1 =
  preds_p_df %>%
    add_column(num_actions_diff = num_actions_diff.model.null$data$num_actions_diff) %>%
    ggplot(aes(x = Estimate, y = num_actions_diff, color = num_actions_diff)) +
      geom_point() +
      geom_abline(slope = 1, intercept = 0) +
      labs(x = "Predicted values") +
      ggtitle("Null Model")

preds_p_df =
  plot_df %>%
  add_predicted_draws(num_actions_diff.model.null) %>%
  ungroup() %>%
  select(num_actions_diff, .prediction)
preds_p_df =
  preds_p_df %>%
  mutate(num_actions_diff = factor(num_actions_diff, levels = seq(from = 1, to = 20), ordered = T),
         .prediction = factor(.prediction, levels = seq(from = 1, to = 20), ordered = T))
preds_p_df =
  as_tibble(caret::confusionMatrix(preds_p_df$.prediction, preds_p_df$num_actions_diff)$table) %>%
  mutate(Prediction = factor(Prediction, levels = seq(from = 1, to = 20), ordered = T),
         Reference = factor(Reference, levels = seq(from = 1, to = 20), ordered = T))
p2 =
  preds_p_df %>%
    ggplot(aes(x = Reference, y = Prediction, fill = n )) +
      geom_tile()

preds_p_df = num_actions_diff.model.t %>% predict() %>% as_tibble()
p3 =
  preds_p_df %>%
    add_column(num_actions_diff = num_actions_diff.model.t$data$num_actions_diff) %>%
    ggplot(aes(x = Estimate, y = num_actions_diff, color = num_actions_diff)) +
      geom_point() +
      geom_abline(slope = 1, intercept = 0) +
      labs(x = "Predicted values") +
      ggtitle("Trends Model")

preds_p_df =
  plot_df %>%
  add_predicted_draws(num_actions_diff.model.t) %>%
  ungroup() %>%
  select(num_actions_diff, .prediction)
preds_p_df =
  preds_p_df %>%
  mutate(num_actions_diff = factor(num_actions_diff, levels = seq(from = 1, to = 20), ordered = T),
         .prediction = factor(.prediction, levels = seq(from = 1, to = 20), ordered = T))
preds_p_df =
  as_tibble(caret::confusionMatrix(preds_p_df$.prediction, preds_p_df$num_actions_diff)$table) %>%
  mutate(Prediction = factor(Prediction, levels = seq(from = 1, to = 20), ordered = T),
         Reference = factor(Reference, levels = seq(from = 1, to = 20), ordered = T))
p4 =
  preds_p_df %>%
    ggplot(aes(x = Reference, y = Prediction, fill = n)) +
      geom_tile()

# Plot predictions vs original
preds_p_df = num_actions_diff.model.v %>% predict() %>% as_tibble()
p5 =
  preds_p_df %>%
    add_column(num_actions_diff = num_actions_diff.model.v$data$num_actions_diff) %>%
    ggplot(aes(x = Estimate, y = num_actions_diff, color = num_actions_diff)) +
      geom_point() +
      geom_abline(slope = 1, intercept = 0) +
      labs(x = "Predicted values") +
      ggtitle("Values Model")

preds_p_df =
  plot_df %>%
  add_predicted_draws(num_actions_diff.model.v) %>%
  ungroup() %>%
  select(num_actions_diff, .prediction)
preds_p_df =
  preds_p_df %>%
  mutate(num_actions_diff = factor(num_actions_diff, levels = seq(from = 1, to = 20), ordered = T),
         .prediction = factor(.prediction, levels = seq(from = 1, to = 20), ordered = T))
preds_p_df =
  as_tibble(caret::confusionMatrix(preds_p_df$.prediction, preds_p_df$num_actions_diff)$table) %>%
  mutate(Prediction = factor(Prediction, levels = seq(from = 1, to = 20), ordered = T),
         Reference = factor(Reference, levels = seq(from = 1, to = 20), ordered = T))
p6 =
  preds_p_df %>%
    ggplot(aes(x = Reference, y = Prediction, fill = n)) +
      geom_tile()

grid.arrange(p1, p3, p5, p2, p4, p6, nrow = 2, ncol = 3)
```

Inferences from the fitting:

```{r, eval=report_bayesfactors}
# Compute the bayes factor for each parameter value. Note: we compare
# here against the ROPE, and not the degenerate null hypothesis of 0
# Bayes-Factor values would be more significant in the latter case
# According to the Raftery, 1995 rules
# bf = 1 - 3: Weak
# bf = 3 - 20: Positive
# bf = 20 - 150: Strong
# bf > 150: Very strong
print(bayesfactor_rope(num_actions_diff.model.null, null = c(-0.55, 0.55)), n = 30)
print(bayesfactor_rope(num_actions_diff.model.t, null = c(-0.55, 0.55)), n = 30)
print(bayesfactor_rope(num_actions_diff.model.v, null = c(-0.55, 0.55)), n = 30)
```
```{r}
options(digits = 3)
# Check the significance of each hypothesis
h_df = as_tibble(insight::get_parameters(num_actions_diff.model.t)) %>%
  transmute(
    Intercept = b_Intercept,
    has_ax_test = -2 * b_has_ax1,
    has_dx_test = -2 * b_has_dx1,
    noise_levelL_test = b_noise_level.L,
    noise_levelQ_test = b_noise_level.Q
  )

hyp_results = test_hypotheses(h_df, rope_values = c(-0.55, 0.55))
print(hyp_results %>% select(Parameter, pd, HDI_low, HDI_high, ROPE_Percentage, ROPE_Equivalence) %>% as.matrix())

h_df = as_tibble(insight::get_parameters(num_actions_diff.model.v)) %>%
  transmute(
    Intercept = b_Intercept,
    has_ax_test = -2 * b_has_ax1,
    has_dx_test = -2 * b_has_dx1,
    noise_level1_test = b_noise_level_f1,
    noise_level2_test = b_noise_level_f2,
    noise_level3_test = - b_noise_level_f1 - b_noise_level_f2
  )

hyp_results = test_hypotheses(h_df, rope_values = c(-0.55, 0.55))
print(hyp_results %>% select(Parameter, pd, HDI_low, HDI_high, ROPE_Percentage, ROPE_Equivalence) %>% as.matrix())
options(digits = 7)
```

**Results**:

There is no significant effect of any of the suggestions parameters on the number of actions taken, *once the effect of action suggestions on the completion of the scenario has been controlled for*. The `pd` value for gender suggests that there is an effect of gender exists on the number of actions, but we don't test the significance of that effect.

(The ROPE is defined as [-0.55, 0.55]. It corresponds to a numerical range of 1.1. Therefore, a change of less than 0.55 actions in either direction is considered no different from a change of 0 in the number of actions)

There are no plots to show (all effects are supposedly non-existent).

```{r, echo=F, results=F, message=F, warning=F}
rm(num_actions_diff.model.null, num_actions_diff.model.t, num_actions_diff.model.v)
gc()
```


# Action Level Metrics {.tabset}

These are metrics where an individual unit of analysis is the action of any given user.

## Taking Correct / Optimal Actions

**Did the user take an optimal action given the state that they were in**

One can almost think of this as an inverse of the "Reliance" metric mentioned by Jessie Yang et al. - If participants were "reliant" on the suggestions, then those under the noise condition would perform worse.

```{r}
plot_df = actions %>%
  select(X1, id, user_id, study_condition, start_condition, num_optimal,
         state_idx, state_idx_rescaled,
         age_group, gender, robot_experience,
         noise_level, noise_level_f, has_noise, has_dx, has_ax,
         scenario_completed, num_actions, optimal_ax)

text_short(report(plot_df))
```

```{r fig.height=7, fig.width=15}
gg_df =
  plot_df %>%
    mutate(noise_level = fct_rev(noise_level)) %>%
    mutate(optimal_ax = factor(optimal_ax)) %>%
    mutate(user_id = factor(user_id)) %>%
    mutate(optimal_ax = fct_recode(optimal_ax, incorrect="0", correct = "1")) %>%
    mutate(.fill_column = paste(noise_level, !(study_condition == 'BASELINE'), sep = ":")) %>%
    count(user_id, study_condition, .fill_column, optimal_ax, num_actions, has_ax, has_dx, noise_level,
          name = "num_optimal_ax")

# Plot by the study condition
gg_df %>%
  ggplot(aes(user_id, num_optimal_ax / num_actions, fill=optimal_ax)) +
    geom_bar(stat="identity") +
    geom_hline(yintercept = 0.5) +
    facet_wrap(vars(study_condition), nrow = 1, scales = "free_y") +
    labs(y = "Fraction correct actions") +
    coord_flip() +
    scale_fill_economist()

gg_df %>%
  filter(optimal_ax == "correct") %>%
  ggplot(aes(study_condition, num_optimal_ax / num_actions, fill=.fill_column)) +
    geom_violin() +
    geom_boxplot(width = 0.1) +
    labs(y = "Fraction correct actions", fill = "noise:has_suggestions") +
    scale_fill_economist()

# Visualize the data facetted by the variables that we care about
p1 = gg_df %>%
  filter(optimal_ax == "correct") %>%
  ggplot(aes(has_ax, num_optimal_ax / num_actions, fill=.fill_column)) +
    geom_boxplot() +
    geom_count() +
    facet_grid(rows = vars(noise_level), cols = vars(has_dx), labeller = label_both) +
    labs(y = "Fraction correct actions") +
    scale_fill_economist() +
    theme(legend.position = "left")

p2 = gg_df %>%
  filter(optimal_ax == "correct") %>%
  ggplot(aes(has_dx, num_optimal_ax / num_actions, fill=.fill_column)) +
    geom_boxplot() +
    geom_count() +
    facet_grid(rows = vars(noise_level), cols = vars(has_ax), labeller = label_both) +
    labs(y = "Fraction correct actions") +
    scale_fill_economist() +
    legend_none()

p3 = gg_df %>%
  filter(optimal_ax == "correct") %>%
  mutate(noise_level = fct_rev(noise_level)) %>%
  mutate(has_ax = factor(has_ax)) %>%
  mutate(has_ax = fct_rev(has_ax)) %>%
  ggplot(aes(noise_level, num_optimal_ax / num_actions, fill=.fill_column)) +
    geom_boxplot() +
    geom_count() +
    facet_grid(rows = vars(has_ax), cols = vars(has_dx), labeller = label_both) +
    labs(y = "Fraction correct actions") +
    scale_fill_economist() +
    legend_none()

grid.arrange(p1, p2, p3, ncol = 3)

# Visualize also the variation in each of the three levels that we care about
p1 = gg_df %>%
  filter(optimal_ax == 'correct') %>%
  ggplot(aes(has_ax, num_optimal_ax/num_actions)) +
    geom_violin() +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep=":"), size = 2)) +
    stat_summary(fun.y = median, geom="point", shape=23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(color = "AX:DX:Noise") +
    theme(legend.position = "left") +
    guides(size = F)

p2 = gg_df %>%
  filter(optimal_ax == 'correct') %>%
  ggplot(aes(has_dx, num_optimal_ax/num_actions)) +
    geom_violin() +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep=":"), size = 2)) +
    stat_summary(fun.y = median, geom="point", shape=23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    legend_none()

p3 = gg_df %>%
  filter(optimal_ax == 'correct') %>%
  ggplot(aes(noise_level, num_optimal_ax/num_actions)) +
    geom_violin() +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep=":"), size = 2)) +
    stat_summary(fun.y = median, geom="point", shape=23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    scale_x_discrete(limits = rev(levels(gg_df$noise_level))) +
    legend_none()

grid.arrange(p1, p2, p3, ncol = 3)
```

Based on the data, we assume the following structural model:

$$correct\_action_i = Bernoulli(p_i)$$
$$\begin{aligned}
logit(p_j) &= \beta_0 + \beta_{0i} + \beta_{ax}\text{ax}_j + \beta_{dx}\text{dx}_j + \beta_{noise}\text{noise}_j +\\
&\beta_{ax:noise}\text{ax}_j\text{noise}_j + \beta_{dx:noise}\text{dx}_j\text{noise}_j + \\
&\beta_{no}\text{no}_j + \mathbf{\beta_{demo}X_{demo,j}} + \beta_{state} \text{state}_{ij}
\end{aligned}$$
$$\begin{aligned}
\beta_{.} &\sim Normal(0, 10) \\
\beta_{0i} &\sim Normal(0, \sigma_i) \\
\sigma_i &\sim HalfStudent(3, 0, 10)
\end{aligned}$$

The prior for the $\sigma_i$ parameter is the default used in `brms`; I see no need to change it.

Model fitting:

```{r, eval=train_models}
# The null model
optimal_ax.model.null = brm(
  optimal_ax ~ 0 + Intercept + (1 | user_id),
  family = "bernoulli",
  prior = c(set_prior("normal(0, 10)", class = "b"),
            set_prior("student_t(3, 0, 10)", class = "sd")),
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
optimal_ax.model.null = add_criterion(optimal_ax.model.null, "waic")
optimal_ax.model.null = add_criterion(optimal_ax.model.null, "loo", reloo = T)
saveModel(optimal_ax.model.null)

# The trend model to see if there is a trend in the noise level variable
optimal_ax.model.t = brm(
  optimal_ax ~ 0 + Intercept + ((has_dx + has_ax) * noise_level) + (age_group + robot_experience + gender) + num_optimal + state_idx_rescaled + (1 | user_id),
  family = bernoulli,
  prior = c(set_prior("normal(0, 10)", class = "b"),
            set_prior("student_t(3, 0, 10)", class = "sd")),
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
optimal_ax.model.t = add_criterion(optimal_ax.model.t, "waic")
optimal_ax.model.t = add_criterion(optimal_ax.model.t, "loo", reloo = T)
saveModel(optimal_ax.model.t)

# The values model, to see if specific values of the noise level variable are significant
optimal_ax.model.v = brm(
  optimal_ax ~ 0 + Intercept + ((has_dx + has_ax) * noise_level_f) + (age_group + robot_experience + gender) + num_optimal + state_idx_rescaled + (1 | user_id),
  family = bernoulli,
  prior = c(set_prior("normal(0, 10)", class = "b"),
            set_prior("student_t(3, 0, 10)", class = "sd")),
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
optimal_ax.model.v = add_criterion(optimal_ax.model.v, "waic")
optimal_ax.model.v = add_criterion(optimal_ax.model.v, "loo", reloo = T)
saveModel(optimal_ax.model.v)
```
```{r, eval=!train_models}
# Load the models
optimal_ax.model.null = loadModel("optimal_ax.model.null")
optimal_ax.model.t = loadModel("optimal_ax.model.t")
optimal_ax.model.v = loadModel("optimal_ax.model.v")
```

Model fitting results:

```{r}
# print(tidy_stan(optimal_ax.model.null))
print(performance::r2(optimal_ax.model.null))

# Print the parameters, and some initial diagnostics
print(tidy_stan(optimal_ax.model.t, effects = "fixed"))
print(performance::r2(optimal_ax.model.t))

print(tidy_stan(optimal_ax.model.v, effects = "fixed"))
print(performance::r2(optimal_ax.model.v))
```

Diagnostics of the fit:

```{r fig.height=7, fig.width=15, eval=plot_diagnostics}
# Alternative diagnostics: diagnostic_posterior(scenario_completed.model.v)

# LOO cross-validation results
print(optimal_ax.model.t$criteria$loo$estimates)
print(optimal_ax.model.v$criteria$loo$estimates)
print(loo_compare(optimal_ax.model.null, optimal_ax.model.t, optimal_ax.model.v))

# Diagnostics of the sampling process
par(mfrow=c(1,2))
plot(optimal_ax.model.t$criteria$loo, main = "Trend Model")
plot(optimal_ax.model.v$criteria$loo, main = "Values Model")

# Include posterior predictive checks of the sampling (are sampling according
# to the data that we actually have?)
p1 = pp_check(optimal_ax.model.t, type = "bars_grouped", group = "has_ax", stat = "median") + ggtitle("t:has_ax")
p2 = pp_check(optimal_ax.model.t, type = "bars_grouped", group = "has_dx") + ggtitle("t:has_dx")
p3 = pp_check(optimal_ax.model.t, type = "bars_grouped", group = "noise_level") + ggtitle("t:noise_level")
grid.arrange(p1, p2, p3, nrow = 3)
mcmc_intervals(as.matrix(optimal_ax.model.t), regex_pars = "b_|^shape", prob = 0.90, prob_outer = 0.95) +
  ggtitle("Trends Model")

p1 = pp_check(optimal_ax.model.v, type = "bars_grouped", group = "has_ax") + ggtitle("v:has_ax")
p2 = pp_check(optimal_ax.model.v, type = "bars_grouped", group = "has_dx") + ggtitle("v:has_dx")
p3 = pp_check(optimal_ax.model.v, type = "bars_grouped", group = "noise_level_f") + ggtitle("v:noise_level")
grid.arrange(p1, p2, p3, nrow = 3)
mcmc_intervals(as.matrix(optimal_ax.model.v), regex_pars = "b_|^shape", prob = 0.90, prob_outer = 0.95) +
  ggtitle("Values Model")

# Plot predictions vs original
preds_p_df = optimal_ax.model.null %>% predict() %>% as_tibble()
p1 =
  preds_p_df %>%
    add_column(optimal_ax = optimal_ax.model.null$data$optimal_ax) %>%
    ggplot(aes(x = Estimate, y = optimal_ax, color = optimal_ax)) +
      geom_point() +
      labs(x = "Predicted values") +
      ylim(0, 1) +
      xlim(0, 1) +
      ggtitle("Null Model")

preds_p_df = plot_df %>%
  add_predicted_draws(optimal_ax.model.null) %>%
  group_by(optimal_ax, .prediction) %>%
  count()
p2 =
  preds_p_df %>%
    ggplot(aes(x = optimal_ax, y = .prediction, fill = n / sum(preds_p_df$n))) +
      geom_tile() +
      geom_text(aes(label = sprintf("%1.2f", n / sum(preds_p_df$n))), size = 10, vjust = 1) +
      scale_fill_gradient(limits = c(0, 1))

preds_p_df = optimal_ax.model.t %>% predict() %>% as_tibble()
p3 =
  preds_p_df %>%
    add_column(optimal_ax = optimal_ax.model.t$data$optimal_ax) %>%
    ggplot(aes(x = Estimate, y = optimal_ax, color = optimal_ax)) +
      geom_point() +
      labs(x = "Predicted values") +
      ylim(0, 1) +
      xlim(0, 1) +
      ggtitle("Trends Model")

preds_p_df = plot_df %>%
  add_predicted_draws(optimal_ax.model.t) %>%
  group_by(optimal_ax, .prediction) %>%
  count()
p4 =
  preds_p_df %>%
    ggplot(aes(x = optimal_ax, y = .prediction, fill = n / sum(preds_p_df$n))) +
      geom_tile() +
      geom_text(aes(label = sprintf("%1.2f", n / sum(preds_p_df$n))), size = 10, vjust = 1) +
      scale_fill_gradient(limits = c(0, 1))

# Plot predictions vs original
preds_p_df = optimal_ax.model.v %>% predict() %>% as_tibble()
p5 =
  preds_p_df %>%
    add_column(optimal_ax = optimal_ax.model.v$data$optimal_ax) %>%
    ggplot(aes(x = Estimate, y = optimal_ax, color = optimal_ax)) +
      geom_point() +
      labs(x = "Predicted values") +
      ylim(0, 1) +
      xlim(0, 1) +
      ggtitle("Values Model")

preds_p_df = plot_df %>%
  add_predicted_draws(optimal_ax.model.v) %>%
  group_by(optimal_ax, .prediction) %>%
  count()
p6 =
  preds_p_df %>%
    ggplot(aes(x = optimal_ax, y = .prediction, fill = n / sum(preds_p_df$n))) +
      geom_tile() +
      geom_text(aes(label = sprintf("%1.2f", n / sum(preds_p_df$n))), size = 10, vjust = 1) +
      scale_fill_gradient(limits = c(0, 1))

grid.arrange(p1, p3, p5, p2, p4, p6, nrow = 2, ncol = 3)
```

Inferences from the fitting:

```{r, eval=report_bayesfactors}
# Compute the bayes factor for each parameter value. Note: we compare
# here against the ROPE, and not the degenerate null hypothesis of 0
# Bayes-Factor values would be more significant in the latter case
# According to the Raftery, 1995 rules
# bf = 1 - 3: Weak
# bf = 3 - 20: Positive
# bf = 20 - 150: Strong
# bf > 150: Very strong
print(bayesfactor_rope(optimal_ax.model.null, null = c(-0.055, 0.055)), n = 30)
print(bayesfactor_rope(optimal_ax.model.t, null = c(-0.055, 0.055)), n = 30)
print(bayesfactor_rope(optimal_ax.model.v, null = c(-0.055, 0.055)), n = 30)
```
```{r}
options(digits = 3)
# Check the significance of each hypothesis
h_df = as_tibble(insight::get_parameters(optimal_ax.model.t)) %>%
  transmute(
    Intercept = b_Intercept,
    has_ax_test = -2 * b_has_ax1,
    has_dx_test = -2 * b_has_dx1,
    noise_levelL_test = b_noise_level.L,
    noise_levelQ_test = b_noise_level.Q
  )

hyp_results = test_hypotheses(h_df, rope_values = c(-0.055, 0.055))
print(hyp_results %>% select(Parameter, pd, HDI_low, HDI_high, ROPE_Percentage, ROPE_Equivalence) %>% as.matrix())

h_df = as_tibble(insight::get_parameters(optimal_ax.model.v)) %>%
  transmute(
    Intercept = b_Intercept,
    has_ax_test = -2 * b_has_ax1,
    has_dx_test = -2 * b_has_dx1,
    noise_level1_test = b_noise_level_f1,
    noise_level2_test = b_noise_level_f2,
    noise_level3_test = - b_noise_level_f1 - b_noise_level_f2
  )

hyp_results = test_hypotheses(h_df, rope_values = c(-0.055, 0.055))
print(hyp_results %>% select(Parameter, pd, HDI_low, HDI_high, ROPE_Percentage, ROPE_Equivalence) %>% as.matrix())
options(digits = 7)
```

**Results**:

1. The effect of incorporating action suggestions (AX) has a probability 1.0 of being positive (Median = 1.21, 89% CI [0.68, 1.73]) and can be considered as significant (0.0% in ROPE).

(The ROPE is defined as [-0.055, 0.055]. It is the range is suggested in the literature for logistic models and it corresponds to a probability range of 0.11. Therefore, a probability change of less than 0.055 is considered no different from a probability change of 0)

So here are the following plots:

```{r eval=plot_posteriors}
# A visualization data frame
gg_df = plot_df %>%
    mutate(noise_level = fct_rev(noise_level)) %>%
    mutate(optimal_ax = factor(optimal_ax)) %>%
    mutate(user_id = factor(user_id)) %>%
    mutate(optimal_ax = fct_recode(optimal_ax, incorrect="0", correct = "1")) %>%
    mutate(.fill_column = paste(noise_level, !(study_condition == 'BASELINE'), sep = ":")) %>%
    count(user_id, study_condition, .fill_column, optimal_ax, num_actions, has_ax, has_dx, noise_level,
          name = "num_optimal_ax")

# Simulate predictions on new data
p_df =
  plot_df %>%
  data_grid(optimal_ax,
            has_ax,
            has_dx,
            noise_level_f,
            gender,
            age_group,
            robot_experience,
            num_optimal,
            state_idx_rescaled = seq_range(state_idx_rescaled, n = 4),
            .model = optimal_ax.model.v)

fits_p_df = p_df %>% add_fitted_draws(optimal_ax.model.v,
                                      re_formula = NA,
                                      n = 20,
                                      seed = default_seed)
gc()
# We don't want predicted draws because that's a raw count of ones and zeros; not the
# probability distribution
# preds_p_df = p_df %>% add_predicted_draws(scenario_completed.model.v)

# Plot the posterior distributions
rope_value = gg_df %>%
  filter(optimal_ax == 'correct', has_ax == F) %>%
  summarise(.value = median(num_optimal_ax / num_actions)) %>%
  pull(.value)
gg_df %>%
  filter(optimal_ax == 'correct') %>%
  ggplot(aes(x = has_ax, y = num_optimal_ax / num_actions)) +
    geom_rect(ymin = rope_value-0.055, ymax = rope_value+0.055,
              xmin = -Inf, xmax = Inf,
              color = "transparent", fill = "white") +
    annotate("text", x = 0.5, y = rope_value, label = "ROPE", angle = 90) +
    stat_halfeye(aes(y = .value), data = fits_p_df, position = position_nudge(x = 0.1)) +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep = ":")), size = 2) +
    stat_summary(fun.y = median, geom = "point", shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(y = "correct_ax", color = "AX:DX:Noise")

rope_value = gg_df %>%
  filter(optimal_ax == 'correct', has_dx == F) %>%
  summarise(.value = median(num_optimal_ax / num_actions)) %>%
  pull(.value)
gg_df %>%
  filter(optimal_ax == 'correct') %>%
  ggplot(aes(x = has_dx, y = num_optimal_ax / num_actions)) +
    geom_rect(ymin = rope_value-0.055, ymax = rope_value+0.055,
              xmin = -Inf, xmax = Inf,
              color = "transparent", fill = "white") +
    annotate("text", x = 0.5, y = rope_value, label = "ROPE", angle = 90) +
    stat_halfeye(aes(y = .value), data = fits_p_df, position = position_nudge(x = 0.1)) +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep = ":")), size = 2) +
    stat_summary(fun.y = median, geom = "point", shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(y = "correct_ax", color = "AX:DX:Noise")

rope_value = gg_df %>%
  filter(optimal_ax == 'correct', noise_level == "0.0") %>%
  summarise(.value = median(num_optimal_ax / num_actions)) %>%
  pull(.value)
gg_df %>%
  filter(optimal_ax == 'correct') %>%
  mutate(noise_level = fct_rev(noise_level)) %>%
  ggplot(aes(x = noise_level, y = num_optimal_ax / num_actions)) +
    geom_rect(ymin = rope_value-0.055, ymax = rope_value+0.055,
              xmin = -Inf, xmax = Inf,
              color = "transparent", fill = "white") +
    annotate("text", x = 0.5, y = rope_value, label = "ROPE", angle = 90) +
    stat_halfeye(aes(y = .value, x = noise_level_f), data = fits_p_df, position = position_nudge(x = 0.1)) +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep = ":")), size = 2) +
    stat_summary(fun.y = median, geom = "point", shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(y = "correct_ax", color = "AX:DX:Noise")

# Remove the giant fits data frames
rm(fits_p_df, p_df, gg_df)
```

```{r, echo=F, results=F, message=F, warning=F}
rm(optimal_ax.model.null, optimal_ax.model.t, optimal_ax.model.v)
gc()
```


## Following Action Suggestions

**Did the user follow the suggestions that were provided to them**

One can almost think of this as the "Compliance" metric mentioned by Jessie Yang et al. - If participants were "compliant" with the suggestions, then there will be a high rate of choosing the suggestions over other actions.

```{r}
plot_df = actions %>%
  filter(has_ax == T) %>%
  select(X1, id, user_id, study_condition, start_condition, num_optimal,
         state_idx, state_idx_rescaled,
         age_group, gender, robot_experience,
         noise_level, noise_level_f, has_noise, has_dx, has_ax,
         scenario_completed, num_actions, chose_ax)

text_short(report(plot_df))
```

```{r fig.height=7, fig.width=15}
gg_df =
  plot_df %>%
    mutate(noise_level = fct_rev(noise_level)) %>%
    mutate(chose_ax = factor(chose_ax)) %>%
    mutate(user_id = factor(user_id)) %>%
    mutate(chose_ax = fct_recode(chose_ax, no_follow="0", follow = "1")) %>%
    mutate(.fill_column = paste(noise_level, !(study_condition == 'BASELINE'), sep = ":")) %>%
    count(user_id, study_condition, .fill_column, chose_ax, num_actions, has_ax, has_dx, noise_level,
          name = "num_follow_ax")

# Plot by the study condition
gg_df %>%
  ggplot(aes(user_id, num_follow_ax / num_actions, fill=chose_ax)) +
    geom_bar(stat="identity") +
    geom_hline(yintercept = 0.5) +
    facet_wrap(vars(study_condition), nrow = 1, scales = "free_y") +
    labs(y = "Fraction followed AX") +
    coord_flip() +
    scale_fill_economist()

gg_df %>%
  filter(chose_ax == "follow") %>%
  ggplot(aes(study_condition, num_follow_ax / num_actions, fill=.fill_column)) +
    geom_violin() +
    geom_boxplot(width = 0.1) +
    labs(y = "Fraction followed AX", fill = "noise:has_suggestions") +
    scale_fill_economist()

# Visualize the data facetted by the variables that we care about
p2 = gg_df %>%
  filter(chose_ax == "follow") %>%
  ggplot(aes(has_dx, num_follow_ax / num_actions, fill=.fill_column)) +
    geom_boxplot() +
    geom_count() +
    facet_grid(rows = vars(noise_level), labeller = label_both) +
    labs(y = "Fraction followed AX") +
    scale_fill_economist() +
    theme(legend.position = "right")

p3 = gg_df %>%
  filter(chose_ax == "follow") %>%
  mutate(noise_level = fct_rev(noise_level)) %>%
  ggplot(aes(noise_level, num_follow_ax / num_actions, fill=.fill_column)) +
    geom_boxplot() +
    geom_count() +
    facet_grid(cols = vars(has_dx), labeller = label_both) +
    labs(y = "Fraction followed AX") +
    scale_fill_economist() +
    legend_none()

grid.arrange(p2, p3, ncol = 2)

# Visualize also the variation in each of the three levels that we care about
p2 = gg_df %>%
  filter(chose_ax == 'follow') %>%
  ggplot(aes(has_dx, num_follow_ax/num_actions)) +
    geom_violin() +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep=":"), size = 2)) +
    stat_summary(fun.y = median, geom="point", shape=23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    theme(legend.position = "right")

p3 = gg_df %>%
  filter(chose_ax == 'follow') %>%
  ggplot(aes(noise_level, num_follow_ax/num_actions)) +
    geom_violin() +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep=":"), size = 2)) +
    stat_summary(fun.y = median, geom="point", shape=23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    scale_x_discrete(limits = rev(levels(gg_df$noise_level))) +
    legend_none()

grid.arrange(p2, p3, ncol = 2)
```

Based on the data, we assume the following structural model:

$$follow\_ax_i = Bernoulli(p_i)$$
$$\begin{aligned}
logit(p_j) &= \beta_0 + \beta_{0i} + \beta_{dx}\text{dx}_j + \beta_{noise}\text{noise}_j + \beta_{dx:noise}\text{dx}_j\text{noise}_j + \\
&\beta_{no}\text{no}_j + \mathbf{\beta_{demo}X_{demo,j}} + \beta_{state} \text{state}_{ij}
\end{aligned}$$
$$\begin{aligned}
\beta_{.} &\sim Normal(0, 10) \\
\beta_{0i} &\sim Normal(0, \sigma_i) \\
\sigma_i &\sim HalfStudent(3, 0, 10)
\end{aligned}$$

The prior for the $\sigma_i$ parameter is the default used in `brms`; I see no need to change it.

Model fitting:

```{r, eval=train_models}
# The null model
chose_ax.model.null = brm(
  chose_ax ~ 0 + Intercept + (1 | user_id),
  family = "bernoulli",
  prior = c(set_prior("normal(0, 10)", class = "b"),
            set_prior("student_t(3, 0, 10)", class = "sd")),
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
chose_ax.model.null = add_criterion(chose_ax.model.null, "waic")
chose_ax.model.null = add_criterion(chose_ax.model.null, "loo", reloo = T)
saveModel(chose_ax.model.null)

# The trend model to see if there is a trend in the noise level variable
chose_ax.model.t = brm(
  chose_ax ~ 0 + Intercept + (has_dx * noise_level) + (age_group + robot_experience + gender) + num_optimal + state_idx_rescaled + (1 | user_id),
  family = bernoulli,
  prior = c(set_prior("normal(0, 10)", class = "b"),
            set_prior("student_t(3, 0, 10)", class = "sd")),
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
chose_ax.model.t = add_criterion(chose_ax.model.t, "waic")
chose_ax.model.t = add_criterion(chose_ax.model.t, "loo", reloo = T)
saveModel(chose_ax.model.t)

# The values model, to see if specific values of the noise level variable are significant
chose_ax.model.v = brm(
  chose_ax ~ 0 + Intercept + (has_dx * noise_level_f) + (age_group + robot_experience + gender) + num_optimal + state_idx_rescaled + (1 | user_id),
  family = bernoulli,
  prior = c(set_prior("normal(0, 10)", class = "b"),
            set_prior("student_t(3, 0, 10)", class = "sd")),
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
chose_ax.model.v = add_criterion(chose_ax.model.v, "waic")
chose_ax.model.v = add_criterion(chose_ax.model.v, "loo", reloo = T)
saveModel(chose_ax.model.v)
```
```{r, eval=!train_models}
# Load the models
chose_ax.model.null = loadModel("chose_ax.model.null")
chose_ax.model.t = loadModel("chose_ax.model.t")
chose_ax.model.v = loadModel("chose_ax.model.v")
```

Model fitting results:

```{r}
# print(tidy_stan(chose_ax.model.null))
print(performance::r2(chose_ax.model.null))

# Print the parameters, and some initial diagnostics
print(tidy_stan(chose_ax.model.t, effects = "fixed"))
print(performance::r2(chose_ax.model.t))

print(tidy_stan(chose_ax.model.v, effects = "fixed"))
print(performance::r2(chose_ax.model.v))
```

Diagnostics of the fit:

```{r fig.height=7, fig.width=15, eval=plot_diagnostics}
# Alternative diagnostics: diagnostic_posterior(scenario_completed.model.v)

# LOO cross-validation results
print(chose_ax.model.t$criteria$loo$estimates)
print(chose_ax.model.v$criteria$loo$estimates)
print(loo_compare(chose_ax.model.null, chose_ax.model.t, chose_ax.model.v))

# Diagnostics of the sampling process
par(mfrow=c(1,2))
plot(chose_ax.model.t$criteria$loo, main = "Trend Model")
plot(chose_ax.model.v$criteria$loo, main = "Values Model")

# Include posterior predictive checks of the sampling (are sampling according
# to the data that we actually have?)
p2 = pp_check(chose_ax.model.t, type = "bars_grouped", group = "has_dx") + ggtitle("t:has_dx")
p3 = pp_check(chose_ax.model.t, type = "bars_grouped", group = "noise_level") + ggtitle("t:noise_level")
grid.arrange(p2, p3, ncol = 2)
mcmc_intervals(as.matrix(chose_ax.model.t), regex_pars = "b_|^shape", prob = 0.90, prob_outer = 0.95) +
  ggtitle("Trends Model")

p2 = pp_check(chose_ax.model.v, type = "bars_grouped", group = "has_dx") + ggtitle("v:has_dx")
p3 = pp_check(chose_ax.model.v, type = "bars_grouped", group = "noise_level_f") + ggtitle("v:noise_level")
grid.arrange(p2, p3, nrow = 2)
mcmc_intervals(as.matrix(chose_ax.model.v), regex_pars = "b_|^shape", prob = 0.90, prob_outer = 0.95) +
  ggtitle("Values Model")

# Plot predictions vs original
preds_p_df = chose_ax.model.null %>% predict() %>% as_tibble()
p1 =
  preds_p_df %>%
    add_column(chose_ax = chose_ax.model.null$data$chose_ax) %>%
    ggplot(aes(x = Estimate, y = chose_ax, color = chose_ax)) +
      geom_point() +
      labs(x = "Predicted values") +
      ylim(0, 1) +
      xlim(0, 1) +
      ggtitle("Null Model")

preds_p_df = plot_df %>%
  add_predicted_draws(chose_ax.model.null) %>%
  group_by(chose_ax, .prediction) %>%
  count()
p2 =
  preds_p_df %>%
    ggplot(aes(x = chose_ax, y = .prediction, fill = n / sum(preds_p_df$n))) +
      geom_tile() +
      geom_text(aes(label = sprintf("%1.2f", n / sum(preds_p_df$n))), size = 10, vjust = 1) +
      scale_fill_gradient(limits = c(0, 1))

preds_p_df = chose_ax.model.t %>% predict() %>% as_tibble()
p3 =
  preds_p_df %>%
    add_column(chose_ax = chose_ax.model.t$data$chose_ax) %>%
    ggplot(aes(x = Estimate, y = chose_ax, color = chose_ax)) +
      geom_point() +
      labs(x = "Predicted values") +
      ylim(0, 1) +
      xlim(0, 1) +
      ggtitle("Trends Model")

preds_p_df = plot_df %>%
  add_predicted_draws(chose_ax.model.t) %>%
  group_by(chose_ax, .prediction) %>%
  count()
p4 =
  preds_p_df %>%
    ggplot(aes(x = chose_ax, y = .prediction, fill = n / sum(preds_p_df$n))) +
      geom_tile() +
      geom_text(aes(label = sprintf("%1.2f", n / sum(preds_p_df$n))), size = 10, vjust = 1) +
      scale_fill_gradient(limits = c(0, 1))

# Plot predictions vs original
preds_p_df = chose_ax.model.v %>% predict() %>% as_tibble()
p5 =
  preds_p_df %>%
    add_column(chose_ax = chose_ax.model.v$data$chose_ax) %>%
    ggplot(aes(x = Estimate, y = chose_ax, color = chose_ax)) +
      geom_point() +
      labs(x = "Predicted values") +
      ylim(0, 1) +
      xlim(0, 1) +
      ggtitle("Values Model")

preds_p_df = plot_df %>%
  add_predicted_draws(chose_ax.model.v) %>%
  group_by(chose_ax, .prediction) %>%
  count()
p6 =
  preds_p_df %>%
    ggplot(aes(x = chose_ax, y = .prediction, fill = n / sum(preds_p_df$n))) +
      geom_tile() +
      geom_text(aes(label = sprintf("%1.2f", n / sum(preds_p_df$n))), size = 10, vjust = 1) +
      scale_fill_gradient(limits = c(0, 1))

grid.arrange(p1, p3, p5, p2, p4, p6, nrow = 2, ncol = 3)
```

Inferences from the fitting:

```{r, eval=report_bayesfactors}
# Compute the bayes factor for each parameter value. Note: we compare
# here against the ROPE, and not the degenerate null hypothesis of 0
# Bayes-Factor values would be more significant in the latter case
# According to the Raftery, 1995 rules
# bf = 1 - 3: Weak
# bf = 3 - 20: Positive
# bf = 20 - 150: Strong
# bf > 150: Very strong
print(bayesfactor_rope(chose_ax.model.null, null = c(-0.055, 0.055)), n = 30)
print(bayesfactor_rope(chose_ax.model.t, null = c(-0.055, 0.055)), n = 30)
print(bayesfactor_rope(chose_ax.model.v, null = c(-0.055, 0.055)), n = 30)
```
```{r}
options(digits = 3)
# Check the significance of each hypothesis
h_df = as_tibble(insight::get_parameters(chose_ax.model.t)) %>%
  transmute(
    Intercept = b_Intercept,
    has_dx_test = -2 * b_has_dx1,
    noise_levelL_test = b_noise_level.L,
    noise_levelQ_test = b_noise_level.Q
  )

hyp_results = test_hypotheses(h_df, rope_values = c(-0.055, 0.055))
print(hyp_results %>% select(Parameter, pd, HDI_low, HDI_high, ROPE_Percentage, ROPE_Equivalence) %>% as.matrix())

h_df = as_tibble(insight::get_parameters(chose_ax.model.v)) %>%
  transmute(
    Intercept = b_Intercept,
    has_dx_test = -2 * b_has_dx1,
    noise_level1_test = b_noise_level_f1,
    noise_level2_test = b_noise_level_f2,
    noise_level3_test = - b_noise_level_f1 - b_noise_level_f2
  )

hyp_results = test_hypotheses(h_df, rope_values = c(-0.055, 0.055))
print(hyp_results %>% select(Parameter, pd, HDI_low, HDI_high, ROPE_Percentage, ROPE_Equivalence) %>% as.matrix())
options(digits = 7)
```

**Results**:

1. The effect of no noise in action suggestions (100% accurate) has a probability 0.97 of being positive (Median = 0.37, 89% CI [0.06, 0.86]) and can be considered as significant (0.0% in ROPE).

(The ROPE is defined as [-0.055, 0.055]. It is the range is suggested in the literature for logistic models and it corresponds to a probability range of 0.11. Therefore, a probability change of less than 0.055 is considered no different from a probability change of 0)

So here are the following plots:

```{r eval=plot_posteriors}
# A visualization data frame
gg_df = plot_df %>%
    mutate(noise_level = fct_rev(noise_level)) %>%
    mutate(chose_ax = factor(chose_ax)) %>%
    mutate(user_id = factor(user_id)) %>%
    mutate(chose_ax = fct_recode(chose_ax, no_follow="0", follow = "1")) %>%
    mutate(.fill_column = paste(noise_level, !(study_condition == 'BASELINE'), sep = ":")) %>%
    count(user_id, study_condition, .fill_column, chose_ax, num_actions, has_ax, has_dx, noise_level,
          name = "num_follow_ax")

# Simulate predictions on new data
p_df =
  plot_df %>%
  data_grid(chose_ax,
            has_dx,
            noise_level_f,
            gender,
            age_group,
            robot_experience,
            num_optimal,
            state_idx_rescaled = seq_range(state_idx_rescaled, n = 4),
            .model = chose_ax.model.v)

fits_p_df = p_df %>% add_fitted_draws(chose_ax.model.v,
                                      re_formula = NA,
                                      seed = default_seed)
gc()
# We don't want predicted draws because that's a raw count of ones and zeros; not the
# probability distribution
# preds_p_df = p_df %>% add_predicted_draws(scenario_completed.model.v)

# Plot the posterior distributions
rope_value = gg_df %>%
  filter(chose_ax == 'follow', has_dx == F) %>%
  summarise(.value = median(num_follow_ax / num_actions)) %>%
  pull(.value)
gg_df %>%
  filter(chose_ax == 'follow') %>%
  ggplot(aes(x = has_dx, y = num_follow_ax / num_actions)) +
    geom_rect(ymin = rope_value-0.055, ymax = rope_value+0.055,
              xmin = -Inf, xmax = Inf,
              color = "transparent", fill = "white") +
    annotate("text", x = 0.5, y = rope_value, label = "ROPE", angle = 90) +
    stat_halfeye(aes(y = .value), data = fits_p_df, position = position_nudge(x = 0.1)) +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep = ":")), size = 2) +
    stat_summary(fun.y = median, geom = "point", shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(y = "follow_ax", color = "AX:DX:Noise")

rope_value = gg_df %>%
  filter(chose_ax == 'follow', noise_level == "0.0") %>%
  summarise(.value = median(num_follow_ax / num_actions)) %>%
  pull(.value)
gg_df %>%
  filter(chose_ax == 'follow') %>%
  mutate(noise_level = fct_rev(noise_level)) %>%
  ggplot(aes(x = noise_level, y = num_follow_ax / num_actions)) +
    geom_rect(ymin = rope_value-0.055, ymax = rope_value+0.055,
              xmin = -Inf, xmax = Inf,
              color = "transparent", fill = "white") +
    annotate("text", x = 0.5, y = rope_value, label = "ROPE", angle = 90) +
    stat_halfeye(aes(y = .value, x = noise_level_f), data = fits_p_df, position = position_nudge(x = 0.1)) +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep = ":")), size = 2) +
    stat_summary(fun.y = median, geom = "point", shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(y = "follow_ax", color = "AX:DX:Noise")

# Remove the giant fits data frames
rm(fits_p_df, p_df, gg_df)
```

```{r, echo=F, results=F, message=F, warning=F}
rm(chose_ax.model.null, chose_ax.model.t, chose_ax.model.v)
gc()
```

## Figuring out the Correct Diagnoses

**Did the user figure out the correct diagnoses for their situation**

One can almost think of this as an inverse of the "Reliance" metric mentioned by Jessie Yang et al. - If participants were "reliant" on the suggestions, then those under the noise condition would perform worse.

```{r}
plot_df = actions %>%
  select(X1, id, user_id, study_condition, start_condition, num_optimal,
         state_idx, state_idx_rescaled,
         age_group, gender, robot_experience,
         noise_level, noise_level_f, has_noise, has_dx, has_ax,
         scenario_completed, num_actions, optimal_dx)

text_short(report(plot_df))
```

```{r fig.height=7, fig.width=15}
gg_df =
  plot_df %>%
    mutate(noise_level = fct_rev(noise_level)) %>%
    mutate(optimal_dx = factor(optimal_dx)) %>%
    mutate(user_id = factor(user_id)) %>%
    mutate(optimal_dx = fct_recode(optimal_dx, incorrect="0", correct = "1")) %>%
    mutate(.fill_column = paste(noise_level, !(study_condition == 'BASELINE'), sep = ":")) %>%
    count(user_id, study_condition, .fill_column, optimal_dx, num_actions, has_ax, has_dx, noise_level,
          name = "num_optimal_dx")

# Plot by the study condition
gg_df %>%
  ggplot(aes(user_id, num_optimal_dx / num_actions, fill=optimal_dx)) +
    geom_bar(stat="identity") +
    geom_hline(yintercept = 0.5) +
    facet_wrap(vars(study_condition), nrow = 1, scales = "free_y") +
    labs(y = "Fraction correct diagnoses") +
    coord_flip() +
    scale_fill_economist()

gg_df %>%
  filter(optimal_dx == "correct") %>%
  ggplot(aes(study_condition, num_optimal_dx / num_actions, fill=.fill_column)) +
    geom_violin() +
    geom_boxplot(width = 0.1) +
    labs(y = "Fraction correct diagnoses", fill = "noise:has_suggestions") +
    scale_fill_economist()

# Visualize the data facetted by the variables that we care about
p1 = gg_df %>%
  filter(optimal_dx == "correct") %>%
  ggplot(aes(has_ax, num_optimal_dx / num_actions, fill=.fill_column)) +
    geom_boxplot() +
    geom_count() +
    facet_grid(rows = vars(noise_level), cols = vars(has_dx), labeller = label_both) +
    labs(y = "Fraction correct diagnoses") +
    scale_fill_economist() +
    theme(legend.position = "left")

p2 = gg_df %>%
  filter(optimal_dx == "correct") %>%
  ggplot(aes(has_dx, num_optimal_dx / num_actions, fill=.fill_column)) +
    geom_boxplot() +
    geom_count() +
    facet_grid(rows = vars(noise_level), cols = vars(has_ax), labeller = label_both) +
    labs(y = "Fraction correct diagnoses") +
    scale_fill_economist() +
    legend_none()

p3 = gg_df %>%
  filter(optimal_dx == "correct") %>%
  mutate(noise_level = fct_rev(noise_level)) %>%
  mutate(has_ax = factor(has_ax)) %>%
  mutate(has_ax = fct_rev(has_ax)) %>%
  ggplot(aes(noise_level, num_optimal_dx / num_actions, fill=.fill_column)) +
    geom_boxplot() +
    geom_count() +
    facet_grid(rows = vars(has_ax), cols = vars(has_dx), labeller = label_both) +
    labs(y = "Fraction correct diagnoses") +
    scale_fill_economist() +
    legend_none()

grid.arrange(p1, p2, p3, ncol = 3)

# Visualize also the variation in each of the three levels that we care about
p1 = gg_df %>%
  filter(optimal_dx == 'correct') %>%
  ggplot(aes(has_ax, num_optimal_dx/num_actions)) +
    geom_violin() +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep=":"), size = 2)) +
    stat_summary(fun.y = median, geom="point", shape=23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(color = "AX:DX:Noise") +
    theme(legend.position = "left") +
    guides(size = F)

p2 = gg_df %>%
  filter(optimal_dx == 'correct') %>%
  ggplot(aes(has_dx, num_optimal_dx/num_actions)) +
    geom_violin() +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep=":"), size = 2)) +
    stat_summary(fun.y = median, geom="point", shape=23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    legend_none()

p3 = gg_df %>%
  filter(optimal_dx == 'correct') %>%
  ggplot(aes(noise_level, num_optimal_dx/num_actions)) +
    geom_violin() +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep=":"), size = 2)) +
    stat_summary(fun.y = median, geom="point", shape=23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    scale_x_discrete(limits = rev(levels(gg_df$noise_level))) +
    legend_none()

grid.arrange(p1, p2, p3, ncol = 3)
```

Based on the data, we assume the following structural model:

$$correct\_diagnosis_i = Bernoulli(p_i)$$
$$\begin{aligned}
logit(p_j) &= \beta_0 + \beta_{0i} + \beta_{ax}\text{ax}_j + \beta_{dx}\text{dx}_j + \beta_{noise}\text{noise}_j +\\
&\beta_{ax:noise}\text{ax}_j\text{noise}_j + \beta_{dx:noise}\text{dx}_j\text{noise}_j + \\
&\beta_{no}\text{no}_j + \mathbf{\beta_{demo}X_{demo,j}} + \beta_{state} \text{state}_{ij}
\end{aligned}$$
$$\begin{aligned}
\beta_{.} &\sim Normal(0, 10) \\
\beta_{0i} &\sim Normal(0, \sigma_i) \\
\sigma_i &\sim HalfStudent(3, 0, 10)
\end{aligned}$$

The prior for the $\sigma_i$ parameter is the default used in `brms`; I see no need to change it.

Model fitting:

```{r, eval=train_models}
# The null model
optimal_dx.model.null = brm(
  optimal_dx ~ 0 + Intercept + (1 | user_id),
  family = "bernoulli",
  prior = c(set_prior("normal(0, 10)", class = "b"),
            set_prior("student_t(3, 0, 10)", class = "sd")),
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
optimal_dx.model.null = add_criterion(optimal_dx.model.null, "waic")
optimal_dx.model.null = add_criterion(optimal_dx.model.null, "loo", reloo = T)
saveModel(optimal_dx.model.null)

# The trend model to see if there is a trend in the noise level variable
optimal_dx.model.t = brm(
  optimal_dx ~ 0 + Intercept + ((has_dx + has_ax) * noise_level) + (age_group + robot_experience + gender) + num_optimal + state_idx_rescaled + (1 | user_id),
  family = bernoulli,
  prior = c(set_prior("normal(0, 10)", class = "b"),
            set_prior("student_t(3, 0, 10)", class = "sd")),
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
optimal_dx.model.t = add_criterion(optimal_dx.model.t, "waic")
optimal_dx.model.t = add_criterion(optimal_dx.model.t, "loo", reloo = T)
saveModel(optimal_dx.model.t)

# The values model, to see if specific values of the noise level variable are significant
optimal_dx.model.v = brm(
  optimal_dx ~ 0 + Intercept + ((has_dx + has_ax) * noise_level_f) + (age_group + robot_experience + gender) + num_optimal + state_idx_rescaled + (1 | user_id),
  family = bernoulli,
  prior = c(set_prior("normal(0, 10)", class = "b"),
            set_prior("student_t(3, 0, 10)", class = "sd")),
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
optimal_dx.model.v = add_criterion(optimal_dx.model.v, "waic")
optimal_dx.model.v = add_criterion(optimal_dx.model.v, "loo", reloo = T)
saveModel(optimal_dx.model.v)
```
```{r, eval=!train_models}
# Load the models
optimal_dx.model.null = loadModel("optimal_dx.model.null")
optimal_dx.model.t = loadModel("optimal_dx.model.t")
optimal_dx.model.v = loadModel("optimal_dx.model.v")
```

Model fitting results:

```{r}
# print(tidy_stan(optimal_dx.model.null))
print(performance::r2(optimal_dx.model.null))

# Print the parameters, and some initial diagnostics
print(tidy_stan(optimal_dx.model.t, effects = "fixed"))
print(performance::r2(optimal_dx.model.t))

print(tidy_stan(optimal_dx.model.v, effects = "fixed"))
print(performance::r2(optimal_dx.model.v))
```

Diagnostics of the fit:

```{r fig.height=7, fig.width=15, eval=plot_diagnostics}
# Alternative diagnostics: diagnostic_posterior(scenario_completed.model.v)

# LOO cross-validation results
print(optimal_dx.model.t$criteria$loo$estimates)
print(optimal_dx.model.v$criteria$loo$estimates)
print(loo_compare(optimal_dx.model.null, optimal_dx.model.t, optimal_dx.model.v))

# Diagnostics of the sampling process
par(mfrow=c(1,2))
plot(optimal_dx.model.t$criteria$loo, main = "Trend Model")
plot(optimal_dx.model.v$criteria$loo, main = "Values Model")

# Include posterior predictive checks of the sampling (are sampling according
# to the data that we actually have?)
p1 = pp_check(optimal_dx.model.t, type = "bars_grouped", group = "has_ax", stat = "median") + ggtitle("t:has_ax")
p2 = pp_check(optimal_dx.model.t, type = "bars_grouped", group = "has_dx") + ggtitle("t:has_dx")
p3 = pp_check(optimal_dx.model.t, type = "bars_grouped", group = "noise_level") + ggtitle("t:noise_level")
grid.arrange(p1, p2, p3, nrow = 3)
mcmc_intervals(as.matrix(optimal_dx.model.t), regex_pars = "b_|^shape", prob = 0.90, prob_outer = 0.95) +
  ggtitle("Trends Model")

p1 = pp_check(optimal_dx.model.v, type = "bars_grouped", group = "has_ax") + ggtitle("v:has_ax")
p2 = pp_check(optimal_dx.model.v, type = "bars_grouped", group = "has_dx") + ggtitle("v:has_dx")
p3 = pp_check(optimal_dx.model.v, type = "bars_grouped", group = "noise_level_f") + ggtitle("v:noise_level")
grid.arrange(p1, p2, p3, nrow = 3)
mcmc_intervals(as.matrix(optimal_dx.model.v), regex_pars = "b_|^shape", prob = 0.90, prob_outer = 0.95) +
  ggtitle("Values Model")

# Plot predictions vs original
preds_p_df = optimal_dx.model.null %>% predict() %>% as_tibble()
p1 =
  preds_p_df %>%
    add_column(optimal_dx = optimal_dx.model.null$data$optimal_dx) %>%
    ggplot(aes(x = Estimate, y = optimal_dx, color = optimal_dx)) +
      geom_point() +
      labs(x = "Predicted values") +
      ylim(0, 1) +
      xlim(0, 1) +
      ggtitle("Null Model")

preds_p_df = plot_df %>%
  add_predicted_draws(optimal_dx.model.null) %>%
  group_by(optimal_dx, .prediction) %>%
  count()
p2 =
  preds_p_df %>%
    ggplot(aes(x = optimal_dx, y = .prediction, fill = n / sum(preds_p_df$n))) +
      geom_tile() +
      geom_text(aes(label = sprintf("%1.2f", n / sum(preds_p_df$n))), size = 10, vjust = 1) +
      scale_fill_gradient(limits = c(0, 1))

preds_p_df = optimal_dx.model.t %>% predict() %>% as_tibble()
p3 =
  preds_p_df %>%
    add_column(optimal_dx = optimal_dx.model.t$data$optimal_dx) %>%
    ggplot(aes(x = Estimate, y = optimal_dx, color = optimal_dx)) +
      geom_point() +
      labs(x = "Predicted values") +
      ylim(0, 1) +
      xlim(0, 1) +
      ggtitle("Trends Model")

preds_p_df = plot_df %>%
  add_predicted_draws(optimal_dx.model.t) %>%
  group_by(optimal_dx, .prediction) %>%
  count()
p4 =
  preds_p_df %>%
    ggplot(aes(x = optimal_dx, y = .prediction, fill = n / sum(preds_p_df$n))) +
      geom_tile() +
      geom_text(aes(label = sprintf("%1.2f", n / sum(preds_p_df$n))), size = 10, vjust = 1) +
      scale_fill_gradient(limits = c(0, 1))

# Plot predictions vs original
preds_p_df = optimal_dx.model.v %>% predict() %>% as_tibble()
p5 =
  preds_p_df %>%
    add_column(optimal_dx = optimal_dx.model.v$data$optimal_dx) %>%
    ggplot(aes(x = Estimate, y = optimal_dx, color = optimal_dx)) +
      geom_point() +
      labs(x = "Predicted values") +
      ylim(0, 1) +
      xlim(0, 1) +
      ggtitle("Values Model")

preds_p_df = plot_df %>%
  add_predicted_draws(optimal_dx.model.v) %>%
  group_by(optimal_dx, .prediction) %>%
  count()
p6 =
  preds_p_df %>%
    ggplot(aes(x = optimal_dx, y = .prediction, fill = n / sum(preds_p_df$n))) +
      geom_tile() +
      geom_text(aes(label = sprintf("%1.2f", n / sum(preds_p_df$n))), size = 10, vjust = 1) +
      scale_fill_gradient(limits = c(0, 1))

grid.arrange(p1, p3, p5, p2, p4, p6, nrow = 2, ncol = 3)
```

Inferences from the fitting:

```{r, eval=report_bayesfactors}
# Compute the bayes factor for each parameter value. Note: we compare
# here against the ROPE, and not the degenerate null hypothesis of 0
# Bayes-Factor values would be more significant in the latter case
# According to the Raftery, 1995 rules
# bf = 1 - 3: Weak
# bf = 3 - 20: Positive
# bf = 20 - 150: Strong
# bf > 150: Very strong
print(bayesfactor_rope(optimal_dx.model.null, null = c(-0.055, 0.055)), n = 30)
print(bayesfactor_rope(optimal_dx.model.t, null = c(-0.055, 0.055)), n = 30)
print(bayesfactor_rope(optimal_dx.model.v, null = c(-0.055, 0.055)), n = 30)
```
```{r}
options(digits = 3)
# Check the significance of each hypothesis
h_df = as_tibble(insight::get_parameters(optimal_dx.model.t)) %>%
  transmute(
    Intercept = b_Intercept,
    has_ax_test = -2 * b_has_ax1,
    has_dx_test = -2 * b_has_dx1,
    noise_levelL_test = b_noise_level.L,
    noise_levelQ_test = b_noise_level.Q
  )

hyp_results = test_hypotheses(h_df, rope_values = c(-0.055, 0.055))
print(hyp_results %>% select(Parameter, pd, HDI_low, HDI_high, ROPE_Percentage, ROPE_Equivalence) %>% as.matrix())

h_df = as_tibble(insight::get_parameters(optimal_dx.model.v)) %>%
  transmute(
    Intercept = b_Intercept,
    has_ax_test = -2 * b_has_ax1,
    has_dx_test = -2 * b_has_dx1,
    noise_level1_test = b_noise_level_f1,
    noise_level2_test = b_noise_level_f2,
    noise_level3_test = - b_noise_level_f1 - b_noise_level_f2
  )

hyp_results = test_hypotheses(h_df, rope_values = c(-0.055, 0.055))
print(hyp_results %>% select(Parameter, pd, HDI_low, HDI_high, ROPE_Percentage, ROPE_Equivalence) %>% as.matrix())
options(digits = 7)
```

**Results**:

1. The effect of incorporating diagnosis suggestions (DX) has a probability 0.98 of being positive (Median = 0.56, 89% CI [0.19, 1.00]) and can be considered as significant (0.0% in ROPE).
1. The noise level has a linear relationship to the probability of choosing the right diagnoses with a negative effect size (negative slope) with probability of 0.97 (Median = -0.42, 89% CI [-0.76, -0.09]) and can be considered significant (0.0% in ROPE).

(The ROPE is defined as [-0.055, 0.055]. It is the range is suggested in the literature for logistic models and it corresponds to a probability range of 0.11. Therefore, a probability change of less than 0.055 is considered no different from a probability change of 0)

So here are the following plots:

```{r eval=plot_posteriors}
# A visualization data frame
gg_df = plot_df %>%
    mutate(noise_level = fct_rev(noise_level)) %>%
    mutate(optimal_dx = factor(optimal_dx)) %>%
    mutate(user_id = factor(user_id)) %>%
    mutate(optimal_dx = fct_recode(optimal_dx, incorrect="0", correct = "1")) %>%
    mutate(.fill_column = paste(noise_level, !(study_condition == 'BASELINE'), sep = ":")) %>%
    count(user_id, study_condition, .fill_column, optimal_dx, num_actions, has_ax, has_dx, noise_level,
          name = "num_optimal_dx")

# Simulate predictions on new data
p_df =
  plot_df %>%
  data_grid(optimal_dx,
            has_ax,
            has_dx,
            noise_level_f,
            gender,
            age_group,
            robot_experience,
            num_optimal,
            state_idx_rescaled = seq_range(state_idx_rescaled, n = 4),
            .model = optimal_dx.model.v)

fits_p_df = p_df %>% add_fitted_draws(optimal_dx.model.v,
                                      re_formula = NA,
                                      n = 20,
                                      seed = default_seed)
gc()
pars_p_df = optimal_dx.model.t %>%
  extract_draws(newdata = p_df %>% rename(noise_level = noise_level_f),
                re_formula = NA,
                nsamples = 100)
gc()

# We don't want predicted draws because that's a raw count of ones and zeros; not the
# probability distribution
# preds_p_df = p_df %>% add_predicted_draws(scenario_completed.model.v)

# Plot the posterior distributions
rope_value = gg_df %>%
  filter(optimal_dx == 'correct', has_ax == F) %>%
  summarise(.value = median(num_optimal_dx / num_actions)) %>%
  pull(.value)
gg_df %>%
  filter(optimal_dx == 'correct') %>%
  ggplot(aes(x = has_ax, y = num_optimal_dx / num_actions)) +
    geom_rect(ymin = rope_value-0.055, ymax = rope_value+0.055,
              xmin = -Inf, xmax = Inf,
              color = "transparent", fill = "white") +
    annotate("text", x = 0.5, y = rope_value, label = "ROPE", angle = 90) +
    stat_halfeye(aes(y = .value), data = fits_p_df, position = position_nudge(x = 0.1)) +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep = ":")), size = 2) +
    stat_summary(fun.y = median, geom = "point", shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(y = "correct_ax", color = "AX:DX:Noise")

rope_value = gg_df %>%
  filter(optimal_dx == 'correct', has_dx == F) %>%
  summarise(.value = median(num_optimal_dx / num_actions)) %>%
  pull(.value)
gg_df %>%
  filter(optimal_dx == 'correct') %>%
  ggplot(aes(x = has_dx, y = num_optimal_dx / num_actions)) +
    geom_rect(ymin = rope_value-0.055, ymax = rope_value+0.055,
              xmin = -Inf, xmax = Inf,
              color = "transparent", fill = "white") +
    annotate("text", x = 0.5, y = rope_value, label = "ROPE", angle = 90) +
    stat_halfeye(aes(y = .value), data = fits_p_df, position = position_nudge(x = 0.1)) +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep = ":")), size = 2) +
    stat_summary(fun.y = median, geom = "point", shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(y = "correct_ax", color = "AX:DX:Noise")

rope_value = gg_df %>%
  filter(optimal_dx == 'correct', noise_level == "0.0") %>%
  summarise(.value = median(num_optimal_dx / num_actions)) %>%
  pull(.value)
gg_df %>%
  filter(optimal_dx == 'correct') %>%
  mutate(noise_level = fct_rev(noise_level)) %>%
  ggplot(aes(x = noise_level, y = num_optimal_dx / num_actions)) +
    geom_rect(ymin = rope_value-0.055, ymax = rope_value+0.055,
              xmin = -Inf, xmax = Inf,
              color = "transparent", fill = "white") +
    annotate("text", x = 0.5, y = rope_value, label = "ROPE", angle = 90) +
    stat_halfeye(aes(y = .value, x = noise_level_f), data = fits_p_df, position = position_nudge(x = 0.1)) +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep = ":")), size = 2) +
    stat_summary(fun.y = median, geom = "point", shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(y = "correct_ax", color = "AX:DX:Noise")

# Create a plot to show the trend
pars_p_df_X = pars_p_df$dpars$mu$fe$X[,c("Intercept", "noise_level.L")]
pars_p_df_b = pars_p_df$dpars$mu$fe$b[,c("b_Intercept", "b_noise_level.L")]
pars_p_df_y = pars_p_df_X %*% t(pars_p_df_b)
pars_p_df_y = inv_logit_scaled(pars_p_df_y)
p_df =
  p_df %>%
  bind_cols(as_tibble(pars_p_df_y)) %>%
  gather(key = ".sample", value = ".value", V1:V100) %>%
  rename(noise_level = noise_level_f)

rope_value = gg_df %>%
  filter(optimal_dx == 'correct', noise_level == "0.0") %>%
  summarise(.value = median(num_optimal_dx / num_actions)) %>%
  pull(.value)
gg_df %>%
  filter(optimal_dx == 'correct') %>%
  mutate(noise_level = fct_rev(noise_level)) %>%
  ggplot(aes(x = noise_level, y = num_optimal_dx / num_actions)) +
    geom_rect(ymin = rope_value-0.055, ymax = rope_value+0.055,
              xmin = -Inf, xmax = Inf,
              color = "transparent", fill = "white") +
    annotate("text", x = 0.5, y = rope_value, label = "ROPE", angle = 90) +
    stat_halfeye(aes(y = .value, x = noise_level_f), data = fits_p_df, position = position_nudge(x = 0.1)) +
    geom_line(aes(y = .value, group = .sample, alpha = 0.3), data = p_df) +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep = ":")), size = 2) +
    stat_summary(fun.y = median, geom = "point", shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(y = "scenario_completed", color = "AX:DX:Noise") +
    guides(alpha = F)

# Remove the giant fits data frames
rm(fits_p_df, pars_p_df, pars_p_df_b, pars_p_df_X, pars_p_df_y, p_df, gg_df)
```

```{r, echo=F, results=F, message=F, warning=F}
rm(optimal_dx.model.null, optimal_dx.model.t, optimal_dx.model.v)
gc()
```


## Following Diagnosis Suggestions

**Did the user take an follow the suggestions that were provided to them**

One can almost think of this as the "Compliance" metric mentioned by Jessie Yang et al. - If participants were "compliant" with the suggestions, then there will be a high rate of choosing the suggestions over other diagnoses

```{r}
plot_df = actions %>%
  filter(has_dx == T) %>%
  select(X1, id, user_id, study_condition, start_condition, num_optimal,
         state_idx, state_idx_rescaled,
         age_group, gender, robot_experience,
         noise_level, noise_level_f, has_noise, has_dx, has_ax,
         scenario_completed, num_actions, chose_dx)

text_short(report(plot_df))
```

```{r fig.height=7, fig.width=15}
gg_df =
  plot_df %>%
    mutate(noise_level = fct_rev(noise_level)) %>%
    mutate(chose_dx = factor(chose_dx)) %>%
    mutate(user_id = factor(user_id)) %>%
    mutate(chose_dx = fct_recode(chose_dx, no_follow="0", follow = "1")) %>%
    mutate(.fill_column = paste(noise_level, !(study_condition == 'BASELINE'), sep = ":")) %>%
    count(user_id, study_condition, .fill_column, chose_dx, num_actions, has_ax, has_dx, noise_level,
          name = "num_follow_dx")

# Plot by the study condition
gg_df %>%
  ggplot(aes(user_id, num_follow_dx / num_actions, fill=chose_dx)) +
    geom_bar(stat="identity") +
    geom_hline(yintercept = 0.5) +
    facet_wrap(vars(study_condition), nrow = 1, scales = "free_y") +
    labs(y = "Fraction followed DX") +
    coord_flip() +
    scale_fill_economist()

gg_df %>%
  filter(chose_dx == "follow") %>%
  ggplot(aes(study_condition, num_follow_dx / num_actions, fill=.fill_column)) +
    geom_violin() +
    geom_boxplot(width = 0.1) +
    labs(y = "Fraction followed DX", fill = "noise:has_suggestions") +
    scale_fill_economist()

# Visualize the data facetted by the variables that we care about
p1 = gg_df %>%
  filter(chose_dx == "follow") %>%
  ggplot(aes(has_ax, num_follow_dx / num_actions, fill=.fill_column)) +
    geom_boxplot() +
    geom_count() +
    facet_grid(rows = vars(noise_level), labeller = label_both) +
    labs(y = "Fraction followed DX") +
    scale_fill_economist() +
    theme(legend.position = "right")

p3 = gg_df %>%
  filter(chose_dx == "follow") %>%
  mutate(noise_level = fct_rev(noise_level)) %>%
  ggplot(aes(noise_level, num_follow_dx / num_actions, fill=.fill_column)) +
    geom_boxplot() +
    geom_count() +
    facet_grid(cols = vars(has_ax), labeller = label_both) +
    labs(y = "Fraction followed DX") +
    scale_fill_economist() +
    legend_none()

grid.arrange(p1, p3, ncol = 2)

# Visualize also the variation in each of the three levels that we care about
p1 = gg_df %>%
  filter(chose_dx == 'follow') %>%
  ggplot(aes(has_ax, num_follow_dx/num_actions)) +
    geom_violin() +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep=":"), size = 2)) +
    stat_summary(fun.y = median, geom="point", shape=23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    theme(legend.position = "right")

p3 = gg_df %>%
  filter(chose_dx == 'follow') %>%
  ggplot(aes(noise_level, num_follow_dx/num_actions)) +
    geom_violin() +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep=":"), size = 2)) +
    stat_summary(fun.y = median, geom="point", shape=23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    scale_x_discrete(limits = rev(levels(gg_df$noise_level))) +
    legend_none()

grid.arrange(p1, p3, ncol = 2)
```

Based on the data, we assume the following structural model:

$$follow\_dx_i = Bernoulli(p_i)$$
$$\begin{aligned}
logit(p_j) &= \beta_0 + \beta_{0i} + \beta_{ax}\text{ax}_j + \beta_{noise}\text{noise}_j + \beta_{ax:noise}\text{ax}_j\text{noise}_j + \\
&\beta_{no}\text{no}_j + \mathbf{\beta_{demo}X_{demo,j}} + \beta_{state} \text{state}_{ij}
\end{aligned}$$
$$\begin{aligned}
\beta_{.} &\sim Normal(0, 10) \\
\beta_{0i} &\sim Normal(0, \sigma_i) \\
\sigma_i &\sim HalfStudent(3, 0, 10)
\end{aligned}$$

The prior for the $\sigma_i$ parameter is the default used in `brms`; I see no need to change it.

Model fitting:

```{r, eval=train_models}
# The null model
chose_dx.model.null = brm(
  chose_dx ~ 0 + Intercept + (1 | user_id),
  family = "bernoulli",
  prior = set_prior("normal(0, 10)", class = "b"),
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
chose_dx.model.null = add_criterion(chose_dx.model.null, "waic")
chose_dx.model.null = add_criterion(chose_dx.model.null, "loo", reloo = T)
saveModel(chose_dx.model.null)

# The trend model to see if there is a trend in the noise level variable
chose_dx.model.t = brm(
  chose_dx ~ 0 + Intercept + (has_ax * noise_level) + (age_group + robot_experience + gender) + num_optimal + state_idx_rescaled + (1 | user_id),
  family = bernoulli,
  prior = c(set_prior("normal(0, 10)", class = "b"),
            set_prior("student_t(3, 0, 10)", class = "sd")),
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
chose_dx.model.t = add_criterion(chose_dx.model.t, "waic")
chose_dx.model.t = add_criterion(chose_dx.model.t, "loo", reloo = T)
saveModel(chose_dx.model.t)

# The values model, to see if specific values of the noise level variable are significant
chose_dx.model.v = brm(
  chose_dx ~ 0 + Intercept + (has_ax * noise_level_f) + (age_group + robot_experience + gender) + num_optimal + state_idx_rescaled + (1 | user_id),
  family = bernoulli,
  prior = c(set_prior("normal(0, 10)", class = "b"),
            set_prior("student_t(3, 0, 10)", class = "sd")),
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
chose_dx.model.v = add_criterion(chose_dx.model.v, "waic")
chose_dx.model.v = add_criterion(chose_dx.model.v, "loo", reloo = T)
saveModel(chose_dx.model.v)
```
```{r, eval=!train_models}
# Load the models
chose_dx.model.null = loadModel("chose_dx.model.null")
chose_dx.model.t = loadModel("chose_dx.model.t")
chose_dx.model.v = loadModel("chose_dx.model.v")
```

Model fitting results:

```{r}
# print(tidy_stan(chose_ax.model.null))
print(performance::r2(chose_dx.model.null))

# Print the parameters, and some initial diagnostics
print(tidy_stan(chose_dx.model.t, effects = "fixed"))
print(performance::r2(chose_dx.model.t))

print(tidy_stan(chose_dx.model.v, effects = "fixed"))
print(performance::r2(chose_dx.model.v))
```

Diagnostics of the fit:

```{r fig.height=7, fig.width=15, eval=plot_diagnostics}
# Alternative diagnostics: diagnostic_posterior(scenario_completed.model.v)

# LOO cross-validation results
print(chose_dx.model.t$criteria$loo$estimates)
print(chose_dx.model.v$criteria$loo$estimates)
print(loo_compare(chose_dx.model.null, chose_dx.model.t, chose_dx.model.v))

# Diagnostics of the sampling process
par(mfrow=c(1,2))
plot(chose_dx.model.t$criteria$loo, main = "Trend Model")
plot(chose_dx.model.v$criteria$loo, main = "Values Model")

# Include posterior predictive checks of the sampling (are sampling according
# to the data that we actually have?)
p1 = pp_check(chose_dx.model.t, type = "bars_grouped", group = "has_ax") + ggtitle("t:has_ax")
p3 = pp_check(chose_dx.model.t, type = "bars_grouped", group = "noise_level") + ggtitle("t:noise_level")
grid.arrange(p1, p3, ncol = 2)
mcmc_intervals(as.matrix(chose_dx.model.t), regex_pars = "b_|^shape", prob = 0.90, prob_outer = 0.95) +
  ggtitle("Trends Model")

p1 = pp_check(chose_dx.model.v, type = "bars_grouped", group = "has_ax") + ggtitle("v:has_ax")
p3 = pp_check(chose_dx.model.v, type = "bars_grouped", group = "noise_level_f") + ggtitle("v:noise_level")
grid.arrange(p1, p3, nrow = 2)
mcmc_intervals(as.matrix(chose_dx.model.v), regex_pars = "b_|^shape", prob = 0.90, prob_outer = 0.95) +
  ggtitle("Values Model")

# Plot predictions vs original
preds_p_df = chose_dx.model.null %>% predict() %>% as_tibble()
p1 =
  preds_p_df %>%
    add_column(chose_dx = chose_dx.model.null$data$chose_dx) %>%
    ggplot(aes(x = Estimate, y = chose_dx, color = chose_dx)) +
      geom_point() +
      labs(x = "Predicted values") +
      ylim(0, 1) +
      xlim(0, 1) +
      ggtitle("Null Model")

preds_p_df = plot_df %>%
  add_predicted_draws(chose_dx.model.null) %>%
  group_by(chose_dx, .prediction) %>%
  count()
p2 =
  preds_p_df %>%
    ggplot(aes(x = chose_dx, y = .prediction, fill = n / sum(preds_p_df$n))) +
      geom_tile() +
      geom_text(aes(label = sprintf("%1.2f", n / sum(preds_p_df$n))), size = 10, vjust = 1) +
      scale_fill_gradient(limits = c(0, 1))

preds_p_df = chose_dx.model.t %>% predict() %>% as_tibble()
p3 =
  preds_p_df %>%
    add_column(chose_dx = chose_dx.model.t$data$chose_dx) %>%
    ggplot(aes(x = Estimate, y = chose_dx, color = chose_dx)) +
      geom_point() +
      labs(x = "Predicted values") +
      ylim(0, 1) +
      xlim(0, 1) +
      ggtitle("Trends Model")

preds_p_df = plot_df %>%
  add_predicted_draws(chose_dx.model.t) %>%
  group_by(chose_dx, .prediction) %>%
  count()
p4 =
  preds_p_df %>%
    ggplot(aes(x = chose_dx, y = .prediction, fill = n / sum(preds_p_df$n))) +
      geom_tile() +
      geom_text(aes(label = sprintf("%1.2f", n / sum(preds_p_df$n))), size = 10, vjust = 1) +
      scale_fill_gradient(limits = c(0, 1))

# Plot predictions vs original
preds_p_df = chose_dx.model.v %>% predict() %>% as_tibble()
p5 =
  preds_p_df %>%
    add_column(chose_dx = chose_dx.model.v$data$chose_dx) %>%
    ggplot(aes(x = Estimate, y = chose_dx, color = chose_dx)) +
      geom_point() +
      labs(x = "Predicted values") +
      ylim(0, 1) +
      xlim(0, 1) +
      ggtitle("Values Model")

preds_p_df = plot_df %>%
  add_predicted_draws(chose_dx.model.v) %>%
  group_by(chose_dx, .prediction) %>%
  count()
p6 =
  preds_p_df %>%
    ggplot(aes(x = chose_dx, y = .prediction, fill = n / sum(preds_p_df$n))) +
      geom_tile() +
      geom_text(aes(label = sprintf("%1.2f", n / sum(preds_p_df$n))), size = 10, vjust = 1) +
      scale_fill_gradient(limits = c(0, 1))

grid.arrange(p1, p3, p5, p2, p4, p6, nrow = 2, ncol = 3)
```

Inferences from the fitting:

```{r, eval=report_bayesfactors}
# Compute the bayes factor for each parameter value. Note: we compare
# here against the ROPE, and not the degenerate null hypothesis of 0
# Bayes-Factor values would be more significant in the latter case
# According to the Raftery, 1995 rules
# bf = 1 - 3: Weak
# bf = 3 - 20: Positive
# bf = 20 - 150: Strong
# bf > 150: Very strong
print(bayesfactor_rope(chose_dx.model.null, null = c(-0.055, 0.055)), n = 30)
print(bayesfactor_rope(chose_dx.model.t, null = c(-0.055, 0.055)), n = 30)
print(bayesfactor_rope(chose_dx.model.v, null = c(-0.055, 0.055)), n = 30)
```
```{r}
options(digits = 3)
# Check the significance of each hypothesis
h_df = as_tibble(insight::get_parameters(chose_dx.model.t)) %>%
  transmute(
    Intercept = b_Intercept,
    has_ax_test = -2 * b_has_ax1,
    noise_levelL_test = b_noise_level.L,
    noise_levelQ_test = b_noise_level.Q
  )

hyp_results = test_hypotheses(h_df, rope_values = c(-0.055, 0.055))
print(hyp_results %>% select(Parameter, pd, HDI_low, HDI_high, ROPE_Percentage, ROPE_Equivalence) %>% as.matrix())

h_df = as_tibble(insight::get_parameters(chose_dx.model.v)) %>%
  transmute(
    Intercept = b_Intercept,
    has_ax_test = -2 * b_has_ax1,
    noise_level1_test = b_noise_level_f1,
    noise_level2_test = b_noise_level_f2,
    noise_level3_test = - b_noise_level_f1 - b_noise_level_f2
  )

hyp_results = test_hypotheses(h_df, rope_values = c(-0.055, 0.055))
print(hyp_results %>% select(Parameter, pd, HDI_low, HDI_high, ROPE_Percentage, ROPE_Equivalence) %>% as.matrix())
options(digits = 7)
```

**Results**:

1. The noise level has a linear relationship to the probability of following diagnosis suggestions with a negative effect size (negative slope) with probability of 0.99 (Median = -0.67, 89% CI [-1.13, -0.22]) and can be considered significant (0.0% in ROPE).
1. The effect of no noise in suggestions (DX, 100% accurate) has a probability 0.99 of being positive (Median = 0.50, 89% CI [0.16, 0.86]) and can be considered as significant (0.0% in ROPE).
1. The effect of 20% noise in suggestions (DX, 80% accurate) has a probability 0.98 of being negative (Median = -0.46, 89% CI [-0.82, -0.08]) and can be considered as significant (0.0% in ROPE).

(The ROPE is defined as [-0.055, 0.055]. It is the range is suggested in the literature for logistic models and it corresponds to a probability range of 0.11. Therefore, a probability change of less than 0.055 is considered no different from a probability change of 0)

So here are the following plots:

```{r eval=plot_posteriors}
# A visualization data frame
gg_df = plot_df %>%
    mutate(noise_level = fct_rev(noise_level)) %>%
    mutate(chose_dx = factor(chose_dx)) %>%
    mutate(user_id = factor(user_id)) %>%
    mutate(chose_dx = fct_recode(chose_dx, no_follow="0", follow = "1")) %>%
    mutate(.fill_column = paste(noise_level, !(study_condition == 'BASELINE'), sep = ":")) %>%
    count(user_id, study_condition, .fill_column, chose_dx, num_actions, has_ax, has_dx, noise_level,
          name = "num_follow_dx")

# Simulate predictions on new data
p_df =
  plot_df %>%
  data_grid(chose_dx,
            has_ax,
            noise_level_f,
            gender,
            age_group,
            robot_experience,
            num_optimal,
            state_idx_rescaled = seq_range(state_idx_rescaled, n = 4),
            .model = chose_dx.model.v)

fits_p_df = p_df %>% add_fitted_draws(chose_dx.model.v,
                                      re_formula = NA,
                                      n = 100,
                                      seed = default_seed)
gc()
pars_p_df = chose_dx.model.t %>%
  extract_draws(newdata = p_df %>% rename(noise_level = noise_level_f),
                re_formula = NA,
                nsamples = 100)
gc()

# We don't want predicted draws because that's a raw count of ones and zeros; not the
# probability distribution
# preds_p_df = p_df %>% add_predicted_draws(scenario_completed.model.v)

# Plot the posterior distributions
rope_value = gg_df %>%
  filter(chose_dx == 'follow', has_ax == F) %>%
  summarise(.value = median(num_follow_dx / num_actions)) %>%
  pull(.value)
gg_df %>%
  filter(chose_dx == 'follow') %>%
  ggplot(aes(x = has_ax, y = num_follow_dx / num_actions)) +
    geom_rect(ymin = rope_value-0.055, ymax = rope_value+0.055,
              xmin = -Inf, xmax = Inf,
              color = "transparent", fill = "white") +
    annotate("text", x = 0.5, y = rope_value, label = "ROPE", angle = 90) +
    stat_halfeye(aes(y = .value), data = fits_p_df, position = position_nudge(x = 0.1)) +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep = ":")), size = 2) +
    stat_summary(fun.y = median, geom = "point", shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(y = "follow_dx", color = "AX:DX:Noise")

rope_value = gg_df %>%
  filter(chose_dx == 'follow', noise_level == "0.0") %>%
  summarise(.value = median(num_follow_dx / num_actions)) %>%
  pull(.value)
gg_df %>%
  filter(chose_dx == 'follow') %>%
  mutate(noise_level = fct_rev(noise_level)) %>%
  ggplot(aes(x = noise_level, y = num_follow_dx / num_actions)) +
    geom_rect(ymin = rope_value-0.055, ymax = rope_value+0.055,
              xmin = -Inf, xmax = Inf,
              color = "transparent", fill = "white") +
    annotate("text", x = 0.5, y = rope_value, label = "ROPE", angle = 90) +
    stat_halfeye(aes(y = .value, x = noise_level_f), data = fits_p_df, position = position_nudge(x = 0.1)) +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep = ":")), size = 2) +
    stat_summary(fun.y = median, geom = "point", shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(y = "follow_dx", color = "AX:DX:Noise")

# Create a plot to show the trend
pars_p_df_X = pars_p_df$dpars$mu$fe$X[,c("Intercept", "noise_level.L")]
pars_p_df_b = pars_p_df$dpars$mu$fe$b[,c("b_Intercept", "b_noise_level.L")]
pars_p_df_y = pars_p_df_X %*% t(pars_p_df_b)
pars_p_df_y = inv_logit_scaled(pars_p_df_y)
p_df =
  p_df %>%
  bind_cols(as_tibble(pars_p_df_y)) %>%
  gather(key = ".sample", value = ".value", V1:V100) %>%
  rename(noise_level = noise_level_f)

rope_value = gg_df %>%
  filter(chose_dx == 'follow', noise_level == "0.0") %>%
  summarise(.value = median(num_follow_dx / num_actions)) %>%
  pull(.value)
gg_df %>%
  filter(chose_dx == 'follow') %>%
  mutate(noise_level = fct_rev(noise_level)) %>%
  ggplot(aes(x = noise_level, y = num_follow_dx / num_actions)) +
    geom_rect(ymin = rope_value-0.055, ymax = rope_value+0.055,
              xmin = -Inf, xmax = Inf,
              color = "transparent", fill = "white") +
    annotate("text", x = 0.5, y = rope_value, label = "ROPE", angle = 90) +
    stat_halfeye(aes(y = .value, x = noise_level_f), data = fits_p_df, position = position_nudge(x = 0.1)) +
    geom_line(aes(y = .value, group = .sample, alpha = 0.3), data = p_df) +
    geom_point(aes(color = paste(has_ax, has_dx, noise_level, sep = ":")), size = 2) +
    stat_summary(fun.y = median, geom = "point", shape = 23, size = 6) +
    scale_color_metro() +
    ylim(0, 1) +
    labs(y = "scenario_completed", color = "AX:DX:Noise") +
    guides(alpha = F)

# Remove the giant fits data frames
rm(fits_p_df, pars_p_df, pars_p_df_b, pars_p_df_X, pars_p_df_y, p_df, gg_df)
```

```{r, echo=F, results=F, message=F, warning=F}
rm(chose_dx.model.null, chose_dx.model.t, chose_dx.model.v)
gc()
```


## Time When Reliant on AX

**How long did people take to pick actions when they were reliant on suggestions?**

- Action picking time or complete decision time?
- Filter by the reliance metric or include that in the model?

```{r}
plot_df = actions %>%
  select(X1, id, user_id, study_condition, start_condition, num_optimal,
         state_idx, state_idx_rescaled,
         age_group, gender, robot_experience,
         noise_level, noise_level_f, has_noise, has_dx, has_ax,
         scenario_completed, num_actions, optimal_ax,
         decision_duration, ax_decision_duration) %>%
  mutate(optimal_ax = factor(optimal_ax))

text_short(report(plot_df))
```

```{r fig.height=7, fig.width=15}
# Plot by the study condition
p1 =
  plot_df %>%
    mutate(.fill_column = paste(noise_level, !(study_condition == 'BASELINE'), sep = ":")) %>%
    ggplot(aes(study_condition, ax_decision_duration, fill = .fill_column)) +
      geom_boxplot() +
      geom_count() +
      scale_y_log10() +
      labs(y = "Log(AX Decision Duration)", fill = "noise:has_suggestions") +
      scale_fill_economist()

p2 =
  plot_df %>%
    mutate(.fill_column = paste(noise_level, !(study_condition == 'BASELINE'), sep = ":")) %>%
    ggplot(aes(study_condition, decision_duration, fill = .fill_column)) +
      geom_boxplot() +
      geom_count() +
      scale_y_log10() +
      labs(y = "Log(Decision Duration)", fill = "noise:has_suggestions") +
      scale_fill_economist()

grid.arrange(p1, p2, ncol = 2)

# Visualize the data by the three variables that we care about
gg_df = plot_df %>%
  mutate(.fill_column = paste(noise_level, !(study_condition == 'BASELINE'), sep = ":")) %>%
  mutate(noise_level = fct_rev(noise_level))

p1 = gg_df %>%
  ggplot(aes(has_ax, ax_decision_duration, fill=optimal_ax)) +
    geom_boxplot() +
    # geom_count(aes(colour = optimal_ax), position = position_jitterdodge()) +
    scale_y_log10() +
    facet_grid(rows = vars(noise_level), cols = vars(has_dx), labeller = label_both) +
    labs(y = "Log(AX Decision Duration)") +
    scale_fill_economist()

p2 = gg_df %>%
  ggplot(aes(has_dx, ax_decision_duration, fill=optimal_ax)) +
    geom_boxplot() +
    # geom_count(aes(colour = optimal_ax), position = position_jitterdodge()) +
    scale_y_log10() +
    facet_grid(rows = vars(noise_level), cols = vars(has_ax), labeller = label_both) +
    labs(y = "Log(AX Decision Duration)") +
    scale_fill_economist()

p3 = gg_df %>%
  mutate(noise_level = fct_rev(noise_level)) %>%
  mutate(has_ax = factor(has_ax)) %>%
  mutate(has_ax = fct_rev(has_ax)) %>%
  ggplot(aes(noise_level, ax_decision_duration, fill=optimal_ax)) +
    geom_boxplot() +
    # geom_count(aes(colour = optimal_ax), position = position_jitterdodge()) +
    scale_y_log10() +
    facet_grid(rows = vars(has_ax), cols = vars(has_dx), labeller = label_both) +
    labs(y = "Log(AX Decision Duration)") +
    scale_fill_economist() +
    legend_none()

grid.arrange(p1, p2, p3, ncol = 3)

# Visualize also the variation in each of the three levels that we care about
p1 = gg_df %>%
  ggplot(aes(ax_decision_duration, group = optimal_ax, fill = optimal_ax)) +
    geom_density(position = "identity", alpha = 0.5) +
    facet_grid(rows = vars(has_ax)) +
    scale_fill_economist() +
    scale_x_continuous(trans = "log") +
    legend_none()

p2 = gg_df %>%
  ggplot(aes(ax_decision_duration, group = optimal_ax, fill = optimal_ax)) +
    geom_density(position = "identity", alpha = 0.5) +
    facet_grid(rows = vars(has_dx)) +
    scale_fill_economist() +
    scale_x_continuous(trans = "log") +
    theme(legend.position = "bottom")

p3 = gg_df %>%
  ggplot(aes(ax_decision_duration, group = optimal_ax, fill = optimal_ax)) +
    geom_density(position = "identity", alpha = 0.5) +
    facet_grid(rows = vars(noise_level)) +
    scale_fill_economist() +
    scale_x_continuous(trans = "log") +
    legend_none()

grid.arrange(p1, p2, p3, ncol = 3)
```

Checking some probability distributions of the durations (given the above plots):

```{r}
# Run a few simple distribution fitting tests using the action
x = fit_and_print_dist(plot_df %>%
                         filter(has_ax == T, optimal_ax == 1) %>%
                         pull(ax_decision_duration),
                       "lognormal", "AX = T, OPT = T; lognormal:", use_mass = T)
x = fit_and_print_dist(plot_df %>%
                         filter(has_ax == T, optimal_ax == 1) %>%
                         pull(ax_decision_duration),
                       "exponential", "AX = T, OPT = T; exponential:", use_mass = T)
x = fit_and_print_dist(plot_df %>%
                         filter(has_ax == T, optimal_ax == 1) %>%
                         pull(ax_decision_duration),
                       "gamma", "AX = T, OPT = T; gamma:", use_mass = T)
x = fit_and_print_dist(plot_df %>%
                         filter(has_ax == T) %>%
                         pull(ax_decision_duration),
                       "lognormal", "AX = T; lognormal:", use_mass = T)
x = fit_and_print_dist(plot_df %>%
                         filter(has_ax == T) %>%
                         pull(ax_decision_duration),
                       "exponential", "AX = T; exponential:", use_mass = T)
x = fit_and_print_dist(plot_df %>%
                         filter(has_ax == T) %>%
                         pull(ax_decision_duration),
                       "gamma", "AX = T; gamma:", use_mass = T)

# Try a couple of other predictor variables
x = fit_and_print_dist(plot_df %>%
                         filter(has_dx == T, optimal_ax == 1) %>%
                         pull(ax_decision_duration),
                       "lognormal", "DX = T, OPT = T; lognormal:", use_mass = T)
x = fit_and_print_dist(plot_df %>%
                         filter(has_dx == T, optimal_ax == 1) %>%
                         pull(ax_decision_duration),
                       "exponential", "DX = T, OPT = T; exponential:", use_mass = T)
x = fit_and_print_dist(plot_df %>%
                         filter(has_dx == T, optimal_ax == 1) %>%
                         pull(ax_decision_duration),
                       "gamma", "DX = T, OPT = T; gamma:", use_mass = T)
x = fit_and_print_dist(plot_df %>%
                         filter(has_dx == T) %>%
                         pull(ax_decision_duration),
                       "lognormal", "DX = T; lognormal:", use_mass = T)
x = fit_and_print_dist(plot_df %>%
                         filter(has_dx == T) %>%
                         pull(ax_decision_duration),
                       "exponential", "DX = T; exponential:", use_mass = T)
x = fit_and_print_dist(plot_df %>%
                         filter(has_dx == T) %>%
                         pull(ax_decision_duration),
                       "gamma", "DX = T; gamma:", use_mass = T)
rm(x)

# If we are training models, then try to use brms to also test this
if (train_models) {
  lognormal.model = brm(
    ax_decision_duration ~ 0 + Intercept + (1 | user_id),
    family = lognormal,
    data = plot_df,
    seed = default_seed,
  )
  lognormal.model = add_criterion(lognormal.model, "loo")

  exgaussian.model = brm(
    ax_decision_duration ~ 0 + Intercept + (1 | user_id),
    family = exgaussian,
    data = plot_df,
    seed = default_seed,
  )
  exgaussian.model = add_criterion(exgaussian.model, "loo")

  shifted_lognormal.model = brm(
    ax_decision_duration ~ 0 + Intercept + (1 | user_id),
    family = shifted_lognormal,
    data = plot_df,
    seed = default_seed,
  )
  shifted_lognormal.model = add_criterion(shifted_lognormal.model, "loo")

  print(loo_compare(lognormal.model,
                    exgaussian.model,
                    shifted_lognormal.model))

  rm(lognormal.model, exgaussian.model, shifted_lognormal.model)
  gc()
}
```

From this, we can take away that:

1. The distribution of counts is most likely a `shifted_lognormal`. The Exponential Gaussian null model did not converge in the number of iterations that we have assigned.
1. Not sure what we're checking here - the difference between when things are optimal vs. not?

Therefore, we assume the following structural model (TODO):

$$ax\_time_i = TBD\_Distr(\mu_i, \phi)$$
$$\begin{aligned}
log(\mu_i) &= \beta_0 + \beta_{ax}\text{ax}_i + \beta_{dx}\text{dx}_i + \beta_{noise}\text{noise}_i +\\ &\beta_{ax:noise}\text{ax}_i\text{noise}_i + \beta_{dx:noise}\text{dx}_i\text{noise}_i + \\
&\beta_{no}\text{no}_i + \mathbf{\beta_{demo}X_{demo,i}}
\end{aligned}$$
$$\begin{aligned}
\beta_{.} &\sim Normal(0, 10)\\
\phi &\sim Gamma(0.01, 0.01)
\end{aligned}$$

The prior for the $\phi$ parameter is the default used in `brms`; I see no need to change it.

Model fitting:

```{r, eval=train_models}
# A null model to compare against
ax_reliance_time.model.null = brm(
  ax_decision_duration ~ 0 + Intercept + (1 | user_id),
  family = shifted_lognormal,
  prior = c(set_prior("normal(0, 10)", class = "b"),
            set_prior("student_t(3, 0, 10)", class = "sd")),  # ndt = uniform(0, min(Y))
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
ax_reliance_time.model.null = add_criterion(ax_reliance_time.model.null, "waic")
ax_reliance_time.model.null = add_criterion(ax_reliance_time.model.null, "loo", reloo = T)
saveModel(ax_reliance_time.model.null)

# The trend model to see if there is a trend in the noise level variable
ax_reliance_time.model.t = brm(
  ax_decision_duration ~ 0 + Intercept + ((has_dx + has_ax) * noise_level) + (gender + age_group + robot_experience) + num_optimal + state_idx_rescaled + optimal_ax + (1 | user_id),
  family = shifted_lognormal,
  prior = c(set_prior("normal(0, 10)", class = "b"),
            set_prior("student_t(3, 0, 10)", class = "sd")),  # ndt = uniform(0, min(Y))
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
ax_reliance_time.model.t = add_criterion(ax_reliance_time.model.t, "waic")
ax_reliance_time.model.t = add_criterion(ax_reliance_time.model.t, "loo", reloo = T)
saveModel(ax_reliance_time.model.t)

# The values model, to see if specific values of the noise level variable are significant
ax_reliance_time.model.v = brm(
  ax_decision_duration ~ 0 + Intercept + ((has_dx + has_ax) * noise_level_f) + (gender + age_group + robot_experience) + num_optimal + state_idx_rescaled + optimal_ax + (1 | user_id),
  family = shifted_lognormal,
  prior = c(set_prior("normal(0, 10)", class = "b"),
            set_prior("student_t(3, 0, 10)", class = "sd")),  # ndt = uniform(0, min(Y))
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
ax_reliance_time.model.v = add_criterion(ax_reliance_time.model.v, "waic")
ax_reliance_time.model.v = add_criterion(ax_reliance_time.model.v, "loo", reloo = T)
saveModel(ax_reliance_time.model.v)

# A possible alternative to these models is one that does not include the optimal_ax
# as a factor. Weirdly, these models show an even lower effect of the study conditions
# on the output. Does this mean the effect is not mediated by the probability of reliance?
# ax_reliance_time.model.t = brm(
#   ax_decision_duration ~ 0 + Intercept + ((has_dx + has_ax) * noise_level) + (gender + age_group + robot_experience) + num_optimal + state_idx_rescaled + (1 | user_id),
#   family = shifted_lognormal,
#   prior = c(set_prior("normal(0, 10)", class = "b"),
#             set_prior("student_t(3, 0, 10)", class = "sd")),  # ndt = uniform(0, min(Y))
#   data = plot_df,
#   seed = default_seed,
#   save_all_pars = T,
#   sample_prior = T
# )
# ax_reliance_time.model.t = add_criterion(ax_reliance_time.model.t, "waic")
# ax_reliance_time.model.t = add_criterion(ax_reliance_time.model.t, "loo", reloo = T)
# 
# ax_reliance_time.model.v = brm(
#   ax_decision_duration ~ 0 + Intercept + ((has_dx + has_ax) * noise_level_f) + (gender + age_group + robot_experience) + num_optimal + state_idx_rescaled + (1 | user_id),
#   family = shifted_lognormal,
#   prior = c(set_prior("normal(0, 10)", class = "b"),
#             set_prior("student_t(3, 0, 10)", class = "sd")),  # ndt = uniform(0, min(Y))
#   data = plot_df,
#   seed = default_seed,
#   save_all_pars = T,
#   sample_prior = T
# )
# ax_reliance_time.model.v = add_criterion(ax_reliance_time.model.v, "waic")
# ax_reliance_time.model.v = add_criterion(ax_reliance_time.model.v, "loo", reloo = T)
```
```{r, eval=!train_models}
# Load the models
ax_reliance_time.model.null = loadModel("ax_reliance_time.model.null")
ax_reliance_time.model.t = loadModel("ax_reliance_time.model.t")
ax_reliance_time.model.v = loadModel("ax_reliance_time.model.v")
```

Model fitting results:

```{r}
# print(tidy_stan(ax_reliance_time.model.null))
print(performance::r2(ax_reliance_time.model.null))

# Print the parameters, and some initial diagnostics
print(tidy_stan(ax_reliance_time.model.t, effects = "fixed"))
print(performance::r2(ax_reliance_time.model.t))

print(tidy_stan(ax_reliance_time.model.t, effects = "fixed"))
print(performance::r2(ax_reliance_time.model.t))
```

Diagnostics of the fit:

```{r fig.height=7, fig.width=15, eval=plot_diagnostics}
# Alternative diagnostics: diagnostic_posterior(scenario_completed.model.v)

# LOO cross-validation results
print(ax_reliance_time.model.null$criteria$loo$estimates)
print(ax_reliance_time.model.t$criteria$loo$estimates)
print(ax_reliance_time.model.v$criteria$loo$estimates)
print(loo_compare(ax_reliance_time.model.null,
                  ax_reliance_time.model.t,
                  ax_reliance_time.model.v))

# Diagnostics of the sampling process
par(mfrow=c(1,2))
plot(ax_reliance_time.model.t$criteria$loo, main = "Trend Model")
plot(ax_reliance_time.model.v$criteria$loo, main = "Values Model")

# Include posterior predictive checks of the sampling (are sampling according
# to the data that we actually have?)
# mcmc_intervals(as.matrix(ax_reliance_time.model.null), regex_pars = "b_", prob = 0.90, prob_outer = 0.95) +
  ggtitle("Null Model")

p1 = pp_check(ax_reliance_time.model.t, group = "has_ax", stat = "median") + ggtitle("t:has_ax")
p2 = pp_check(ax_reliance_time.model.t, group = "has_dx") + ggtitle("t:has_dx")
p3 = pp_check(ax_reliance_time.model.t, group = "noise_level") + ggtitle("t:noise_level")
grid.arrange(p1, p2, p3, nrow = 3)
mcmc_intervals(as.matrix(ax_reliance_time.model.t), regex_pars = "b_|^ndt", prob = 0.90, prob_outer = 0.95) +
  ggtitle("Trends Model")

p1 = pp_check(ax_reliance_time.model.v, group = "has_ax", stat = "median") + ggtitle("v:has_ax")
p2 = pp_check(ax_reliance_time.model.v, group = "has_dx") + ggtitle("v:has_dx")
p3 = pp_check(ax_reliance_time.model.v, group = "noise_level") + ggtitle("v:noise_level")
grid.arrange(p1, p2, p3, nrow = 3)
mcmc_intervals(as.matrix(ax_reliance_time.model.v), regex_pars = "b_|^ndt", prob = 0.90, prob_outer = 0.95) +
  ggtitle("Values Model")

# Plot predictions vs original
preds_p_df = ax_reliance_time.model.null %>% predict() %>% as_tibble()
p1 =
  preds_p_df %>%
    add_column(ax_response_time = ax_reliance_time.model.null$data$ax_decision_duration) %>%
    ggplot(aes(x = ax_response_time, y = Estimate)) +
      geom_point() +
      scale_x_log10() + scale_y_log10() +
      labs(y = "Predicted values") +
      ggtitle("Null Model")

preds_p_df = ax_reliance_time.model.t %>% predict() %>% as_tibble()
p3 =
  preds_p_df %>%
    add_column(ax_response_time = ax_reliance_time.model.t$data$ax_decision_duration) %>%
    ggplot(aes(x = ax_response_time, y = Estimate)) +
      geom_point() +
      scale_x_log10() + scale_y_log10() +
      labs(y = "Predicted values") +
      ggtitle("Trends Model")

preds_p_df = ax_reliance_time.model.v %>% predict() %>% as_tibble()
p5 =
  preds_p_df %>%
    add_column(ax_response_time = ax_reliance_time.model.v$data$ax_decision_duration) %>%
    ggplot(aes(x = ax_response_time, y = Estimate)) +
      geom_point() +
      scale_x_log10() + scale_y_log10() +
      labs(y = "Predicted values") +
      ggtitle("Values Model")

grid.arrange(p1, p3, p5, nrow = 1, ncol = 3)
```

Inferences from the fitting:

```{r, eval=report_bayesfactors}
# Compute the bayes factor for each parameter value. Note: we compare
# here against the ROPE, and not the degenerate null hypothesis of 0
# Bayes-Factor values would be more significant in the latter case
# According to the Raftery, 1995 rules
# bf = 1 - 3: Weak
# bf = 3 - 20: Positive
# bf = 20 - 150: Strong
# bf > 150: Very strong
# The MAD is 4.89. This is more reasonable than the std. dev of 1.12
print(bayesfactor_rope(ax_reliance_time.model.null, null = c(-0.49, 0.49)), n = 30)
print(bayesfactor_rope(ax_reliance_time.model.t, null = c(-0.49, 0.49)), n = 30)
print(bayesfactor_rope(ax_reliance_time.model.v, null = c(-0.49, 0.49)), n = 30)
```
```{r}
options(digits = 3)
# Check the significance of each hypothesis
h_df = as_tibble(insight::get_parameters(ax_reliance_time.model.t)) %>%
  transmute(
    Intercept = b_Intercept,
    has_ax_test = -2 * b_has_ax1,
    has_dx_test = -2 * b_has_dx1,
    noise_levelL_test = b_noise_level.L,
    noise_levelQ_test = b_noise_level.Q
  )

hyp_results = test_hypotheses(h_df, rope_values = c(-0.49, 0.49))
print(hyp_results %>% select(Parameter, pd, HDI_low, HDI_high, ROPE_Percentage, ROPE_Equivalence) %>% as.matrix())

h_df = as_tibble(insight::get_parameters(ax_reliance_time.model.v)) %>%
  transmute(
    Intercept = b_Intercept,
    has_ax_test = -2 * b_has_ax1,
    has_dx_test = -2 * b_has_dx1,
    noise_level1_test = b_noise_level_f1,
    noise_level2_test = b_noise_level_f2,
    noise_level3_test = - b_noise_level_f1 - b_noise_level_f2
  )

hyp_results = test_hypotheses(h_df, rope_values = c(-0.49, 0.49))
print(hyp_results %>% select(Parameter, pd, HDI_low, HDI_high, ROPE_Percentage, ROPE_Equivalence) %>% as.matrix())
options(digits = 7)
```

**Results**:

There is no significant effect of any of the suggestions parameters on the time taken to pick an action. There is a certain effect of being reliant on the suggestions vs. not, but the effect is not significant.

(The ROPE is defined as [-0.49, 0.49]. It corresponds to a reaction time of slightly less than a second and is obtained from `0.1 \times MAD(ax\_response\_time)`. Therefore, a change of less than 0.55 actions in either direction is considered no different from a change of 0 in the number of actions)

There are no plots to show (all effects are supposedly non-existent).

```{r, echo=F, results=F, message=F, warning=F}
rm(ax_reliance_time.model.null, ax_reliance_time.model.t, ax_reliance_time.model.v)
gc()
```


# Likert Scales {.tabset}

The individual unit of analysis is again the user.

## SUS

```{r}
plot_df = users %>%
  select(X1, id, study_condition, start_condition, num_optimal,
         age_group, gender, robot_experience,
         noise_level, noise_level_f, has_noise, has_dx, has_ax,
         sus, scenario_completed) %>%
  mutate(scenario_completed = factor(scenario_completed))

text_short(report(plot_df))
```

```{r fig.height=7, fig.width=15}
# Plot by the study condition
plot_df %>%
  mutate(.fill_column = paste(noise_level, !(study_condition == 'BASELINE'), sep = ":")) %>%
  ggplot(aes(study_condition, sus, fill = .fill_column)) +
    geom_boxplot() +
    geom_count() +
    labs(y = "Number unnecessary actions", fill = "noise:has_suggestions") +
    scale_fill_economist()

# Visualize the data by the three variables that we care about
gg_df = plot_df %>%
  mutate(.fill_column = paste(noise_level, !(study_condition == 'BASELINE'), sep = ":")) %>%
  mutate(noise_level = fct_rev(noise_level))

p1 = gg_df %>%
  ggplot(aes(has_ax, sus, fill=.fill_column)) +
    geom_boxplot() +
    geom_count() +
    facet_grid(rows = vars(noise_level), cols = vars(has_dx), labeller = label_both) +
    labs(y = "Number unnecessary actions") +
    scale_fill_economist()

p2 = gg_df %>%
  ggplot(aes(has_dx, sus, fill=.fill_column)) +
    geom_boxplot() +
    geom_count() +
    facet_grid(rows = vars(noise_level), cols = vars(has_ax), labeller = label_both) +
    labs(y = "Number unnecessary actions") +
    scale_fill_economist()

p3 = gg_df %>%
  mutate(noise_level = fct_rev(noise_level)) %>%
  mutate(has_ax = factor(has_ax)) %>%
  mutate(has_ax = fct_rev(has_ax)) %>%
  ggplot(aes(noise_level, sus, fill=.fill_column)) +
    geom_boxplot() +
    geom_count() +
    facet_grid(rows = vars(has_ax), cols = vars(has_dx), labeller = label_both) +
    labs(y = "Number unnecessary actions") +
    scale_fill_economist()

grid.arrange(p1, p2, p3, ncol = 3)

# Visualize also the variation in each of the three levels that we care about
p1 = gg_df %>%
  ggplot(aes(sus)) +
    geom_histogram(binwidth = 10) +
    facet_grid(rows = vars(has_ax)) +
    ylim(0, 30) +
    scale_fill_economist() +
    legend_none()

p2 = gg_df %>%
  ggplot(aes(sus)) +
    geom_histogram(binwidth = 10) +
    facet_grid(rows = vars(has_dx)) +
    ylim(0, 30) +
    scale_fill_economist() +
    theme(legend.position = "bottom")

p3 = gg_df %>%
  ggplot(aes(sus)) +
    geom_histogram(binwidth = 10) +
    facet_grid(rows = vars(noise_level)) +
    ylim(0, 30) +
    scale_fill_economist() +
    legend_none()

grid.arrange(p1, p2, p3, ncol = 3)
```

Based on the data, we assume a skew-normal linear model:

$$sus_i = SkewNormal(\mu_i, \sigma, \alpha)$$
$$\begin{aligned}
\mu_i &= \beta_0 + \beta_{ax}\text{ax}_i + \beta_{dx}\text{dx}_i + \beta_{noise}\text{noise}_i +\\ &\beta_{ax:noise}\text{ax}_i\text{noise}_i + \beta_{dx:noise}\text{dx}_i\text{noise}_i + \\
&\beta_{no}\text{no}_i + \mathbf{\beta_{demo}X_{demo,i}}
\end{aligned}$$
$$\begin{aligned}
\beta_{.} &\sim Normal(0, 10) \\
\sigma &\sim HalfStudent(3, 0, 22) \\
\alpha &\sim Normal(0, 4)
\end{aligned}$$

The prior for the $\sigma, \alpha$ parameters are the default used in `brms`; I see no need to change it.

Model fitting:

```{r, eval=train_models}
# A null model to compare against
sus.model.null = brm(
  sus ~ 0 + Intercept,
  family = "skew_normal",
  prior = c(set_prior("normal(0, 10)", class = "b"),
            set_prior("student_t(3, 0, 22)", class = "sigma"),
            set_prior("normal(0, 4)", class = "alpha")),
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
sus.model.null = add_criterion(sus.model.null, "waic")
sus.model.null = add_criterion(sus.model.null, "loo", reloo = T)
saveModel(sus.model.null)

# The trend model to see if there is a trend in the noise level variable
sus.model.t = brm(
  sus ~ 0 + Intercept + ((has_dx + has_ax) * noise_level) + (gender + age_group + robot_experience) + num_optimal,
  family = "skew_normal",
  prior = c(set_prior("normal(0, 10)", class = "b"),
            set_prior("student_t(3, 0, 22)", class = "sigma"),
            set_prior("normal(0, 4)", class = "alpha")),
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
sus.model.t = add_criterion(sus.model.t, "waic")
sus.model.t = add_criterion(sus.model.t, "loo", reloo = T)
saveModel(sus.model.t)

# The values model, to see if specific values of the noise level variable are significant
sus.model.v = brm(
  sus ~ 0 + Intercept + ((has_dx + has_ax) * noise_level_f) + (gender + age_group + robot_experience) + num_optimal,
  family = "skew_normal",
  prior = c(set_prior("normal(0, 10)", class = "b"),
            set_prior("student_t(3, 0, 22)", class = "sigma"),
            set_prior("normal(0, 4)", class = "alpha")),
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
sus.model.v = add_criterion(sus.model.v, "waic")
sus.model.v = add_criterion(sus.model.v, "loo", reloo = T)
saveModel(sus.model.v)
# More model plots are available at:
# https://bookdown.org/ajkurz/Statistical_Rethinking_recoded/counting-and-classification.html

# Attempted RStanarm models: The estimate contrasts doesn't seem to be as helpful?
# scenario_completed.model.v = stan_glm(
#   scenario_completed ~ ((has_dx + has_ax) * noise_level) + (gender + age_group + robot_experience) + num_optimal,
#   family = binomial(link = "logit"),
#   prior = normal(0, 10),
#   prior_intercept = normal(0, 10),
#   data = plot_df,
#   QR = T,
#   seed = default_seed
# )
```
```{r, eval=!train_models}
# Load the models
sus.model.null = loadModel("sus.model.null")
sus.model.t = loadModel("sus.model.t")
sus.model.v = loadModel("sus.model.v")
```

Model fitting results:

```{r}
# print(summary(sus.model.null))
print(performance::r2(sus.model.null))

# Print the parameters, and some initial diagnostics
print(tidy_stan(sus.model.t))
print(performance::r2(sus.model.t))

print(tidy_stan(sus.model.v))
print(performance::r2(sus.model.v))
```

Diagnostics of the fit:

```{r fig.height=7, fig.width=15, eval=plot_diagnostics}
# Alternative diagnostics: diagnostic_posterior(scenario_completed.model.v)

# LOO cross-validation results
print(sus.model.t$criteria$loo$estimates)
print(sus.model.v$criteria$loo$estimates)
print(loo_compare(sus.model.null,
                  sus.model.t,
                  sus.model.v))

# Diagnostics of the sampling process
par(mfrow=c(1,2))
plot(sus.model.t$criteria$loo, main = "Trend Model")
plot(sus.model.v$criteria$loo, main = "Values Model")

# Include posterior predictive checks of the sampling (are sampling according
# to the data that we actually have?)
mcmc_intervals(as.matrix(sus.model.null), regex_pars = "b_", prob = 0.90, prob_outer = 0.95) +
  ggtitle("Null Model")

p1 = pp_check(sus.model.t, group = "has_ax", stat = "median") + ggtitle("t:has_ax")
p2 = pp_check(sus.model.t, group = "has_dx") + ggtitle("t:has_dx")
p3 = pp_check(sus.model.t, group = "noise_level") + ggtitle("t:noise_level")
grid.arrange(p1, p2, p3, nrow = 3)
mcmc_intervals(as.matrix(sus.model.t), regex_pars = "b_", prob = 0.90, prob_outer = 0.95) +
  ggtitle("Trends Model")

p1 = pp_check(sus.model.v, group = "has_ax") + ggtitle("v:has_ax")
p2 = pp_check(sus.model.v, group = "has_dx") + ggtitle("v:has_dx")
p3 = pp_check(sus.model.v, group = "noise_level_f") + ggtitle("v:noise_level")
grid.arrange(p1, p2, p3, nrow = 3)
mcmc_intervals(as.matrix(sus.model.v), regex_pars = "b_", prob = 0.90, prob_outer = 0.95) +
  ggtitle("Values Model")

# Plot predictions vs original
preds_p_df = sus.model.null %>% predict() %>% as_tibble()
p1 =
  preds_p_df %>%
    add_column(sus = sus.model.null$data$sus) %>%
    ggplot(aes(x = sus, y = Estimate)) +
      geom_point() +
      labs(y = "Predicted values") +
      ggtitle("Null Model")

preds_p_df = sus.model.t %>% predict() %>% as_tibble()
p3 =
  preds_p_df %>%
    add_column(sus = sus.model.t$data$sus) %>%
    ggplot(aes(x = sus, y = Estimate)) +
      geom_point() +
      labs(y = "Predicted values") +
      ggtitle("Trends Model")

preds_p_df = sus.model.v %>% predict() %>% as_tibble()
p5 =
  preds_p_df %>%
    add_column(sus = sus.model.v$data$sus) %>%
    ggplot(aes(x = sus, y = Estimate)) +
      geom_point() +
      labs(y = "Predicted values") +
      ggtitle("Values Model")

grid.arrange(p1, p3, p5, nrow = 1, ncol = 3)
```

Inferences from the fitting:

```{r, eval=report_bayesfactors}
# Compute the bayes factor for each parameter value. Note: we compare
# here against the ROPE, and not the degenerate null hypothesis of 0
# Bayes-Factor values would be more significant in the latter case
# According to the Raftery, 1995 rules
# bf = 1 - 3: Weak
# bf = 3 - 20: Positive
# bf = 20 - 150: Strong
# bf > 150: Very strong
print(bayesfactor_rope(sus.model.null), n = 30)
print(bayesfactor_rope(sus.model.t), n = 30)
print(bayesfactor_rope(sus.model.v), n = 30)
```
```{r}
# Compare the models. Note: we cannot do this because we don't have enough samples.
# The default is 4000; apparently these functions are only meaningful with 40000
# comparison = bayesfactor_models(scenario_completed.model.t, scenario_completed.model.v,
#                                 denominator = scenario_completed.model.null)
# print(comparison)
# print(bayesfactor_inclusion(comparison))

# # If we're using the brms functions, then use the following (make sure to update!)
# common_hyp_to_test = c("-2 * has_ax1 = 0", "-2 * has_dx1 = 0")
# noise_levels_hyp_to_test = c(
#   "Intercept-noise_level_f1 = 0", "Intercept-noise_level_f2 = 0", "Intercept-noise_level_f1-noise_level_f2 = 0",
#   "Intercept-has_ax1:noise_level_f1 = 0", "Intercept-has_dx1:noise_level_f1 = 0",
#   "Intercept-has_ax1:noise_level_f2 = 0", "Intercept-has_dx1:noise_level_f2 = 0",
#   "Intercept-has_ax1:noise_level_f1-has_ax1:noise_level_f2 = 0",
#   "Intercept-has_dx1:noise_level_f1-has_dx1:noise_level_f2 = 0"
# )
# hyp_results = test_hypotheses(hypotheses_list = c(common_hyp_to_test, noise_levels_hyp_to_test), model = scenario_completed.model.v)
# print(hyp_results$hypothesis)
# plot(hyp_results, ask=F)

options(digits = 3)
# Check the significance of each hypothesis
h_df = as_tibble(insight::get_parameters(sus.model.t)) %>%
  transmute(
    Intercept = b_Intercept,
    has_ax_test = -2 * b_has_ax1,
    has_dx_test = -2 * b_has_dx1,
    noise_levelL_test = b_noise_level.L,
    noise_levelQ_test = b_noise_level.Q
  )

hyp_results = test_hypotheses(h_df, rope_values = c(-2.3, 2.3))
print(hyp_results %>% select(Parameter, pd, HDI_low, HDI_high, ROPE_Percentage, ROPE_Equivalence) %>% as.matrix())

h_df = as_tibble(insight::get_parameters(sus.model.v)) %>%
  transmute(
    Intercept = b_Intercept,
    has_ax_test = -2 * b_has_ax1,
    has_dx_test = -2 * b_has_dx1,
    noise_level1_test = b_noise_level_f1,
    noise_level2_test = b_noise_level_f2,
    noise_level3_test = - b_noise_level_f1 - b_noise_level_f2
  )

hyp_results = test_hypotheses(h_df, rope_values = c(-2.3, 2.3))
print(hyp_results %>% select(Parameter, pd, HDI_low, HDI_high, ROPE_Percentage, ROPE_Equivalence) %>% as.matrix())
options(digits = 7)
```

**Results**:

There is no significant effect of any of the suggestions parameters on the SUS.

(The ROPE is defined as [-2.3, 2.3]. It corresponds to `0.1 * SD` of the output)

There are no plots to show (all effects are supposedly non-existent).

```{r, echo=F, results=F, message=F, warning=F}
rm(sus.model.null, sus.model.t, sus.model.v)
gc()
```


## Ease of Diagnosis

TODO

## Ease of Actions

TODO


# Remaining Metrics

1. Time taken when compliant / inverse-reliant (for diagnoses and for actions)
1. Diagnosis Certainty
