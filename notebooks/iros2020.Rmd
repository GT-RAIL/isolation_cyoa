---
title: "RO-MAN 2020"
output:
  html_document:
    # theme: readable
    code_folding: hide
    df_print: tibble
    toc: true
    toc_depth: 2
    number_sections: true
---

For each of our dependent variables, there are 2 models:

1. With the noise variable as an ordered factor, so that we can make inferences on the trends in the variable
1. With the noise variable as an unordered factor, so that we can make inferences on the values of the variable

We use Deviation / Sum coding in the coding of non-ordered factors so that we can test the influence of individual factor levels over and above the grand mean.

In all our models:

- $\beta_0$ is assumed to be the intercept. In a mixed effects model, this might be indexed by $i$, the user.
- $\mathbf{X_{demo,i}}$ is a vector of demographic information
- $\text{ax}_i$ denotes if sample $i$ received AX suggestions or not
- $\text{dx}_i$ denotes if sample $i$ received DX suggestions or not
- $\text{noise}_i$ denotes the level of noise in the suggestions that sample $i$ received. 0 if none was present.
- $\text{no}_i$ denotes the number of optimal actions for the scenario presented to user $i$. This is a proxy for a factor encoding of the start scenario

We test the following hypotheses (the explanations are a statement of the null hypotheses):

- $(\beta_0 - \beta_{ax_0}) - (\beta_0 + \beta_{ax_0}) = -2\beta_{ax_0} = 0 \Rightarrow \beta_{ax_0} = 0$: The main difference in effects from having action suggestions vs. not is not negligible (ceterus paribus)
- $(\beta_0 - \beta_{dx_0}) - (\beta_0 + \beta_{dx_0}) = -2\beta_{dx_0} = 0 \Rightarrow \beta_{dx_0} = 0$: The main difference in effects from having diagnosis suggestions vs. not is not neglible (ceterus paribus)
- $(\beta_0 - \beta_{noise_0}) = 0$: The main effect of no noise in suggestions is not different from averaging the effects of noise.
- $(\beta_0 - \beta_{noise_1}) = 0$: The main effect of level 1 noise in suggestions is not different from averaging the effects of noise.
- $(\beta_0 - \beta_{noise_0} - \beta_{noise_1}) = 0$: The main effect of level 2 noise in suggestions is not different from averaging the effect of noise.
- $\beta_{noise_L} = 0; \beta_{noise_Q} = 0$: Noise does not have a linear / quadratic effect on the outcome.

We also test the interaction terms for significance (relative to the grand mean).


```{r, message=F, warning=F}
library(car)
library(stats)
library(grid)
library(gridExtra)
# library(dplyr)
# library(forcats)
# library(tibble)
# library(tidyr)
# library(readr)
# library(ggplot2)
# library(stringr)
library(tidyverse)
library(ggsignif)
library(ggthemes)
library(jtools)
library(broom)
library(modelr)
library(loo)
# library(rstanarm)
library(brms)
library(bayesplot)
library(tidybayes)
library(bayestestR)
library(sjstats)
library(see)

# Setup for multiprocessing
options(mc.cores = 4)
# library(future)
# plan(multiprocess)

# Make the default coding of contrasts sum-coding; just in case
options(contrasts = c("contr.sum", "contr.poly"))
```

Global options:

```{r}
# Script execution globals
train_models = F
default_seed = 0x1331
data_folder = "~/Documents/GT/Research/Data/arbitration/2019-12-09/results"
```

Helper functions:

```{r, message=FALSE, warning=FALSE}
# Create a simple coding contrasts matrix
contr.deviation = function(nlevels) {
  x1 = contr.treatment(nlevels)
  x2 = matrix(rep(1/nlevels, (nlevels-1) * nlevels), ncol = (nlevels-1))
  return(x1-x2)
}

# NOTE: For now we're ignoring this advice
# # Use a simple coding contrasts matrix to add our own contrasts columns
# # Explained in: http://talklab.psy.gla.ac.uk/tvw/catpred/
# create_simple_contrasts = function(d, column_name) {
#  factor_column = d %>% pull(column_name)
#  contrast_matrix = contr.deviation(length(levels(factor_column)))
# }

# Save a model
saveModel = function(model, folder = data_folder) {
  saveRDS(model, file = file.path(folder, paste(substitute(model), ".rds", sep = '')))
}

# Load a model
loadModel = function(model_name, folder = data_folder) {
  return(readRDS(file.path(folder, paste(model_name, '.rds', sep = ''))))
}
```

Data loading:

```{r}
# Load the CSV files. If the age_group fill model is run again and we get a different output,
# then remember to update the value here
loadCSV = function(filename, age_group_fill, contrast_func) {
  dat = read_csv(
    file.path(data_folder, filename),
    col_types = cols(
      study_condition = col_factor(),
      noise_level = col_factor(ordered = T),
      gender = col_factor(levels = c("F", "M", "U")),
      age_group = col_factor(levels = seq(from = 0, to = 8)),
      robot_experience = col_factor(levels = seq(from = 0, to = 4))
    )
  )

  # Relabel the factors
  dat = dat %>%
    mutate(study_condition = fct_recode(study_condition,
                                        BASELINE="1",
                                        DX_100="2", AX_100="3", DXAX_100="4",
                                        DX_90="5", AX_90="6", DXAX_90="7",
                                        DX_80="8", AX_80="9", DXAX_80="10")) %>%
    mutate(study_condition = fct_relevel(study_condition, c("DX_100", "AX_100", "DXAX_100",
                                                            "DX_90", "AX_90", "DXAX_90",
                                                            "DX_80", "AX_80", "DXAX_80")))

  # Relevel the non-binary gender
  dat$gender[dat$gender == 'U'] = 'M'

  # Change binary responses to integers
  dat$scenario_completed = as.integer(dat$scenario_completed)
  
  # Relevel age_group
  dat[dat$age_group == 0,]$age_group = age_group_fill
  
  # Create an unordered noise_level. Also drop unused levels
  dat$age_group = droplevels(dat$age_group)
  dat$gender = droplevels(dat$gender)
  dat$noise_level_f = factor(dat$noise_level, ordered = F)

  # Set the contrasts for everything
  contrasts(dat$age_group) = contrast_func(length(levels(dat$age_group)))
  contrasts(dat$robot_experience) = contrast_func(length(levels(dat$robot_experience)))
  contrasts(dat$gender) = contrast_func(length(levels(dat$gender)))

  contrasts(dat$has_ax) = contrast_func(2)
  contrasts(dat$has_dx) = contrast_func(2)
  contrasts(dat$has_noise) = contrast_func(2)
  contrasts(dat$has_dxax) = contrast_func(2)

  contrasts(dat$noise_level_f) = contrast_func(length(levels(dat$noise_level_f)))
  
  # Return the data frame
  return(dat)
}

# Get the users df and the actions df
users = loadCSV("users.csv", 3, contr.sum)
actions = loadCSV("actions.csv", 3, contr.sum)

# Change more binary responses to integers
actions$optimal_ax = as.integer(actions$optimal_ax)
actions$chose_ax = as.integer(actions$chose_ax)
actions$optimal_dx = as.integer(actions$optimal_dx)
actions$chose_dx = as.integer(actions$optimal_dx)

# Relabel user ids to be in the range 1-200. Otherwise, we're using DB ids
# The user ID column in the users table that we can now join on is X1
actions$user_id = actions %>% group_indices(user_id)

# Code to relevel the age group factor to remove unused levels. Also, predict the
# value of the 1 unknown age from the other indicators of demographics
# NOTE: If we retrain the model and the output doesn't match the hardcoded
# values, then update those values
if (train_models) {
  plot_df = subset(users, users$age_group != 0)
  plot_df$age_group = factor(plot_df$age_group, levels = seq(from = 0, to = 8), ordered = T)
  plot_df$age_group = droplevels(plot_df$age_group)
  age_group_model = brm(age_group ~ gender + robot_experience, data = plot_df, family = "cumulative")
  data_to_predict = users %>% filter(age_group == 0) %>% select(c("robot_experience", "gender"))
  as_tibble(predict(age_group_model, data_to_predict))
# # A tibble: 1 x 7
#   `P(Y = 2)` `P(Y = 3)` `P(Y = 4)` `P(Y = 5)` `P(Y = 6)` `P(Y = 7)` `P(Y = 8)`
#        <dbl>      <dbl>      <dbl>      <dbl>      <dbl>      <dbl>      <dbl>
# 1     0.0707      0.230      0.218      0.174      0.105       0.09      0.113
  rm(age_group_model)
}
```


# User Level Metrics {.tabset}

These are metrics where an individual unit of analysis is the user.

## Scenario Completed?

**Did the person complete the scenario or not?**

```{r}
plot_df = users %>%
  select(X1, id, study_condition, start_condition, num_optimal,
         age_group, gender, robot_experience,
         noise_level, noise_level_f, has_noise, has_dx, has_ax,
         scenario_completed)
```

```{r fig.height=7, fig.width=15}
# Plot by the study condition
plot_df %>%
  mutate(scenario_completed = factor(scenario_completed)) %>%
  mutate(scenario_completed = fct_recode(scenario_completed, incomplete="0", complete = "1")) %>%
  count(study_condition, scenario_completed) %>%
  ggplot(aes(study_condition, n, fill=scenario_completed)) +
    geom_bar(stat="identity") +
    labs(y = "Number completed")

# Visualize the data by the three variables that we care about
gg_df = plot_df %>%
  mutate(scenario_completed = factor(scenario_completed)) %>%
  mutate(scenario_completed = fct_recode(scenario_completed, incomplete="0", complete = "1")) %>%
  mutate(noise_level = fct_rev(noise_level)) %>%
  count(has_ax, has_dx, noise_level, scenario_completed)

p1 = gg_df %>%
  ggplot(aes(has_ax, n, fill=scenario_completed)) +
    geom_bar(stat = "identity") +
    facet_grid(rows = vars(noise_level), cols = vars(has_dx), labeller = label_both) +
    labs(y = "Number completed")

p2 = gg_df %>%
  ggplot(aes(has_dx, n, fill=scenario_completed)) +
    geom_bar(stat = "identity") +
    facet_grid(rows = vars(noise_level), cols = vars(has_ax), labeller = label_both) +
    labs(y = "Number completed")

p3 = gg_df %>%
  mutate(noise_level = fct_rev(noise_level)) %>%
  mutate(has_ax = factor(has_ax)) %>%
  mutate(has_ax = fct_rev(has_ax)) %>%
  ggplot(aes(noise_level, n, fill=scenario_completed)) +
    geom_bar(stat = "identity") +
    facet_grid(rows = vars(has_ax), cols = vars(has_dx), labeller = label_both) +
    labs(y = "Number completed")

grid.arrange(p1, p2, p3, ncol = 3)
```

Based on the data, we assume the following structural model:

$$scenario\_completed_i = Bernoulli(p_i)$$
$$\begin{aligned}
logit(p_i) &= \beta_0 + \beta_{ax}\text{ax}_i + \beta_{dx}\text{dx}_i + \beta_{noise}\text{noise}_i +\\ &\beta_{ax:noise}\text{ax}_i\text{noise}_i + \beta_{dx:noise}\text{dx}_i\text{noise}_i + \\
&\beta_{no}\text{no}_i + \mathbf{\beta_{demo}X_{demo,i}}
\end{aligned}$$
$$\beta_{.} \sim Normal(0, 10)$$
~~(the exact prior on the parameters is left unspecified so that `brms` can choose intelligent defaults).~~

Model fitting:

```{r, eval=train_models}
# The trend model to see if there is a trend in the noise level variable
scenario_completed.model.t = brm(
  scenario_completed ~ 0 + Intercept + ((has_dx + has_ax) * noise_level) + (gender + age_group + robot_experience) + num_optimal,
  family = "bernoulli",
  prior = set_prior("normal(0, 10)", class = "b"),
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
scenario_completed.model.t = add_criterion(scenario_completed.model.t, "waic")
scenario_completed.model.t = add_criterion(scenario_completed.model.t, "loo", reloo = T)
saveModel(scenario_completed.model.t)

# The values model, to see if specific values of the noise level variable are significant
scenario_completed.model.v = brm(
  scenario_completed ~ 0 + Intercept + ((has_dx + has_ax) * noise_level_f) + (gender + age_group + robot_experience) + num_optimal,
  family = "bernoulli",
  prior = set_prior("normal(0, 10)", class = "b"),
  data = plot_df,
  seed = default_seed,
  save_all_pars = T,
  sample_prior = T
)
scenario_completed.model.v = add_criterion(scenario_completed.model.v, "waic")
scenario_completed.model.v = add_criterion(scenario_completed.model.v, "loo", reloo = T)
saveModel(scenario_completed.model.v)
# More model plots are available at:
# https://bookdown.org/ajkurz/Statistical_Rethinking_recoded/counting-and-classification.html

# Attempted RStanarm models: The estimate contrasts doesn't seem to be as helpful?
# scenario_completed.model.v = stan_glm(
#   scenario_completed ~ ((has_dx + has_ax) * noise_level) + (gender + age_group + robot_experience) + num_optimal,
#   family = binomial(link = "logit"),
#   prior = normal(0, 10),
#   prior_intercept = normal(0, 10),
#   data = plot_df,
#   QR = T,
#   seed = default_seed
# )
```
```{r, eval=!train_models}
# Load the models
scenario_completed.model.t = loadModel("scenario_completed.model.t")
scenario_completed.model.v = loadModel("scenario_completed.model.v")
```

Model fitting results:

```{r}
# Print the parameters, and some initial diagnostics
print(tidy_stan(scenario_completed.model.t))
print(performance::r2(scenario_completed.model.t))

print(tidy_stan(scenario_completed.model.v))
print(performance::r2(scenario_completed.model.v))
```

Diagnostics of the fit:

```{r}
# Alternative diagnostics: diagnostic_posterior(scenario_completed.model.v)

# LOO cross-validation results
print(scenario_completed.model.t$criteria$loo$estimates)
print(scenario_completed.model.v$criteria$loo$estimates)
print(loo_compare(scenario_completed.model.t, scenario_completed.model.v))

# Diagnostics of the sampling process
par(mfrow=c(1,2))
plot(scenario_completed.model.t$criteria$loo, main = "Trend Model")
plot(scenario_completed.model.v$criteria$loo, main = "Values Model")

# Include posterior predictive checks of the sampling (are sampling according
# to the data that we actually have?)
p1 = pp_check(scenario_completed.model.t, type = "bars_grouped", group = "has_ax", stat = "median") + ggtitle("t:has_ax")
p2 = pp_check(scenario_completed.model.t, type = "bars_grouped", group = "has_dx") + ggtitle("t:has_dx")
p3 = pp_check(scenario_completed.model.t, type = "bars_grouped", group = "noise_level") + ggtitle("t:noise_level")
grid.arrange(p1, p2, p3, nrow = 3)
mcmc_intervals(as.matrix(scenario_completed.model.t), regex_pars = "b_", prob = 0.90, prob_outer = 0.95)

p1 = pp_check(scenario_completed.model.v, type = "bars_grouped", group = "has_ax") + ggtitle("v:has_ax")
p2 = pp_check(scenario_completed.model.v, type = "bars_grouped", group = "has_dx") + ggtitle("v:has_dx")
p3 = pp_check(scenario_completed.model.v, type = "bars_grouped", group = "noise_level_f") + ggtitle("v:noise_level")
grid.arrange(p1, p2, p3, nrow = 3)
mcmc_intervals(as.matrix(scenario_completed.model.v), regex_pars = "b_", prob = 0.90, prob_outer = 0.95)
```

Inferences from the fitting:

```{r, eval=F}
# Test hypotheses. The NULL arguments need to be provided in this case
test_hypotheses = function(h_df = NULL, hypotheses_list = NULL, model = NULL) {
  # # Option 1: Use the hypothesis function in brms
  # hyp_results = hypothesis(model, hypothesis = hypotheses_list, seed = default_seed)
  # print(hyp_results$hypothesis)
  # return(hyp_results)

  # Option 2: Use the ROPE & Overlap amount
  hyp_results =
    h_df %>%
      equivalence_test() %>%
      as_tibble() %>%
      bind_cols(h_df %>% pd() %>% select(pd))
  print(hyp_results)
  return(hyp_results)
}

# # If we're using the brms functions, then use the following
# common_hyp_to_test = c("-2 * has_ax1 = 0", "-2 * has_dx1 = 0")
# noise_levels_hyp_to_test = c(
#   "Intercept-noise_level_f1 = 0", "Intercept-noise_level_f2 = 0", "Intercept-noise_level_f1-noise_level_f2 = 0",
#   "Intercept-has_ax1:noise_level_f1 = 0", "Intercept-has_dx1:noise_level_f1 = 0",
#   "Intercept-has_ax1:noise_level_f2 = 0", "Intercept-has_dx1:noise_level_f2 = 0",
#   "Intercept-has_ax1:noise_level_f1-has_ax1:noise_level_f2 = 0",
#   "Intercept-has_dx1:noise_level_f1-has_dx1:noise_level_f2 = 0"
# )

# Check the significance of each hypothesis
h_df = as_tibble(insight::get_parameters(scenario_completed.model.t)) %>%
  transmute(
    has_ax_test = -2 * b_has_ax1,
    has_dx_test = -2 * b_has_dx1,
    noise_levelL_test = b_noise_level.L,
    noise_levelQ_test = b_noise_level.Q,
    has_ax_noise_levelL_test = b_has_ax1.noise_level.L,
    has_ax_noise_levelQ_test = b_has_ax1.noise_level.Q,
    has_dx_noise_levelL_test = b_has_dx1.noise_level.L,
    has_dx_noise_levelQ_test = b_has_dx1.noise_level.Q
  )

hyp_results = test_hypotheses(h_df)

h_df = as_tibble(insight::get_parameters(scenario_completed.model.v)) %>%
  transmute(
    has_ax_test = -2 * b_has_ax1,
    has_dx_test = -2 * b_has_dx1,
    noise_level1_test = b_Intercept - b_noise_level_f1,
    noise_level2_test = b_Intercept - b_noise_level_f2,
    noise_level3_test = b_Intercept - b_noise_level_f1 - b_noise_level_f2,
    has_ax_noise_level_1_test = b_Intercept - b_has_ax1.noise_level_f1,
    has_ax_noise_level_2_test = b_Intercept - b_has_ax1.noise_level_f2,
    has_ax_noise_level_3_test = b_Intercept - b_has_ax1.noise_level_f1 - b_has_ax1.noise_level_f2,
    has_dx_noise_level_1_test = b_Intercept - b_has_dx1.noise_level_f1,
    has_dx_noise_level_2_test = b_Intercept - b_has_dx1.noise_level_f2,
    has_dx_noise_level_3_test = b_Intercept - b_has_dx1.noise_level_f1 - b_has_dx1.noise_level_f2,
  )

hyp_results = test_hypotheses(h_df)
```

So here are the following plots:

```{r, eval=F}
# Generate our own posterior predictions
p_df =
  plot_df %>%
  data_grid(scenario_completed, has_ax, has_dx, noise_level_f, gender, age_group, robot_experience, num_optimal)

fits_p_df = p_df %>% add_fitted_draws(scenario_completed.model.v)
preds_p_df = p_df %>% add_predicted_draws(scenario_completed.model.v)

# Binary responses are obvious
plot_df %>%
  ggplot(aes(y = gender, x = chose_ax / num_actions)) +
  stat_halfeyeh(aes(x = .value / num_actions), position = position_nudge(y = 0.1), data = fits_p_df) +
  stat_intervalh(aes(x = .prediction / num_actions), data = preds_p_df) +
  geom_point(data = plot_df, color = 'gray')

gg_df = plot_df %>%
  mutate(scenario_completed = factor(scenario_completed)) %>%
  mutate(scenario_completed = fct_recode(scenario_completed, incomplete="0", complete = "1")) %>%
  mutate(noise_level = fct_rev(noise_level)) %>%
  count(has_ax, has_dx, noise_level, scenario_completed)

p1 = gg_df %>%
  ggplot(aes(has_ax, n, fill=scenario_completed)) +
    geom_bar(stat = "identity") +
    facet_grid(rows = vars(noise_level), cols = vars(has_dx), labeller = label_both) +
    labs(y = "Number completed")


# # For multi-level responses, we should do a pairwise comparison
# plot_df %>%
#   ggplot(aes(y = robot_experience, x = chose_ax / num_actions)) +
#   stat_halfeyeh(aes(x = .value / num_actions), position = position_nudge(y = 0.1), data = fits) +
#   stat_intervalh(aes(x = .prediction / num_actions), data = preds) +
#   geom_point(data = plot_df, color = 'gray')
# 
# # I don't understand why there are more than 2 age groups... is it because of the sampling process?
# plot_df %>%
#   ggplot(aes(y = age_group, x = chose_ax / num_actions)) +
#   stat_halfeyeh(aes(x = .value / num_actions), position = position_nudge(y = 0.1), data = fits) +
#   stat_intervalh(aes(x = .prediction / num_actions), data = preds) +
#   geom_point(data = plot_df, color = 'gray')

```

Based on the data, only the effect of the ax variable matters

```{r, eval=F}
plot_summs(scenario_completed.model, scenario_completed.model.sparse,
           omit.coefs = c("age_group.L", "age_group.Q", "age_group.C", "age_group^4",
                          "age_group^5", "age_group^6", "age_group^7", "(Intercept)"))
plot_summs(scenario_completed.model.sparse, omit.coefs = NULL, plot.distributions = T)
# ggPredict(scenario_completed.model.sparse, se = T)
```

```{r, message=F, warning=F}
rm(scenario_completed.model.t, scenario_completed.model.v)
# rm(fits_p_df, preds_p_df, p_df, gg_df, plot_df)
gc()
```


## Num Actions Diff {.tabset}

We assume a structural model where we remove the effect that the action suggestions might have on the number of actions over the number of optimal actions taken by using the prediction of action completion based on the action suggestions

```
num_actions_diff ~ ((has_dx + has_ax) * noise_level) + (gender + age_group + robot_experience) + num_optimal
# Optionally
num_actions_diff ~ ((has_dx + has_ax) * noise_level) + (gender + age_group + robot_experience) + num_optimal + scenario_completed
```

We use a negative binomial distribution to estimate the params, based on tests in the other notebook. First, let's examine without a control for scenario completed.

### Naive Model

```{r, eval=F}
plot_df = users;
plot_df$scenario_completed = factor(plot_df$scenario_completed)
```
```{r, eval=F}
actions_diff.model = glm.nb(
  num_actions_diff ~ ((has_dx + has_ax) * noise_level) + (gender + age_group + robot_experience) + num_optimal,
  data = plot_df
)
saveModel(actions_diff.model)
```
```{r, eval=F}
actions_diff.model = loadModel("actions_diff.model")
```
```{r, eval=F}
print(with(actions_diff.model, pchisq(null.deviance - deviance, df.null - df.residual, lower.tail = FALSE)))
print(summ(actions_diff.model))
```

There is a high effect of robot experience and of action suggestions. How does a reduced model fare?

```{r, eval=F}
actions_diff.model.sparse = glm.nb(
  num_actions_diff ~ robot_experience + has_ax,
  data = plot_df
)
saveModel(actions_diff.model.sparse)
```
```{r, eval=F}
print(with(actions_diff.model.sparse, pchisq(null.deviance - deviance, df.null - df.residual, lower.tail = FALSE)))
print(summ(actions_diff.model.sparse))

# Also show comparisons (remove the age factors because there's massive variation in those)
# Turns out that we can't plot the non-sparse model for some reason.
# plot_summs(actions_diff.model, actions_diff.model.sparse,
#            omit.coefs = c("age_group.L", "age_group.Q", "age_group.C", "age_group^4",
#                           "age_group^5", "age_group^6", "age_group^7", "(Intercept)"))
plot_summs(actions_diff.model.sparse, omit.coefs = NULL, plot.distributions = T)
# Cannot plot binomial responses?
plot_df$fitted = actions_diff.model.sparse$fitted.values
ggplot(plot_df, aes(x = robot_experience, y = num_actions_diff, color = has_ax, fill = has_ax)) +
  geom_boxplot(aes(alpha = 0.3)) +
  geom_jitter() +
  geom_line(aes(y = fitted, x = robot_experience, color = has_ax, group = has_ax, size = 2))
```

Unfortunately, while AIC is lower, the R^2 has also gone lower. Let's look at the data.

```{r, eval=F}
gg_df = plot_df
gg_df$study_condition = factor(plot_df$study_condition, levels = c("DX_100", "AX_100", "DXAX_100", "DX_90", "AX_90", "DXAX_90", "DX_80", "AX_80", "DXAX_80", "BASELINE"))
ggplot(gg_df, aes(x = study_condition, y = num_actions_diff, fill = noise_level)) +
  geom_boxplot(aes(alpha = 0.3)) +
  geom_jitter(aes(color = noise_level)) +
  theme(axis.text.x = element_text(angle = 45))

# Normalize by the number of optimal actions...?
ggplot(gg_df, aes(x = study_condition, y = frac_actions_diff, fill = noise_level)) +
  geom_boxplot(aes(alpha = 0.3)) +
  geom_jitter(aes(color = noise_level)) +
  theme(axis.text.x = element_text(angle = 45))
```

So according to the second plot, there is a significant difference of whether a participant completed the scenario or not (fraction of people at max is its own distribution). So we're going to add that as a factor


### Augmented Model

This time, we plot the data first:

```{r, eval=F}
gg_df = subset(plot_df, scenario_completed == "1")
gg_df$study_condition = factor(gg_df$study_condition, levels = c("DX_100", "AX_100", "DXAX_100", "DX_90", "AX_90", "DXAX_90", "DX_80", "AX_80", "DXAX_80", "BASELINE"))
ggplot(gg_df, aes(x = study_condition, y = num_actions_diff, fill = noise_level)) +
  geom_boxplot(aes(alpha = 0.3)) +
  geom_jitter(aes(color = noise_level)) +
  theme(axis.text.x = element_text(angle = 45))
```

```{r, eval=F}
plot_df = users;
plot_df$scenario_completed = factor(plot_df$scenario_completed)
```
```{r, eval=F}
actions_diff.model.augmented = glm.nb(
  num_actions_diff ~ ((has_dx + has_ax) * noise_level) + (gender + age_group + robot_experience) + num_optimal + scenario_completed,
  data = plot_df
)
saveModel(actions_diff.model.augmented)
```
```{r, eval=F}
actions_diff.model.augmented = loadModel("actions_diff.model.augmented")
```
```{r, eval=F}
print(with(actions_diff.model.augmented, pchisq(null.deviance - deviance, df.null - df.residual, lower.tail = FALSE)))
print(summ(actions_diff.model.augmented))
```

There is indeed a large effect of the scenario completed; and controlling for it seems to negate the effect of whether action suggestions are present. Creating the sparser model from this full(er) model based on the factors that are deemed significant

```{r, eval=F}
actions_diff.model.augmented.sparse = glm.nb(
  num_actions_diff ~ gender + robot_experience + scenario_completed + has_dx*noise_level,
  data = plot_df
)
saveModel(actions_diff.model.augmented.sparse)
```
```{r, eval=F}
actions_diff.model.augmented.sparse = loadModel("actions_diff.model.augmented.sparse")
```
```{r, eval=F}
print(with(actions_diff.model.augmented.sparse, pchisq(null.deviance - deviance, df.null - df.residual, lower.tail = FALSE)))
print(summ(actions_diff.model.augmented.sparse))

# Plot the coefficients. For some unknown reason, we cannot plot this model either
plot_summs(actions_diff.model.augmented.sparse, plot.distributions = T)

# Plot the DX and noise fit
plot_df$fitted = actions_diff.model.augmented.sparse$fitted.values
plot_df$scenario_completed = factor(plot_df$scenario_completed)
ggplot(plot_df, aes(x = gender, y = num_actions_diff, color = scenario_completed, fill = scenario_completed)) +
  geom_boxplot(aes(alpha = 0.3)) +
  geom_jitter() +
  geom_line(aes(y = fitted, x = gender, color = scenario_completed, group = scenario_completed, size = 2))
```

```{r, message=F, warning=F, eval=F}
rm(actions_diff.model, actions_diff.model.sparse, actions_diff.model.augmented, actions_diff.model.augmented.sparse)
gc()
```


# Action Level Metrics {.tabset}

## Taking Optimal Actions

One can almost think of this as an inverse of the "reliance" metric mentioned by Jessie Yang et al. We're going to need brms to fit all the models here.

```{r}
gg_df = users
gg_df$study_condition = factor(gg_df$study_condition, levels = c("DX_100", "AX_100", "DXAX_100", "DX_90", "AX_90", "DXAX_90", "DX_80", "AX_80", "DXAX_80", "BASELINE"))
ggplot(gg_df, aes(x = study_condition, y = num_ax_optimal, fill = noise_level)) +
  geom_boxplot(aes(alpha = 0.3)) +
  # geom_jitter(aes(color = noise_level)) +
  theme(axis.text.x = element_text(angle = 45))

# This is technically what the binomial regression is checking
ggplot(gg_df, aes(x = study_condition, y = frac_ax_optimal, fill = noise_level)) +
  geom_boxplot(aes(alpha = 0.3)) +
  # geom_jitter(aes(color = noise_level)) +
  theme(axis.text.x = element_text(angle = 45))
```

The choice of the model family is informed by the tests in the second markdown file.

```{r, eval=F}
plot_df = actions
```
```{r, eval=F}
# Fit the main model
optimal_action.model = brm(
  optimal_ax | trials(num_actions) ~ ((has_dx + has_ax) * noise_level) + (age_group + robot_experience + gender) + state_idx + (1 | user_id),
  data = plot_df,
  family = "binomial",
  cores = 4
)
optimal_action.model = add_criterion(optimal_action.model, "waic")
optimal_action.model = add_criterion(optimal_action.model, "loo")
saveModel(optimal_action.model)
```
```{r, eval=F}
optimal_action.model = loadModel("optimal_action.model")
```
```{r, eval=F}
print(summary(optimal_action.model))
print(optimal_action.model$criteria$loo$estimates)
```

```{r, eval=F}
# Plot the effects of the condition (the stuff we care about)
hyp_to_test = c("has_axTRUE = 0", "has_dxTRUE = 0",
                "noise_level.Q = 0", "noise_level.L = 0",
                "has_axTRUE:noise_level.Q = 0", "has_axTRUE:noise_level.L = 0",
                "has_dxTRUE:noise_level.Q = 0", "has_dxTRUE:noise_level.L = 0")
hyp_results = hypothesis(optimal_action.model, hypothesis = hyp_to_test)
print(as.matrix(hyp_results$hypothesis))
plot(hyp_results, ask = F)

mcmc_areas(as.matrix(optimal_action.model), regex_pars = "has_|noise_", prob = 0.95, prob_outer = 1)
mcmc_intervals(as.matrix(optimal_action.model), regex_pars = "b_", prob = 0.90, prob_outer = 0.95)
```

Based on the plots, it seems that a reduced model for this model might include state_idx, robot_experience, age_group, and has_ax.

```{r, eval=F}
optimal_action.model.sparse = brm(
  optimal_ax | trials(num_actions) ~ has_ax + age_group + robot_experience + state_idx + (1 | user_id),
  data = plot_df,
  family = "binomial",
  cores = 4,
)
optimal_action.model.sparse = add_criterion(optimal_action.model.sparse, "waic")
optimal_action.model.sparse = add_criterion(optimal_action.model.sparse, "loo")
saveModel(optimal_action.model.sparse)
```
```{r, eval=F}
optimal_action.model.sparse = loadModel("optimal_action.model.sparse")
```
```{r, eval=F}
print(summary(optimal_action.model.sparse))
print(optimal_action.model.sparse$criteria$loo$estimates)
print(loo_compare(optimal_action.model, optimal_action.model.sparse))

# Plot the effects of the condition (the stuff we care about)
hyp_to_test = c("has_axTRUE = 0")
hyp_results = hypothesis(optimal_action.model.sparse, hypothesis = hyp_to_test)
print(as.matrix(hyp_results$hypothesis))
plot(hyp_results, ask = F)

mcmc_areas(as.matrix(optimal_action.model.sparse), regex_pars = "has_|noise_", prob = 0.95, prob_outer = 1)
mcmc_intervals(as.matrix(optimal_action.model.sparse), regex_pars = "b_", prob = 0.90, prob_outer = 0.95)
```

So there are three independent effects; get the conditional plots for those

```{r, eval=F}
plot(conditional_effects(optimal_action.model.sparse, effects = "has_ax"), ask = F, points = T)
plot(conditional_effects(optimal_action.model.sparse, effects = "robot_experience"), ask = F, points = T)
plot(conditional_effects(optimal_action.model.sparse, effects = "age_group"), ask = F, points = T)
```
```{r, eval=F}
# We want to create our own conditional sample, but don't want to kill our machine
plot_df$state_idx = mean(plot_df$state_idx)
g = subset(plot_df, age_group == 0 | age_group == 8) %>%
  data_grid(num_actions, has_ax, age_group, robot_experience, state_idx, user_id)
fits = g %>% add_fitted_draws(optimal_action.model.sparse, n=10)
preds = g %>% add_predicted_draws(optimal_action.model.sparse, n=10)

# Binary responses are obvious
plot_df %>%
  ggplot(aes(y = has_ax, x = optimal_ax / num_actions)) +
  stat_halfeyeh(aes(x = .value / num_actions), position = position_nudge(y = 0.1), data = fits) +
  stat_intervalh(aes(x = .prediction / num_actions), data = preds) +
  geom_point(data = plot_df, color = 'gray')

# For multi-level responses, we should do a pairwise comparison
plot_df %>%
  ggplot(aes(y = robot_experience, x = optimal_ax / num_actions)) +
  stat_halfeyeh(aes(x = .value / num_actions), position = position_nudge(y = 0.1), data = fits) +
  stat_intervalh(aes(x = .prediction / num_actions), data = preds) +
  geom_point(data = plot_df, color = 'gray')

# I don't understand why there are more than 2 age groups... is it because of the sampling process?
plot_df %>%
  ggplot(aes(y = age_group, x = optimal_ax / num_actions)) +
  stat_halfeyeh(aes(x = .value / num_actions), position = position_nudge(y = 0.1), data = fits) +
  stat_intervalh(aes(x = .prediction / num_actions), data = preds) +
  geom_point(data = plot_df, color = 'gray')
```

```{r, message=F, warning=F, eval=F}
rm(optimal_action.model, optimal_action.model.sparse)
gc()
```


## Following Suggestions of Actions


```{r, eval=F}
gg_df = subset(users, has_ax)
gg_df$study_condition = factor(gg_df$study_condition, levels = c("DX_100", "AX_100", "DXAX_100", "DX_90", "AX_90", "DXAX_90", "DX_80", "AX_80", "DXAX_80", "BASELINE"))
ggplot(gg_df, aes(x = study_condition, y = num_ax_followed, fill = noise_level)) +
  geom_boxplot(aes(alpha = 0.3)) +
  # geom_jitter(aes(color = noise_level)) +
  theme(axis.text.x = element_text(angle = 45))

# This is technically what the binomial regression is checking
ggplot(gg_df, aes(x = study_condition, y = frac_ax_followed, fill = noise_level)) +
  geom_boxplot(aes(alpha = 0.3)) +
  # geom_jitter(aes(color = noise_level)) +
  theme(axis.text.x = element_text(angle = 45))
```

The choice of the model family is informed by the tests in the second markdown file.

```{r, eval=F}
plot_df = subset(actions, has_ax)
```
```{r, eval=F}
# Fit the main model
follow_action.model = brm(
  chose_ax | trials(num_actions) ~ (has_dx * noise_level) + (age_group + robot_experience + gender) + state_idx + (1 | user_id),
  data = plot_df,
  family = "binomial",
  cores = 4
)
follow_action.model = add_criterion(follow_action.model, "waic")
follow_action.model = add_criterion(follow_action.model, "loo")
saveModel(follow_action.model)
```
```{r, eval=F}
follow_action.model = loadModel("follow_action.model")
```
```{r, eval=F}
print(summary(follow_action.model))
print(follow_action.model$criteria$loo$estimates)
```

```{r, eval=F}
# Plot the effects of the condition (the stuff we care about)
hyp_to_test = c("has_dxTRUE = 0",
                "noise_level.Q = 0", "noise_level.L = 0",
                "has_dxTRUE:noise_level.Q = 0", "has_dxTRUE:noise_level.L = 0")
hyp_results = hypothesis(follow_action.model, hypothesis = hyp_to_test)
print(as.matrix(hyp_results$hypothesis))
plot(hyp_results, ask = F)

mcmc_areas(as.matrix(follow_action.model), regex_pars = "has_|noise_", prob = 0.95, prob_outer = 1)
mcmc_intervals(as.matrix(follow_action.model), regex_pars = "b_", prob = 0.90, prob_outer = 0.95)
```

Based on the plots, it seems that a reduced model for this model might include state_idx, robot_experience, age_group, and gender

```{r, eval=F}
follow_action.model.sparse = brm(
  chose_ax | trials(num_actions) ~ gender + age_group + robot_experience + state_idx + (1 | user_id),
  data = plot_df,
  family = "binomial",
  cores = 4,
)
follow_action.model.sparse = add_criterion(follow_action.model.sparse, "waic")
follow_action.model.sparse = add_criterion(follow_action.model.sparse, "loo")
saveModel(follow_action.model.sparse)
```
```{r, eval=F}
follow_action.model.sparse = loadModel("follow_action.model.sparse")
```
```{r, eval=F}
print(summary(follow_action.model.sparse))
print(follow_action.model.sparse$criteria$loo$estimates)
print(loo_compare(follow_action.model, follow_action.model.sparse))

# Our IV is not part of this condition
# hyp_to_test = c("has_axTRUE = 0")
# hyp_results = hypothesis(follow_action.model.sparse, hypothesis = hyp_to_test)
# print(as.matrix(hyp_results$hypothesis))
# plot(hyp_results, ask = F)

# mcmc_areas(as.matrix(follow_action.model.sparse), regex_pars = "has_|noise_", prob = 0.95, prob_outer = 1)
mcmc_intervals(as.matrix(follow_action.model.sparse), regex_pars = "b_", prob = 0.90, prob_outer = 0.95)
```

So there are three independent effects; get the conditional plots for those

```{r, eval=F}
plot(conditional_effects(follow_action.model.sparse, effects = "gender"), ask = F, points = T)
plot(conditional_effects(follow_action.model.sparse, effects = "robot_experience"), ask = F, points = T)
plot(conditional_effects(follow_action.model.sparse, effects = "age_group"), ask = F, points = T)
```
```{r, eval=F}
# We want to create our own conditional sample, but don't want to kill our machine
plot_df$state_idx = mean(plot_df$state_idx)
g = subset(plot_df, age_group == 0 | age_group == 8) %>%
  data_grid(num_actions, gender, age_group, robot_experience, state_idx, user_id)
fits = g %>% add_fitted_draws(follow_action.model.sparse, n=10)
preds = g %>% add_predicted_draws(follow_action.model.sparse, n=10)

# Binary responses are obvious
plot_df %>%
  ggplot(aes(y = gender, x = chose_ax / num_actions)) +
  stat_halfeyeh(aes(x = .value / num_actions), position = position_nudge(y = 0.1), data = fits) +
  stat_intervalh(aes(x = .prediction / num_actions), data = preds) +
  geom_point(data = plot_df, color = 'gray')

# For multi-level responses, we should do a pairwise comparison
plot_df %>%
  ggplot(aes(y = robot_experience, x = chose_ax / num_actions)) +
  stat_halfeyeh(aes(x = .value / num_actions), position = position_nudge(y = 0.1), data = fits) +
  stat_intervalh(aes(x = .prediction / num_actions), data = preds) +
  geom_point(data = plot_df, color = 'gray')

# I don't understand why there are more than 2 age groups... is it because of the sampling process?
plot_df %>%
  ggplot(aes(y = age_group, x = chose_ax / num_actions)) +
  stat_halfeyeh(aes(x = .value / num_actions), position = position_nudge(y = 0.1), data = fits) +
  stat_intervalh(aes(x = .prediction / num_actions), data = preds) +
  geom_point(data = plot_df, color = 'gray')
```

```{r, message=F, warning=F, eval=F}
rm(follow_action.model, follow_action.model.sparse)
gc()
```
