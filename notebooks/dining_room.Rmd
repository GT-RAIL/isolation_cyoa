---
title: "AMT Effects of Suggestions"
output:
  html_document:
    theme: readable
    code_folding: hide
    df_print: kable
    toc: true
    toc_depth: 2
    toc_float: true
---

```{r message=FALSE, warning=FALSE}
library(car)
library(lme4)
library(stats)
library(grid)
library(gridExtra)
library(fitdistrplus)
# library(dplyr)
# library(forcats)
# library(tibble)
# library(tidyr)
# library(readr)
# library(ggplot2)
# library(stringr)
library(tidyverse)
library(ggsignif)
library(ggthemes)
library(jtools)
library(broom)
library(modelr)
library(loo)
# library(rstanarm)
library(brms)
library(bayesplot)
library(GGally)
library(tidybayes)
library(bayestestR)
library(sjstats)
library(report)
library(see)

# Setup for multiprocessing
options(mc.cores = 4)
# library(future)
# plan(multiprocess)

# Make the default coding of contrasts sum-coding; just in case
options(contrasts = c("contr.sum", "contr.poly"))
```
```{r}
# Script execution globals
train_models = F     # Retrain the MCMC models. Also infer unknown demographics
plot_posteriors = T  # Plot the posterior distributions
plot_diagnostics = T # Plot the diagnostics
report_bayesfactors = F  # Report the Bayes Factor scores of model parameters
default_seed = 0x1331 # For repeatable models and diagnostics
data_folder = "~/Documents/GT/Research/Data/arbitration/2019-12-09/results"
```
```{r, message=FALSE, warning=FALSE}
# Create a simple coding contrasts matrix
contr.deviation = function(nlevels) {
  x1 = contr.treatment(nlevels)
  x2 = matrix(rep(1/nlevels, (nlevels-1) * nlevels), ncol = (nlevels-1))
  return(x1-x2)
}

# NOTE: For now we're ignoring this advice
# # Use a simple coding contrasts matrix to add our own contrasts columns
# # Explained in: http://talklab.psy.gla.ac.uk/tvw/catpred/
# create_simple_contrasts = function(d, column_name) {
#  factor_column = d %>% pull(column_name)
#  contrast_matrix = contr.deviation(length(levels(factor_column)))
# }

# Save a model
saveModel = function(model, folder = data_folder) {
  saveRDS(model, file = file.path(folder, paste(substitute(model), ".rds", sep = '')))
}

# Load a model
loadModel = function(model_name, folder = data_folder) {
  return(readRDS(file.path(folder, paste(model_name, '.rds', sep = ''))))
}

# Test hypotheses. The NULL arguments need to be provided in this case
test_hypotheses = function(h_df = NULL, rope_values = NULL, hypotheses_list = NULL, model = NULL) {
  # Option 1: Use the hypothesis function in brms
  if (!is.null(hypotheses_list)) {
    hyp_results = hypothesis(model, hypothesis = hypotheses_list, seed = default_seed)
    return(hyp_results)
  }

  # Option 2: Use the ROPE & Overlap amount
  else {
    if (is.null(rope_values)) {
      rope_values = c(-0.1, 0.1)
    }
    hyp_results =
      h_df %>%
        equivalence_test(range = rope_values) %>%
        as_tibble() %>%
        bind_cols(h_df %>% pd() %>% select(pd))
    return(hyp_results)
  }
}

# Fit distributions to vectors and print the results. Also return the fit, if necessary
fit_and_print_dist = function(vec, distr, string) {
  f = fitdist(vec, distr)
  print(paste(string, f$loglik, f$aic))
  return(f)
}
```
```{r, message=F, warning=F}
# Load the CSV files. If the age_group fill model is run again and we get a different output,
# then remember to update the value here
loadCSV = function(filename, age_group_fill, contrast_func) {
  dat = read_csv(
    file.path(data_folder, filename),
    col_types = cols(
      study_condition = col_factor(),
      suggestion_type = col_factor(levels = c("NONE", "AX", "DX", "DXAX")),
      noise_level = col_factor(ordered = T),
      gender = col_factor(levels = c("F", "M", "U")),
      age_group = col_factor(levels = seq(from = 0, to = 8)),
      robot_experience = col_factor(levels = seq(from = 0, to = 4))
    )
  )

  # Relabel the factors
  dat = dat %>%
    mutate(study_condition = fct_recode(study_condition,
                                        BASELINE="1",
                                        DX_100="2", AX_100="3", DXAX_100="4",
                                        DX_90="5", AX_90="6", DXAX_90="7",
                                        DX_80="8", AX_80="9", DXAX_80="10")) %>%
    mutate(study_condition = fct_relevel(study_condition, c("DX_100", "AX_100", "DXAX_100",
                                                            "DX_90", "AX_90", "DXAX_90",
                                                            "DX_80", "AX_80", "DXAX_80")))

  # Relevel the non-binary gender
  dat$gender[dat$gender == 'U'] = 'M'

  # Change binary responses to integers
  dat$scenario_completed = as.integer(dat$scenario_completed)
  
  # Relevel age_group
  dat[dat$age_group == 0,]$age_group = age_group_fill
  
  # Create an unordered noise_level. Also drop unused levels
  dat$age_group = droplevels(dat$age_group)
  dat$gender = droplevels(dat$gender)
  dat$noise_level_f = factor(dat$noise_level, ordered = F)

  # Set the contrasts for everything
  contrasts(dat$age_group) = contrast_func(length(levels(dat$age_group)))
  contrasts(dat$robot_experience) = contrast_func(length(levels(dat$robot_experience)))
  contrasts(dat$gender) = contrast_func(length(levels(dat$gender)))

  contrasts(dat$has_ax) = contrast_func(2)
  contrasts(dat$has_dx) = contrast_func(2)
  contrasts(dat$has_noise) = contrast_func(2)
  contrasts(dat$has_dxax) = contrast_func(2)

  contrasts(dat$noise_level_f) = contrast_func(length(levels(dat$noise_level_f)))
  
  # Return the data frame
  return(dat)
}

# Get the users df and the actions df
users = loadCSV("users.csv", 4, contr.sum)
actions = loadCSV("actions.csv", 4, contr.sum)

# Change more binary responses to integers
actions$optimal_ax = as.integer(actions$optimal_ax)
actions$chose_ax = as.integer(actions$chose_ax)
actions$optimal_dx = as.integer(actions$optimal_dx)
actions$chose_dx = as.integer(actions$optimal_dx)

# Relabel user ids to be in the range 1-200. Otherwise, we're using DB ids
# The user ID column in the users table that we can now join on is X1
actions$user_id = actions %>% group_indices(user_id)

# Rescale state ids
actions$state_idx_rescaled = scales::rescale(actions$state_idx)

# Code to relevel the age group factor to remove unused levels. Also, predict the
# value of the 1 unknown age from the other indicators of demographics
# NOTE: If we retrain the model and the output doesn't match the hardcoded
# values, then update those values
if (train_models) {
  plot_df = subset(users, users$age_group != 0)
  plot_df$age_group = factor(plot_df$age_group, levels = seq(from = 0, to = 8), ordered = T)
  plot_df$age_group = droplevels(plot_df$age_group)
  age_group_model = brm(age_group ~ gender + robot_experience, data = plot_df, family = "cumulative")
  data_to_predict = users %>% filter(age_group == 0) %>% select(c("robot_experience", "gender"))
  as_tibble(predict(age_group_model, data_to_predict))
# # A tibble: 1 x 7
#   `P(Y = 2)` `P(Y = 3)` `P(Y = 4)` `P(Y = 5)` `P(Y = 6)` `P(Y = 7)` `P(Y = 8)`
#        <dbl>      <dbl>      <dbl>      <dbl>      <dbl>      <dbl>      <dbl>
# 1     0.0838      0.202      0.215      0.188      0.110      0.089      0.111
  rm(age_group_model)
}
```
```{r, message=FALSE, warning=FALSE}
# Helper functions copied from http://www.cookbook-r.com/Graphs/Plotting_means_and_error_bars_(ggplot2)/

## Gives count, mean, standard deviation, standard error of the mean, and confidence interval (default 95%).
##   data: a data frame.
##   measurevar: the name of a column that contains the variable to be summariezed
##   groupvars: a vector containing names of columns that contain grouping variables
##   na.rm: a boolean that indicates whether to ignore NA's
##   conf.interval: the percent range of the confidence interval (default is 95%)
summarySE <- function(data=NULL, measurevar, groupvars=NULL, na.rm=FALSE,
                      conf.interval=.95, .drop=TRUE) {
    # library(plyr)

    # New version of length which can handle NA's: if na.rm==T, don't count them
    length2 <- function (x, na.rm=FALSE) {
        if (na.rm) sum(!is.na(x))
        else       length(x)
    }

    # This does the summary. For each group's data frame, return a vector with
    # N, mean, and sd
    datac <- ddply(data, groupvars, .drop=.drop,
      .fun = function(xx, col) {
        c(N    = length2(xx[[col]], na.rm=na.rm),
          mean = mean   (xx[[col]], na.rm=na.rm),
          sd   = sd     (xx[[col]], na.rm=na.rm)
        )
      },
      measurevar
    )

    # Rename the "mean" column    
    datac <- rename(datac, c("mean" = measurevar))

    datac$se <- datac$sd / sqrt(datac$N)  # Calculate standard error of the mean

    # Confidence interval multiplier for standard error
    # Calculate t-statistic for confidence interval: 
    # e.g., if conf.interval is .95, use .975 (above/below), and use df=N-1
    ciMult <- qt(conf.interval/2 + .5, datac$N-1)
    datac$ci <- datac$se * ciMult

    return(datac)
}

powerTransform <- function(y, lambda1, lambda2 = NULL, method = "boxcox") {

  boxcoxTrans <- function(x, lam1, lam2 = NULL) {
    # if we set lambda2 to zero, it becomes the one parameter transformation
    lam2 <- ifelse(is.null(lam2), 0, lam2)
  
    if (lam1 == 0L) {
      log(y + lam2)
    } else {
      (((y + lam2)^lam1) - 1) / lam1
    }
  }

  switch(method
  , boxcox = boxcoxTrans(y, lambda1, lambda2)
  , tukey = y^lambda1
  )
}

# Function for computing the predictions and CIs for the conditional probability
predictCIsLogistic <- function(object, newdata, level = 0.95) {
  # Compute predictions in the log-odds
  pred <- predict(object = object, newdata = newdata, se.fit = TRUE)

  # CI in the log-odds
  za <- qnorm(p = (1 - level) / 2)
  lwr <- pred$fit + za * pred$se.fit
  upr <- pred$fit - za * pred$se.fit

  # Transform to probabilities
  fit <- 1 / (1 + exp(-pred$fit))
  lwr <- 1 / (1 + exp(-lwr))
  upr <- 1 / (1 + exp(-upr))

  # Return a matrix with column names "fit", "lwr" and "upr"
  result <- cbind(fit, lwr, upr)
  colnames(result) <- c("fit", "lwr", "upr")
  return(result)
}

# -loglik(beta)
minusLogLik <- function(beta) {
  p <- 1 / (1 + exp(-(beta[1] + beta[2] * x)))
  -sum(y1 * log(p) + (1 - y1) * log(1 - p))
}

# Plotting function
plotFunGLM <- function(x, eta, y, n = 200, ind = 1, fam = "poisson") {
  
  if (missing(y)) {
    y <- switch(fam,
                "poisson" = rpois(n, lambda = exp(eta)),
                "binomial" = rbinom(n, prob = 1 / (1 + exp(-eta)), size = 1))
  }
  mod <- glm(y ~ x, family = fam)
  if (ind < 7) {
    plot(mod, which = ind,
         main = list("Residuals vs Fitted", "Normal Q-Q", "Scale-Location", 
                     "Cook's distance", "Residuals vs Leverage", 
                     expression("Cook's dist vs Leverage  " 
                                * h[ii]/(1 - h[ii])))[ind], caption = "")
  } else {
    plot(mod$residuals, type = "o", ylab = "Residuals", 
         main = "Residuals series")
  }
  plot(x, y, pch = 16, main = "Data scatterplot")
  t <- seq(-100, 100, l = 1e4)
  lines(t, predict(mod, newdata = data.frame(x = t), type = "response"), 
        col = 2)
  
}


# Computation of the R^2 with a function -- useful for repetitive computations
r2glm <- function(model) {

  summaryLog <- summary(model)
  1 - summaryLog$deviance / summaryLog$null.deviance

}
```

## Metrics

### Scenario Completed

Did the person complete the scenario?

```{r message=FALSE, warning=FALSE}
plot_df = users
x = plot_df$scenario_completed
```

```{r, eval=F}
fnorm = fitdist(x, "norm")
flogis = fitdist(x, "logis")
fpois = fitdist(x, "pois")
fnbinom = fitdist(x, "nbinom")
fgeom = fitdist(x, "geom")

descdist(x, boot=1000)
plotdist(x, histo=T, demp=T)
plot(sort(x))

par(mfrow = c(2, 2))
plot.legend = c("norm", "logit", "poisson", "negbinom", "geom")
denscomp(list(fnorm, flogis, fpois, fnbinom, fgeom), legendtext = plot.legend)
qqcomp(list(fnorm, flogis, fpois, fnbinom, fgeom), legendtext = plot.legend)
cdfcomp(list(fnorm, flogis, fpois, fnbinom, fgeom), legendtext = plot.legend)
ppcomp(list(fnorm, flogis, fpois, fnbinom, fgeom), legendtext = plot.legend)
```

```{r}
plot_df = users #%>% filter(suggestion_type != 'NONE') %>% mutate(suggestion_type = fct_drop(suggestion_type))
contrasts(plot_df$suggestion_type) = contr.treatment
scenario_completed.model = glm(
  scenario_completed ~ (suggestion_type + noise_level) + (gender + age_group + robot_experience),
  data=plot_df,
  family = 'binomial'
)
summary(scenario_completed.model)
with(scenario_completed.model, pchisq(null.deviance - deviance, df.null - df.residual, lower.tail = FALSE))
```
```{r, eval=FALSE}
# Additional diagnostics
r2glm(scenario_completed.model)
anova(scenario_completed.model, test = "Chisq")
car::vif(scenario_completed.model)
```

```{r, eval=F}
# MCMC estimation
scenario_completed.model1 = brm(
  scenario_completed ~ ((has_dx + has_ax) * noise_level) + (gender + age_group + robot_experience),
  data=plot_df,
  family = 'bernoulli',
  iter = 4000,
  cores = 8
)
summary(scenario_completed.model1)
scenario_completed.model1 = add_criterion(scenario_completed.model1, "waic")
scenario_completed.model1 = add_criterion(scenario_completed.model1, "loo")
show(scenario_completed.model1$criteria$loo)

# MCMC estimation
scenario_completed.model2 = brm(
  scenario_completed | trials(1) ~ ((has_dx + has_ax) * noise_level) + (gender + age_group + robot_experience),
  data=plot_df,
  family = 'binomial',
  cores = 8
  # future = T
)
summary(scenario_completed.model2)
scenario_completed.model2 = add_criterion(scenario_completed.model2, "waic")
scenario_completed.model2 = add_criterion(scenario_completed.model2, "loo")

# Compare the models
loo_compare(scenario_complete.model1, scenario_completed.model2)
```

```{r, eval=F}
summary(scenario_completed.model2)
print(scenario_completed.model2$criteria$loo)
bayes_R2(scenario_completed.model2)
# scenario_completed.model2.soo = launch_shinystan(scenario_completed.model2)
```

### Num Actions Diff

This is the number of actions taken by the user minus the number of optimal actions for the scenario they were provided.

```{r message=FALSE, warning=FALSE}
plot_df = users
x = plot_df$num_actions_diff
```

```{r, eval=F}
fnorm = fitdist(x, "norm")
flogis = fitdist(x, "logis")
fpois = fitdist(x, "pois")
fnbinom = fitdist(x, "nbinom")
fgeom = fitdist(x, "geom")

descdist(x, boot=1000)
plotdist(x, histo=T, demp=T)
plot(sort(x))

par(mfrow = c(2, 2))
plot.legend = c("norm", "logit", "poisson", "negbinom", "geom")
denscomp(list(fnorm, flogis, fpois, fnbinom, fgeom), legendtext = plot.legend)
qqcomp(list(fnorm, flogis, fpois, fnbinom, fgeom), legendtext = plot.legend)
cdfcomp(list(fnorm, flogis, fpois, fnbinom, fgeom), legendtext = plot.legend)
ppcomp(list(fnorm, flogis, fpois, fnbinom, fgeom), legendtext = plot.legend)
```

```{r}
# Plot the number of actions taken by users using GLM
plot_df = users
plot_df$scenario_completed_pred = scenario_completed.model$fitted.values

# actions_diff.model = glm.nb(
#   num_actions_diff ~ ((has_dx + has_ax) * noise_level) + (gender + age_group + robot_experience) + num_optimal + scenario_completed, # scenario_completed_pred has higher AIC
#   data=plot_df,
# )

actions_diff.model = glm(
  num_actions_diff ~ ((has_dx + has_ax) * noise_level) + (gender + age_group + robot_experience) + num_optimal + scenario_completed,  # scenario_completed_pred has higher AIC
  data=plot_df,
  family = 'poisson'
)

summary(actions_diff.model)
with(actions_diff.model, pchisq(null.deviance - deviance, df.null - df.residual, lower.tail = FALSE))
```

```{r, eval=FALSE}
# Additional diagnostics
r2glm(actions_diff.model)
anova(actions_diff.model, test = "Chisq")
car::vif(actions_diff.model)
```

```{r, eval=F}
# MCMC estimation
actions_diff.model1 = brm(
  num_actions_diff ~ ((has_dx + has_ax) * noise_level) + (gender + age_group + robot_experience) + num_optimal + scenario_completed,
  data=plot_df,
  family = 'poisson',
  cores = 8
  # future = T
)
summary(actions_diff.model1)
actions_diff.model1 = add_criterion(actions_diff.model1, "waic")
actions_diff.model1 = add_criterion(actions_diff.model1, "loo")

# MCMC estimation
actions_diff.model2 = brm(
  num_actions_diff ~ ((has_dx + has_ax) * noise_level) + (gender + age_group + robot_experience) + num_optimal + scenario_completed,
  data=plot_df,
  family = 'negbinomial',
  cores = 8
)
summary(actions_diff.model2)
actions_diff.model2 = add_criterion(actions_diff.model2, "waic")
actions_diff.model2 = add_criterion(actions_diff.model2, "loo")

# MCMC estimation
actions_diff.model3 = brm(
  num_actions_diff ~ ((has_dx + has_ax) * noise_level) + (gender + age_group + robot_experience) + num_optimal + scenario_completed,
  data=plot_df,
  family = 'geometric',
  cores = 8
)
summary(actions_diff.model3)
actions_diff.model3 = add_criterion(actions_diff.model3, "waic")
actions_diff.model3 = add_criterion(actions_diff.model3, "loo")

# All models are invalid
loo_compare(actions_diff.model1, actions_diff.model2, actions_diff.model3)
# loo_compare(actions_diff.model1, actions_diff.model2, criterion = "loo")
```

```{r, eval=F}
summary(actions_diff.model2)
print(actions_diff.model2$criteria$loo)
bayes_R2(actions_diff.model2)
# actions_diff.model1.soo = launch_shinystan(actions_diff.model1)
```


### Action (Inverse) Reliance

Given the state, did the user take the optimal action?

```{r message=FALSE, warning=FALSE}
plot_df = actions
x = plot_df$optimal_ax
```

```{r, eval=F}
fnorm = fitdist(x, "norm")
flogis = fitdist(x, "logis")
fpois = fitdist(x, "pois")
fnbinom = fitdist(x, "nbinom")
fgeom = fitdist(x, "geom")

descdist(x, boot=1000)
plotdist(x, histo=T, demp=T)
plot(sort(x))

par(mfrow = c(2, 2))
plot.legend = c("norm", "logit", "poisson", "negbinom", "geom")
denscomp(list(fnorm, flogis, fpois, fnbinom, fgeom), legendtext = plot.legend)
qqcomp(list(fnorm, flogis, fpois, fnbinom, fgeom), legendtext = plot.legend)
cdfcomp(list(fnorm, flogis, fpois, fnbinom, fgeom), legendtext = plot.legend)
ppcomp(list(fnorm, flogis, fpois, fnbinom, fgeom), legendtext = plot.legend)
```

```{r, eval=F}
# Change to continuous values
plot_df$age_group = as.numeric(plot_df$age_group)
plot_df$robot_experience = as.numeric(plot_df$robot_experience)

# Use linear model
# optimal_action.model = lmer(
#   optimal_ax ~ ((has_dx + has_ax) * noise_level) + (age_group + robot_experience + gender) + state_idx + (1 | user_id),
#   data=plot_df
# )
# shapiro.test(resid(optimal_action.model))

# Use a GLM to estimate parameters (unidentifiable)
optimal_action.model = glmer(
  optimal_ax ~ ((has_dx + has_ax) * noise_level) + (age_group + robot_experience + gender) + state_idx + (1 | user_id),
  data=plot_df,
  family = 'binomial',
)
# optimal_action.model = glmer.nb(
#   optimal_ax ~ ((has_dx + has_ax) * noise_level) + (age_group + robot_experience + gender) + scale(state_idx) + (1 | user_id),
#   data=plot_df
# )

summary(optimal_action.model)
```

```{r message=FALSE}
# Making the variable a factor makes inference take much longer
# plot_df$state_idx = factor(plot_df$state_idx)

# Random tests
plot_df = actions
# Lower loo score if the noise is treated as numeric
# plot_df$noise_level = as.numeric(plot_df$noise_level)
optimal_action_test.model = brm(
  optimal_ax | trials(num_actions) ~ ((has_dx + has_ax) * noise_level) + (age_group + robot_experience + gender) + state_idx + (1 | user_id),
  data = plot_df,
  family = "binomial",
  cores = 4,
  
)
optimal_action_test.model = add_criterion(optimal_action_test.model, "waic")
optimal_action_test.model = add_criterion(optimal_action_test.model, "loo")
print(summary(optimal_action_test.model))
print(optimal_action_test.model$criteria$loo)

# MCMC estimation
optimal_action.model1 = brm(
  optimal_ax ~ ((has_dx + has_ax) * noise_level) + (age_group + robot_experience + gender) + state_idx + (1 | user_id),
  data=plot_df,
  family = 'bernoulli',
  cores = 4
)
summary(optimal_action.model1)
optimal_action.model1 = add_criterion(optimal_action.model1, "waic")
optimal_action.model1 = add_criterion(optimal_action.model1, "loo")

# MCMC estimation
optimal_action.model2 = brm(
  optimal_ax | trials(num_actions) ~ ((has_dx + has_ax) * noise_level) + (age_group + robot_experience + gender) + state_idx + (1 | user_id),
  data=plot_df,
  family = 'binomial',
  cores = 8
  # future = T
)
summary(optimal_action.model2)
optimal_action.model2 = add_criterion(optimal_action.model2, "waic")
optimal_action.model2 = add_criterion(optimal_action.model2, "loo")

# MCMC estimation
optimal_action.model3 = brm(
  optimal_ax ~ ((has_dx + has_ax) * noise_level) + (age_group + robot_experience + gender) + state_idx + (1 | user_id),
  data=plot_df,
  family = 'negbinomial',
  cores = 8
)
summary(optimal_action.model3)
optimal_action.model3 = add_criterion(optimal_action.model3, "waic")
optimal_action.model3 = add_criterion(optimal_action.model3, "loo")

# Model 1 is invalid, so only compare 2 & 3
# loo_compare(optimal_action.model1, optimal_action.model2, optimal_action.model3)
loo_compare(optimal_action.model2, optimal_action.model3, criterion = "loo")
```

```{r}
# We choose model 2. Get the model details again
summary(optimal_action.model2)
print(optimal_action.model2$criteria$loo)
bayes_R2(optimal_action.model2)
# optimal_action.model2.soo = launch_shinystan(optimal_action.model2)
```


### Action Compliance

Given suggestions, did the user follow the actions? P(follow | suggestion)

```{r message=FALSE, warning=FALSE}
plot_df = subset(actions, has_ax == T)
x = plot_df$chose_ax
```

```{r, eval=F}
fnorm = fitdist(x, "norm")
flogis = fitdist(x, "logis")
fpois = fitdist(x, "pois")
fnbinom = fitdist(x, "nbinom")
fgeom = fitdist(x, "geom")

descdist(x, boot=1000)
plotdist(x, histo=T, demp=T)
plot(sort(x))

par(mfrow = c(2, 2))
plot.legend = c("norm", "logit", "poisson", "negbinom", "geom")
denscomp(list(fnorm, flogis, fpois, fnbinom, fgeom), legendtext = plot.legend)
qqcomp(list(fnorm, flogis, fpois, fnbinom, fgeom), legendtext = plot.legend)
cdfcomp(list(fnorm, flogis, fpois, fnbinom, fgeom), legendtext = plot.legend)
ppcomp(list(fnorm, flogis, fpois, fnbinom, fgeom), legendtext = plot.legend)
```

```{r, eval=F}
# Use linear model
# chose_action.model = lmer(
#   chose_ax ~ (has_dx * noise_level) + (age_group + robot_experience + gender) + state_idx + (1 | user_id),
#   data=plot_df
# )
# shapiro.test(resid(chose_action.model))

# Use a GLM to estimate parameters (unidentifiable)
chose_action.model = glmer(
  chose_ax ~ () + (age_group + robot_experience + gender) + state_idx_rescaled + (1 | user_id),
  data=plot_df,
  family = 'binomial',
)
# chose_action.model = glmer.nb(
#   chose_ax ~ (has_dx * noise_level) + (age_group + robot_experience + gender) + state_idx_rescaled + (1 | user_id),
#   data=plot_df
# )

summary(chose_action.model)
```

```{r, message=FALSE}
# MCMC estimation
chose_action.model1 = brm(
  chose_ax ~ (has_dx * noise_level) + (age_group + robot_experience + gender) + state_idx + (1 | user_id),
  data=plot_df,
  family = 'bernoulli',
  cores = 8
)
summary(chose_action.model1)
chose_action.model1 = add_criterion(chose_action.model1, "waic")
chose_action.model1 = add_criterion(chose_action.model1, "loo")

# MCMC estimation
chose_action.model2 = brm(
  chose_ax | trials(num_actions) ~ (has_dx * noise_level) + (age_group + robot_experience + gender) + state_idx + (1 | user_id),
  data=plot_df,
  family = 'binomial',
  cores = 8
  # future = T
)
summary(chose_action.model2)
chose_action.model2 = add_criterion(chose_action.model2, "waic")
chose_action.model2 = add_criterion(chose_action.model2, "loo")

# MCMC estimation
chose_action.model3 = brm(
  chose_ax ~ (has_dx * noise_level) + (age_group + robot_experience + gender) + state_idx + (1 | user_id),
  data=plot_df,
  family = 'negbinomial',
  cores = 8
)
summary(chose_action.model3)
chose_action.model3 = add_criterion(chose_action.model3, "waic")
chose_action.model3 = add_criterion(chose_action.model3, "loo")

# Model 1 is invalid, so only compare 2 & 3
# loo_compare(chose_action.model1, chose_action.model2, chose_action.model3)
loo_compare(chose_action.model2, chose_action.model3, criterion = "loo")
```

```{r}
# We choose model 2. Get the model details again
summary(chose_action.model2)
print(chose_action.model2$criteria$loo)
bayes_R2(chose_action.model2)
# chose_action.model2.soo = launch_shinystan(chose_action.model2)
```


### Decision Time

How much time did it take to make a decision?

```{r message=FALSE, warning=FALSE}
plot_df = actions
x = plot_df$decision_duration
```

```{r, eval=F}
fnorm = fitdist(x, "norm")
fexp = fitdist(x, "exp")
fgamma = fitdist(x, "gamma")
flnorm = fitdist(x, "lnorm")

descdist(x, boot=1000)
plotdist(x, histo=T, demp=T)
plot(sort(x))

par(mfrow = c(2, 2))
plot.legend = c("norm", "exp", "gamma", "lnorm")
denscomp(list(fnorm, fexp, fgamma, flnorm), legendtext = plot.legend)
qqcomp(list(fnorm, fexp, fgamma, flnorm), legendtext = plot.legend)
cdfcomp(list(fnorm, fexp, fgamma, flnorm), legendtext = plot.legend)
ppcomp(list(fnorm, fexp, fgamma, flnorm), legendtext = plot.legend)
```

```{r, eval=F}
# Use linear model
# decision_duration.model = lmer(
#   decision_duration ~ ((has_ax + has_dx) * noise_level) + (age_group + robot_experience + gender) + state_idx + (1 | user_id),
#   data=plot_df
# )
# shapiro.test(resid(decision_duration.model))

# Use a GLM to estimate parameters
# None of the GLMER models work
decision_duration.model = glmer(
  decision_duration ~ ((has_ax + has_dx) * noise_level) + (age_group + robot_experience + gender) + state_idx + (1 | user_id),
  data=plot_df,
  family = 'gamma',
)

summary(decision_duration.model)
```

```{r, message=FALSE}
# MCMC estimation
decision_duration.model1 = brm(
  decision_duration ~ ((has_ax + has_dx) * noise_level) + (age_group + robot_experience + gender) + state_idx + (1 | user_id),
  data=plot_df,
  family = 'exponential',
  cores = 8
)
summary(decision_duration.model1)
decision_duration.model1 = add_criterion(decision_duration.model1, "waic")
decision_duration.model1 = add_criterion(decision_duration.model1, "loo")

# MCMC estimation
decision_duration.model2 = brm(
  decision_duration ~ ((has_ax + has_dx) * noise_level) + (age_group + robot_experience + gender) + state_idx + (1 | user_id),
  data=plot_df,
  family = 'weibull',
  cores = 8
  # future = T
)
summary(decision_duration.model2)
decision_duration.model2 = add_criterion(decision_duration.model2, "waic")
decision_duration.model2 = add_criterion(decision_duration.model2, "loo")

# MCMC estimation
decision_duration.model3 = brm(
  decision_duration ~ ((has_ax + has_dx) * noise_level) + (age_group + robot_experience + gender) + state_idx + (1 | user_id),
  data=plot_df,
  family = 'Gamma',
  cores = 8
)
summary(decision_duration.model3)
decision_duration.model3 = add_criterion(decision_duration.model3, "waic")
decision_duration.model3 = add_criterion(decision_duration.model3, "loo")

# MCMC estimation
decision_duration.model4 = brm(
  decision_duration ~ ((has_ax + has_dx) * noise_level) + (age_group + robot_experience + gender) + state_idx + (1 | user_id),
  data=plot_df,
  family = 'lognormal',
  cores = 8
)
summary(decision_duration.model4)
decision_duration.model4 = add_criterion(decision_duration.model4, "waic")
decision_duration.model4 = add_criterion(decision_duration.model4, "loo")

# MCMC estimation
decision_duration.model5 = brm(
  decision_duration ~ ((has_ax + has_dx) * noise_level) + (age_group + robot_experience + gender) + state_idx + (1 | user_id),
  data=plot_df,
  family = 'exgaussian',
  cores = 8
)
summary(decision_duration.model5)
decision_duration.model5 = add_criterion(decision_duration.model5, "waic")
decision_duration.model5 = add_criterion(decision_duration.model5, "loo")

# MCMC estimation
decision_duration.model6 = brm(
  decision_duration ~ ((has_ax + has_dx) * noise_level) + (age_group + robot_experience + gender) + state_idx + (1 | user_id),
  data=plot_df,
  family = 'shifted_lognormal',
  cores = 8
)
summary(decision_duration.model6)
decision_duration.model6 = add_criterion(decision_duration.model6, "waic")
decision_duration.model6 = add_criterion(decision_duration.model6, "loo")

loo_compare(decision_duration.model1,
            # decision_duration.model2, # Really high Rhat
            decision_duration.model3,
            decision_duration.model4,
            decision_duration.model5,
            decision_duration.model6)
```

```{r}
# We choose model 6. Get the model details again
summary(decision_duration.model6)
print(decision_duration.model6$criteria$loo)
bayes_R2(decision_duration.model6)
# decision_duration.model6.soo = launch_shinystan(decision_duration.model6)
```


### Action Decision Time

Maybe the action decision time is affected?

```{r message=FALSE, warning=FALSE}
plot_df = actions
x = plot_df$ax_decision_duration
```

```{r, eval=F}
fnorm = fitdist(x, "norm")
fexp = fitdist(x, "exp")
fgamma = fitdist(x, "gamma")
flnorm = fitdist(x, "lnorm")

descdist(x, boot=1000)
plotdist(x, histo=T, demp=T)
plot(sort(x))

par(mfrow = c(2, 2))
plot.legend = c("norm", "exp", "gamma", "lnorm")
denscomp(list(fnorm, fexp, fgamma, flnorm), legendtext = plot.legend)
qqcomp(list(fnorm, fexp, fgamma, flnorm), legendtext = plot.legend)
cdfcomp(list(fnorm, fexp, fgamma, flnorm), legendtext = plot.legend)
ppcomp(list(fnorm, fexp, fgamma, flnorm), legendtext = plot.legend)
```

```{r, eval=F}
# Use linear model
# ax_decision_duration.model = lmer(
#   ax_decision_duration ~ ((has_ax + has_dx) * noise_level) + (age_group + robot_experience + gender) + state_idx + (1 | user_id),
#   data=plot_df
# )
# shapiro.test(resid(ax_decision_duration.model))

# Use a GLM to estimate parameters
# None of the GLMER models work
ax_decision_duration.model = glmer(
  ax_decision_duration ~ ((has_ax + has_dx) * noise_level) + (age_group + robot_experience + gender) + state_idx + (1 | user_id),
  data = plot_df,
  family = Gamma(),
)

summary(ax_decision_duration.model)
```

```{r, message=FALSE}
# MCMC estimation
ax_decision_duration.model1 = brm(
  ax_decision_duration ~ ((has_ax + has_dx) * noise_level) + (age_group + robot_experience + gender) + state_idx + (1 | user_id),
  data=plot_df,
  family = 'exponential',
  cores = 8
)
summary(ax_decision_duration.model1)
ax_decision_duration.model1 = add_criterion(ax_decision_duration.model1, "waic")
ax_decision_duration.model1 = add_criterion(ax_decision_duration.model1, "loo")

# MCMC estimation
ax_decision_duration.model2 = brm(
  ax_decision_duration ~ ((has_ax + has_dx) * noise_level) + (age_group + robot_experience + gender) + state_idx + (1 | user_id),
  data=plot_df,
  family = 'weibull',
  cores = 8
  # future = T
)
summary(ax_decision_duration.model2)
ax_decision_duration.model2 = add_criterion(ax_decision_duration.model2, "waic")
ax_decision_duration.model2 = add_criterion(ax_decision_duration.model2, "loo")

# MCMC estimation
ax_decision_duration.model3 = brm(
  ax_decision_duration ~ ((has_ax + has_dx) * noise_level) + (age_group + robot_experience + gender) + state_idx + (1 | user_id),
  data=plot_df,
  family = 'Gamma',
  cores = 8
)
summary(ax_decision_duration.model3)
ax_decision_duration.model3 = add_criterion(ax_decision_duration.model3, "waic")
ax_decision_duration.model3 = add_criterion(ax_decision_duration.model3, "loo")

# MCMC estimation
ax_decision_duration.model4 = brm(
  ax_decision_duration ~ ((has_ax + has_dx) * noise_level) + (age_group + robot_experience + gender) + state_idx + (1 | user_id),
  data=plot_df,
  family = 'lognormal',
  cores = 8
)
summary(ax_decision_duration.model4)
ax_decision_duration.model4 = add_criterion(ax_decision_duration.model4, "waic")
ax_decision_duration.model4 = add_criterion(ax_decision_duration.model4, "loo")

# MCMC estimation
ax_decision_duration.model5 = brm(
  ax_decision_duration ~ ((has_ax + has_dx) * noise_level) + (age_group + robot_experience + gender) + state_idx + (1 | user_id),
  data=plot_df,
  family = 'exgaussian',
  cores = 8
)
summary(ax_decision_duration.model5)
ax_decision_duration.model5 = add_criterion(ax_decision_duration.model5, "waic")
ax_decision_duration.model5 = add_criterion(ax_decision_duration.model5, "loo")

# MCMC estimation
ax_decision_duration.model6 = brm(
  ax_decision_duration ~ ((has_ax + has_dx) * noise_level) + (age_group + robot_experience + gender) + state_idx + (1 | user_id),
  data=plot_df,
  family = 'shifted_lognormal',
  cores = 8
)
summary(ax_decision_duration.model6)
ax_decision_duration.model6 = add_criterion(ax_decision_duration.model6, "waic")
ax_decision_duration.model6 = add_criterion(ax_decision_duration.model6, "loo")

loo_compare(ax_decision_duration.model1,
            # ax_decision_duration.model2, # Really high Rhat
            ax_decision_duration.model3,
            ax_decision_duration.model4,
            # ax_decision_duration.model5, # Really high Rhat
            ax_decision_duration.model6)
```

```{r}
# We choose model 6. Get the model details again
summary(ax_decision_duration.model6)
print(ax_decision_duration.model6$criteria$loo)
bayes_R2(ax_decision_duration.model6)
# ax_decision_duration.model6.soo = launch_shinystan(ax_decision_duration.model6)
```


### Diagnosis Decision Time

Maybe the action decision time is affected?

```{r message=FALSE, warning=FALSE}
plot_df = actions
x = plot_df$dx_decision_duration
```

```{r, eval=F}
fnorm = fitdist(x, "norm")
fexp = fitdist(x, "exp")
fgamma = fitdist(x, "gamma")
flnorm = fitdist(x, "lnorm")

descdist(x, boot=1000)
plotdist(x, histo=T, demp=T)
plot(sort(x))

par(mfrow = c(2, 2))
plot.legend = c("norm", "exp", "gamma", "lnorm")
denscomp(list(fnorm, fexp, fgamma, flnorm), legendtext = plot.legend)
qqcomp(list(fnorm, fexp, fgamma, flnorm), legendtext = plot.legend)
cdfcomp(list(fnorm, fexp, fgamma, flnorm), legendtext = plot.legend)
ppcomp(list(fnorm, fexp, fgamma, flnorm), legendtext = plot.legend)
```

```{r, eval=F}
# Use linear model
# dx_decision_duration.model = lmer(
#   dx_decision_duration ~ ((has_ax + has_dx) * noise_level) + (age_group + robot_experience + gender) + state_idx + (1 | user_id),
#   data=plot_df
# )
# shapiro.test(resid(dx_decision_duration.model))

# Use a GLM to estimate parameters
# None of the GLMER models work
dx_decision_duration.model = glmer(
  dx_decision_duration ~ ((has_ax + has_dx) * noise_level) + (age_group + robot_experience + gender) + state_idx + (1 | user_id),
  data = plot_df,
  family = Gamma(link = "log"),
)

summary(dx_decision_duration.model)
```

```{r, message=FALSE}
# MCMC estimation
dx_decision_duration.model1 = brm(
  dx_decision_duration ~ ((has_ax + has_dx) * noise_level) + (age_group + robot_experience + gender) + state_idx + (1 | user_id),
  data=plot_df,
  family = 'exponential',
  cores = 8
)
summary(dx_decision_duration.model1)
dx_decision_duration.model1 = add_criterion(dx_decision_duration.model1, "waic")
dx_decision_duration.model1 = add_criterion(dx_decision_duration.model1, "loo")

# MCMC estimation
dx_decision_duration.model2 = brm(
  dx_decision_duration ~ ((has_ax + has_dx) * noise_level) + (age_group + robot_experience + gender) + state_idx + (1 | user_id),
  data=plot_df,
  family = 'weibull',
  cores = 8
  # future = T
)
summary(dx_decision_duration.model2)
dx_decision_duration.model2 = add_criterion(dx_decision_duration.model2, "waic")
dx_decision_duration.model2 = add_criterion(dx_decision_duration.model2, "loo")

# MCMC estimation
dx_decision_duration.model3 = brm(
  dx_decision_duration ~ ((has_ax + has_dx) * noise_level) + (age_group + robot_experience + gender) + state_idx + (1 | user_id),
  data=plot_df,
  family = 'Gamma',
  cores = 8
)
summary(dx_decision_duration.model3)
dx_decision_duration.model3 = add_criterion(dx_decision_duration.model3, "waic")
dx_decision_duration.model3 = add_criterion(dx_decision_duration.model3, "loo")

# MCMC estimation
dx_decision_duration.model4 = brm(
  dx_decision_duration ~ ((has_ax + has_dx) * noise_level) + (age_group + robot_experience + gender) + state_idx + (1 | user_id),
  data=plot_df,
  family = 'lognormal',
  cores = 8
)
summary(dx_decision_duration.model4)
dx_decision_duration.model4 = add_criterion(dx_decision_duration.model4, "waic")
dx_decision_duration.model4 = add_criterion(dx_decision_duration.model4, "loo")

# MCMC estimation
dx_decision_duration.model5 = brm(
  dx_decision_duration ~ ((has_ax + has_dx) * noise_level) + (age_group + robot_experience + gender) + state_idx + (1 | user_id),
  data=plot_df,
  family = 'exgaussian',
  cores = 8
)
summary(dx_decision_duration.model5)
dx_decision_duration.model5 = add_criterion(dx_decision_duration.model5, "waic")
dx_decision_duration.model5 = add_criterion(dx_decision_duration.model5, "loo")

# MCMC estimation
dx_decision_duration.model6 = brm(
  dx_decision_duration ~ ((has_ax + has_dx) * noise_level) + (age_group + robot_experience + gender) + state_idx + (1 | user_id),
  data=plot_df,
  family = 'shifted_lognormal',
  cores = 8
)
summary(dx_decision_duration.model6)
dx_decision_duration.model6 = add_criterion(dx_decision_duration.model6, "waic")
dx_decision_duration.model6 = add_criterion(dx_decision_duration.model6, "loo")

loo_compare(dx_decision_duration.model1,
            dx_decision_duration.model2,
            dx_decision_duration.model3,
            dx_decision_duration.model4,
            dx_decision_duration.model5,
            dx_decision_duration.model6)
```

```{r}
# We choose model 6. Get the model details again
summary(dx_decision_duration.model6)
print(dx_decision_duration.model6$criteria$loo)
bayes_R2(dx_decision_duration.model6)
# dx_decision_duration.model6.soo = launch_shinystan(dx_decision_duration.model6)
```


### Time when compliant / reliant

Does the AX or DX suggestion time change (as perhaps indicated by Yang et al.) when the choices of problems and the actions are optimal, or if people are blindly following the suggestions? (add additional factors to the regression models)

```{r message=FALSE, warning=FALSE}
plot_df = actions
x = plot_df$decision_duration
plot_df$optimal_ax = factor(plot_df$optimal_ax, levels = c(0, 1), ordered = T)
plot_df$optimal_dx = factor(plot_df$optimal_dx, levels = c(0, 1), ordered = T)
plot_df$chose_ax = factor(plot_df$chose_ax, levels = c(0, 1), ordered = T)
plot_df$chose_ax = factor(plot_df$chose_ax, levels = c(0, 1), ordered = T)
```

```{r, message=FALSE}
# MCMC estimation
conditional_decision_duration.model1 = brm(
  decision_duration ~ ((has_ax + has_dx) * noise_level) + (age_group + robot_experience + gender) + state_idx + optimal_ax + chose_ax + chose_dx + optimal_dx + (1 | user_id),
  data=plot_df,
  family = 'exponential',
  cores = 8
)
summary(conditional_decision_duration.model1)
conditional_decision_duration.model1 = add_criterion(conditional_decision_duration.model1, "waic")
conditional_decision_duration.model1 = add_criterion(conditional_decision_duration.model1, "loo")

# MCMC estimation
conditional_decision_duration.model2 = brm(
  decision_duration ~ ((has_ax + has_dx) * noise_level) + (age_group + robot_experience + gender) + state_idx + optimal_ax + chose_ax + chose_dx + optimal_dx + (1 | user_id),
  data=plot_df,
  family = 'weibull',
  cores = 8
  # future = T
)
summary(conditional_decision_duration.model2)
conditional_decision_duration.model2 = add_criterion(conditional_decision_duration.model2, "waic")
conditional_decision_duration.model2 = add_criterion(conditional_decision_duration.model2, "loo")

# MCMC estimation
conditional_decision_duration.model3 = brm(
  decision_duration ~ ((has_ax + has_dx) * noise_level) + (age_group + robot_experience + gender) + state_idx + optimal_ax + chose_ax + chose_dx + optimal_dx + (1 | user_id),
  data=plot_df,
  family = 'Gamma',
  cores = 8
)
summary(conditional_decision_duration.model3)
conditional_decision_duration.model3 = add_criterion(conditional_decision_duration.model3, "waic")
conditional_decision_duration.model3 = add_criterion(conditional_decision_duration.model3, "loo")

# MCMC estimation
conditional_decision_duration.model4 = brm(
  decision_duration ~ ((has_ax + has_dx) * noise_level) + (age_group + robot_experience + gender) + state_idx + optimal_ax + chose_ax + chose_dx + optimal_dx + (1 | user_id),
  data=plot_df,
  family = 'lognormal',
  cores = 8
)
summary(conditional_decision_duration.model4)
conditional_decision_duration.model4 = add_criterion(conditional_decision_duration.model4, "waic")
conditional_decision_duration.model4 = add_criterion(conditional_decision_duration.model4, "loo")

# MCMC estimation
conditional_decision_duration.model5 = brm(
  decision_duration ~ ((has_ax + has_dx) * noise_level) + (age_group + robot_experience + gender) + state_idx + optimal_ax + chose_ax + chose_dx + optimal_dx + (1 | user_id),
  data=plot_df,
  family = 'exgaussian',
  cores = 8
)
summary(conditional_decision_duration.model5)
conditional_decision_duration.model5 = add_criterion(conditional_decision_duration.model5, "waic")
conditional_decision_duration.model5 = add_criterion(conditional_decision_duration.model5, "loo")

# MCMC estimation
conditional_decision_duration.model6 = brm(
  decision_duration ~ ((has_ax + has_dx) * noise_level) + (age_group + robot_experience + gender) + state_idx + optimal_ax + chose_ax + chose_dx + optimal_dx + (1 | user_id),
  data=plot_df,
  family = 'shifted_lognormal',
  cores = 8
)
summary(conditional_decision_duration.model6)
conditional_decision_duration.model6 = add_criterion(conditional_decision_duration.model6, "waic")
conditional_decision_duration.model6 = add_criterion(conditional_decision_duration.model6, "loo")

# All models are terrible
# loo_compare(conditional_decision_duration.model1,
#             conditional_decision_duration.model2,
#             conditional_decision_duration.model3,
#             conditional_decision_duration.model4,
#             conditional_decision_duration.model5,
#             conditional_decision_duration.model6)
```

Maybe if we subset to the action duration when actions were provided?

```{r message=FALSE, warning=FALSE}
plot_df = actions
x = plot_df$ax_decision_duration
plot_df$optimal_ax = factor(plot_df$optimal_ax, levels = c(0, 1), ordered = T)
plot_df$optimal_dx = factor(plot_df$optimal_dx, levels = c(0, 1), ordered = T)
plot_df$chose_ax = factor(plot_df$chose_ax, levels = c(0, 1), ordered = T)
plot_df$chose_ax = factor(plot_df$chose_ax, levels = c(0, 1), ordered = T)
```

```{r, message=FALSE}
# MCMC estimation
conditional_ax_decision_duration.model1 = brm(
  ax_decision_duration ~ ((has_ax + has_dx) * noise_level) + (age_group + robot_experience + gender) + state_idx + optimal_ax + chose_ax + (1 | user_id),
  data=plot_df,
  family = 'exponential',
  cores = 8
)
summary(conditional_ax_decision_duration.model1)
conditional_ax_decision_duration.model1 = add_criterion(conditional_ax_decision_duration.model1, "waic")
conditional_ax_decision_duration.model1 = add_criterion(conditional_ax_decision_duration.model1, "loo")

# MCMC estimation
conditional_ax_decision_duration.model2 = brm(
  ax_decision_duration ~ ((has_ax + has_dx) * noise_level) + (age_group + robot_experience + gender) + state_idx + optimal_ax + chose_ax + (1 | user_id),
  data=plot_df,
  family = 'weibull',
  cores = 8
  # future = T
)
summary(conditional_ax_decision_duration.model2)
conditional_ax_decision_duration.model2 = add_criterion(conditional_ax_decision_duration.model2, "waic")
conditional_ax_decision_duration.model2 = add_criterion(conditional_ax_decision_duration.model2, "loo")

# MCMC estimation
conditional_ax_decision_duration.model3 = brm(
  ax_decision_duration ~ ((has_ax + has_dx) * noise_level) + (age_group + robot_experience + gender) + state_idx + optimal_ax + chose_ax + (1 | user_id),
  data=plot_df,
  family = 'Gamma',
  cores = 8
)
summary(conditional_ax_decision_duration.model3)
conditional_ax_decision_duration.model3 = add_criterion(conditional_ax_decision_duration.model3, "waic")
conditional_ax_decision_duration.model3 = add_criterion(conditional_ax_decision_duration.model3, "loo")

# MCMC estimation
conditional_ax_decision_duration.model4 = brm(
  ax_decision_duration ~ ((has_ax + has_dx) * noise_level) + (age_group + robot_experience + gender) + state_idx + optimal_ax + chose_ax + (1 | user_id),
  data=plot_df,
  family = 'lognormal',
  cores = 8
)
summary(conditional_ax_decision_duration.model4)
conditional_ax_decision_duration.model4 = add_criterion(conditional_ax_decision_duration.model4, "waic")
conditional_ax_decision_duration.model4 = add_criterion(conditional_ax_decision_duration.model4, "loo")

# MCMC estimation
conditional_ax_decision_duration.model5 = brm(
  ax_decision_duration ~ ((has_ax + has_dx) * noise_level) + (age_group + robot_experience + gender) + state_idx + optimal_ax + chose_ax + (1 | user_id),
  data=plot_df,
  family = 'exgaussian',
  cores = 8
)
summary(conditional_ax_decision_duration.model5)
conditional_ax_decision_duration.model5 = add_criterion(conditional_ax_decision_duration.model5, "waic")
conditional_ax_decision_duration.model5 = add_criterion(conditional_ax_decision_duration.model5, "loo")

# MCMC estimation
conditional_ax_decision_duration.model6 = brm(
  ax_decision_duration ~ ((has_ax + has_dx) * noise_level) + (age_group + robot_experience + gender) + state_idx + optimal_ax + chose_ax + (1 | user_id),
  data=plot_df,
  family = 'shifted_lognormal',
  cores = 8
)
summary(conditional_ax_decision_duration.model6)
conditional_ax_decision_duration.model6 = add_criterion(conditional_ax_decision_duration.model6, "waic")
conditional_ax_decision_duration.model6 = add_criterion(conditional_ax_decision_duration.model6, "loo")

# All models are terrible
loo_compare(conditional_ax_decision_duration.model1,
            # conditional_ax_decision_duration.model2, # Terrible Rhat
            conditional_ax_decision_duration.model3,
            conditional_ax_decision_duration.model4,
            conditional_ax_decision_duration.model5,
            conditional_ax_decision_duration.model6)
```

```{r}
# We choose model 6. Get the model details again
summary(conditional_ax_decision_duration.model6)
print(conditional_ax_decision_duration.model6$criteria$loo)
bayes_R2(conditional_ax_decision_duration.model6)
# conditional_ax_decision_duration.model6.soo = launch_shinystan(dx_decision_duration.model6)
```

The same might hold true for diagnoses?

```{r message=FALSE, warning=FALSE}
plot_df = actions
x = plot_df$dx_decision_duration
plot_df$optimal_ax = factor(plot_df$optimal_ax, levels = c(0, 1), ordered = T)
plot_df$optimal_dx = factor(plot_df$optimal_dx, levels = c(0, 1), ordered = T)
plot_df$chose_ax = factor(plot_df$chose_ax, levels = c(0, 1), ordered = T)
plot_df$chose_ax = factor(plot_df$chose_ax, levels = c(0, 1), ordered = T)
```

```{r, message=FALSE}
# MCMC estimation
conditional_dx_decision_duration.model1 = brm(
  dx_decision_duration ~ ((has_ax + has_dx) * noise_level) + (age_group + robot_experience + gender) + state_idx + optimal_dx + chose_dx + (1 | user_id),
  data=plot_df,
  family = 'exponential',
  cores = 8
)
summary(conditional_dx_decision_duration.model1)
conditional_dx_decision_duration.model1 = add_criterion(conditional_dx_decision_duration.model1, "waic")
conditional_dx_decision_duration.model1 = add_criterion(conditional_dx_decision_duration.model1, "loo")

# MCMC estimation
conditional_dx_decision_duration.model2 = brm(
  dx_decision_duration ~ ((has_ax + has_dx) * noise_level) + (age_group + robot_experience + gender) + state_idx + optimal_dx + chose_dx + (1 | user_id),
  data=plot_df,
  family = 'weibull',
  cores = 8
  # future = T
)
summary(conditional_dx_decision_duration.model2)
conditional_dx_decision_duration.model2 = add_criterion(conditional_dx_decision_duration.model2, "waic")
conditional_dx_decision_duration.model2 = add_criterion(conditional_dx_decision_duration.model2, "loo")

# MCMC estimation
conditional_dx_decision_duration.model3 = brm(
  dx_decision_duration ~ ((has_ax + has_dx) * noise_level) + (age_group + robot_experience + gender) + state_idx + optimal_dx + chose_dx + (1 | user_id),
  data=plot_df,
  family = 'Gamma',
  cores = 8
)
summary(conditional_dx_decision_duration.model3)
conditional_dx_decision_duration.model3 = add_criterion(conditional_dx_decision_duration.model3, "waic")
conditional_dx_decision_duration.model3 = add_criterion(conditional_dx_decision_duration.model3, "loo")

# MCMC estimation
conditional_dx_decision_duration.model4 = brm(
  dx_decision_duration ~ ((has_ax + has_dx) * noise_level) + (age_group + robot_experience + gender) + state_idx + optimal_dx + chose_dx + (1 | user_id),
  data=plot_df,
  family = 'lognormal',
  cores = 8
)
summary(conditional_dx_decision_duration.model4)
conditional_dx_decision_duration.model4 = add_criterion(conditional_dx_decision_duration.model4, "waic")
conditional_dx_decision_duration.model4 = add_criterion(conditional_dx_decision_duration.model4, "loo")

# MCMC estimation
conditional_dx_decision_duration.model5 = brm(
  dx_decision_duration ~ ((has_ax + has_dx) * noise_level) + (age_group + robot_experience + gender) + state_idx + optimal_dx + chose_dx + (1 | user_id),
  data=plot_df,
  family = 'exgaussian',
  cores = 8
)
summary(conditional_dx_decision_duration.model5)
conditional_dx_decision_duration.model5 = add_criterion(conditional_dx_decision_duration.model5, "waic")
conditional_dx_decision_duration.model5 = add_criterion(conditional_dx_decision_duration.model5, "loo")

# MCMC estimation
conditional_dx_decision_duration.model6 = brm(
  dx_decision_duration ~ ((has_ax + has_dx) * noise_level) + (age_group + robot_experience + gender) + state_idx + optimal_dx + chose_dx + (1 | user_id),
  data=plot_df,
  family = 'shifted_lognormal',
  cores = 8
)
summary(conditional_dx_decision_duration.model6)
conditional_dx_decision_duration.model6 = add_criterion(conditional_dx_decision_duration.model6, "waic")
conditional_dx_decision_duration.model6 = add_criterion(conditional_dx_decision_duration.model6, "loo")

# All models are terrible
loo_compare(conditional_dx_decision_duration.model1,
            conditional_dx_decision_duration.model2, # Terrible Rhat
            conditional_dx_decision_duration.model3,
            conditional_dx_decision_duration.model4,
            conditional_dx_decision_duration.model5,
            conditional_dx_decision_duration.model6)
```


### Diagnosis (Inverse) Reliance

Given the state, did the user take the optimal action?

```{r message=FALSE, warning=FALSE}
plot_df = actions
x = plot_df$optimal_dx
```

```{r, eval=F}
fnorm = fitdist(x, "norm")
flogis = fitdist(x, "logis")
fpois = fitdist(x, "pois")
fnbinom = fitdist(x, "nbinom")
fgeom = fitdist(x, "geom")

descdist(x, boot=1000)
plotdist(x, histo=T, demp=T)
plot(sort(x))

par(mfrow = c(2, 2))
plot.legend = c("norm", "logit", "poisson", "negbinom", "geom")
denscomp(list(fnorm, flogis, fpois, fnbinom, fgeom), legendtext = plot.legend)
qqcomp(list(fnorm, flogis, fpois, fnbinom, fgeom), legendtext = plot.legend)
cdfcomp(list(fnorm, flogis, fpois, fnbinom, fgeom), legendtext = plot.legend)
ppcomp(list(fnorm, flogis, fpois, fnbinom, fgeom), legendtext = plot.legend)
```

```{r, eval=F}
# Change to continuous values
# plot_df$age_group = as.numeric(plot_df$age_group)
# plot_df$robot_experience = as.numeric(plot_df$robot_experience)

# Use linear model
# optimal_dx.model = lmer(
#   optimal_dx ~ ((has_dx + has_ax) * noise_level) + (age_group + robot_experience + gender) + state_idx + (1 | user_id),
#   data=plot_df
# )
# shapiro.test(resid(optimal_action.model))

# Use a GLM to estimate parameters (unidentifiable)
# optimal_action.model = glmer(
#   optimal_dx ~ ((has_dx + has_ax) * noise_level) + (age_group + robot_experience + gender) + state_idx + (1 | user_id),
#   data=plot_df,
#   family = 'binomial',
# )
optimal_dx.model = glmer.nb(
  optimal_dx ~ ((has_dx + has_ax) * noise_level) + (age_group + robot_experience + gender) + scale(state_idx) + (1 | user_id),
  data=plot_df
)

summary(optimal_dx.model)
```

```{r message=FALSE}
# MCMC estimation
optimal_dx.model1 = brm(
  optimal_dx ~ ((has_dx + has_ax) * noise_level) + (age_group + robot_experience + gender) + state_idx + (1 | user_id),
  data=plot_df,
  family = 'bernoulli',
  cores = 8
)
summary(optimal_dx.model1)
optimal_dx.model1 = add_criterion(optimal_dx.model1, "waic")
optimal_dx.model1 = add_criterion(optimal_dx.model1, "loo")

# MCMC estimation
optimal_dx.model2 = brm(
  optimal_dx | trials(num_actions) ~ ((has_dx + has_ax) * noise_level) + (age_group + robot_experience + gender) + state_idx + (1 | user_id),
  data=plot_df,
  family = 'binomial',
  cores = 8
  # future = T
)
summary(optimal_dx.model2)
optimal_dx.model2 = add_criterion(optimal_dx.model2, "waic")
optimal_dx.model2 = add_criterion(optimal_dx.model2, "loo")

# MCMC estimation
optimal_dx.model3 = brm(
  optimal_dx ~ ((has_dx + has_ax) * noise_level) + (age_group + robot_experience + gender) + state_idx + (1 | user_id),
  data=plot_df,
  family = 'negbinomial',
  cores = 8
)
summary(optimal_dx.model3)
optimal_dx.model3 = add_criterion(optimal_dx.model3, "waic")
optimal_dx.model3 = add_criterion(optimal_dx.model3, "loo")

# Model 1 is invalid, so only compare 2 & 3. Model 1 is valid if age and robot experience
# are treated as continuous variables
# loo_compare(optimal_dx.model1, optimal_dx.model2, optimal_dx.model3)
loo_compare(optimal_dx.model2, optimal_dx.model3, criterion = "loo")
```

```{r}
# We choose model 2. Get the model details again
summary(optimal_dx.model2)
print(optimal_dx.model2$criteria$loo)
bayes_R2(optimal_dx.model2)
# optimal_action.model2.soo = launch_shinystan(optimal_action.model2)
```


### Diagnosis Compliance

Given suggestions, did the user follow the actions? P(follow | suggestion)

```{r message=FALSE, warning=FALSE}
plot_df = subset(actions, has_dx)
x = plot_df$chose_dx
```

```{r, eval=F}
fnorm = fitdist(x, "norm")
flogis = fitdist(x, "logis")
fpois = fitdist(x, "pois")
fnbinom = fitdist(x, "nbinom")
fgeom = fitdist(x, "geom")

descdist(x, boot=1000)
plotdist(x, histo=T, demp=T)
plot(sort(x))

par(mfrow = c(2, 2))
plot.legend = c("norm", "logit", "poisson", "negbinom", "geom")
denscomp(list(fnorm, flogis, fpois, fnbinom, fgeom), legendtext = plot.legend)
qqcomp(list(fnorm, flogis, fpois, fnbinom, fgeom), legendtext = plot.legend)
cdfcomp(list(fnorm, flogis, fpois, fnbinom, fgeom), legendtext = plot.legend)
ppcomp(list(fnorm, flogis, fpois, fnbinom, fgeom), legendtext = plot.legend)
```

```{r, eval=F}
# Try with continuous valued predictors
plot_df$age_group = as.numeric(plot_df$age_group)
plot_df$robot_experience = as.numeric(plot_df$robot_experience)

# Use linear model
# chose_dx.model = lmer(
#   chose_dx ~ (has_ax * noise_level) + (age_group + robot_experience + gender) + state_idx + (1 | user_id),
#   data=plot_df
# )
# shapiro.test(resid(chose_dx.model))

# Use a GLM to estimate parameters (unidentifiable)
# chose_dx.model = glmer(
#   chose_dx ~ (has_ax * noise_level) + (age_group + robot_experience + gender) + state_idx + (1 | user_id),
#   data=plot_df,
#   family = 'binomial',
# )
chose_dx.model = glmer.nb(
  chose_dx ~ (has_ax * noise_level) + (age_group + robot_experience + gender) + state_idx + (1 | user_id),
  data=plot_df
)

# Somewhat promising if continuous predictors are used
summary(chose_dx.model)
```

```{r, message=FALSE}
# MCMC estimation
chose_dx.model1 = brm(
  chose_dx ~ (has_ax * noise_level) + (age_group + robot_experience + gender) + state_idx + (1 | user_id),
  data=plot_df,
  family = 'bernoulli',
  cores = 8
)
summary(chose_dx.model1)
chose_dx.model1 = add_criterion(chose_dx.model1, "waic")
chose_dx.model1 = add_criterion(chose_dx.model1, "loo")

# MCMC estimation
chose_dx.model2 = brm(
  chose_dx | trials(num_actions) ~ (has_ax * noise_level) + (age_group + robot_experience + gender) + state_idx + (1 | user_id),
  data=plot_df,
  family = 'binomial',
  # cores = 8
  future = T
)
summary(chose_dx.model2)
chose_dx.model2 = add_criterion(chose_dx.model2, "waic")
chose_dx.model2 = add_criterion(chose_dx.model2, "loo")

# MCMC estimation
chose_dx.model3 = brm(
  chose_dx ~ (has_ax * noise_level) + (age_group + robot_experience + gender) + state_idx + (1 | user_id),
  data=plot_df,
  family = 'negbinomial',
  cores = 8
)
summary(chose_dx.model3)
chose_dx.model3 = add_criterion(chose_dx.model3, "waic")
chose_dx.model3 = add_criterion(chose_dx.model3, "loo")

# Model 1 is invalid, so only compare 2 & 3
# loo_compare(chose_dx.model1, chose_dx.model2, chose_dx.model3)
loo_compare(chose_dx.model2, chose_dx.model3, criterion = "loo")
```

```{r}
# We choose model 2. Get the model details again
summary(chose_dx.model2)
print(chose_dx.model2$criteria$loo)
bayes_R2(chose_dx.model2)
```


### Diagnosis Certainty

What's the effect on the diagnosis certainty?

```{r message=FALSE, warning=FALSE}
plot_df = actions
x = plot_df$diagnosis_certainty
```

```{r, eval=F}
fnorm = fitdist(x, "norm")
flogis = fitdist(x, "logis")
fpois = fitdist(x, "pois")
fnbinom = fitdist(x, "nbinom")
fgeom = fitdist(x, "geom")

descdist(x, boot=1000)
plotdist(x, histo=T, demp=T)
plot(sort(x))

par(mfrow = c(2, 2))
plot.legend = c("norm", "logit", "poisson", "negbinom", "geom")
denscomp(list(fnorm, flogis, fpois, fnbinom, fgeom), legendtext = plot.legend)
qqcomp(list(fnorm, flogis, fpois, fnbinom, fgeom), legendtext = plot.legend)
cdfcomp(list(fnorm, flogis, fpois, fnbinom, fgeom), legendtext = plot.legend)
ppcomp(list(fnorm, flogis, fpois, fnbinom, fgeom), legendtext = plot.legend)
```

```{r, eval=F}
# Change to continuous values
# plot_df$age_group = as.numeric(plot_df$age_group)
# plot_df$robot_experience = as.numeric(plot_df$robot_experience)

# Use linear model
# dx_certainty.model = lmer(
#   diagnosis_certainty ~ ((has_dx + has_ax) * noise_level) + (age_group + robot_experience + gender) + state_idx + (1 | user_id),
#   data=plot_df
# )
# shapiro.test(resid(dx_certainty.model))

# Use a GLM to estimate parameters (unidentifiable)
dx_certainty.model = glmer(
  diagnosis_certainty ~ ((has_dx + has_ax) * noise_level) + (age_group + robot_experience + gender) + state_idx + (1 | user_id),
  data=plot_df,
  family = 'poisson',
)
# dx_certainty.model = glmer.nb(
#   diagnosis_certainty ~ ((has_dx + has_ax) * noise_level) + (age_group + robot_experience + gender) + scale(state_idx) + (1 | user_id),
#   data=plot_df
# )

summary(dx_certainty.model)
```

```{r message=FALSE}
# MCMC estimation
# Cannot calculate criterion on categorical?
# dx_certainty.model1 = brm(
#   diagnosis_certainty ~ ((has_dx + has_ax) * noise_level) + (age_group + robot_experience + gender) + state_idx + (1 | user_id),
#   data=plot_df,
#   family = 'categorical',
#   cores = 8
# )
# summary(dx_certainty.model1)
# dx_certainty.model1 = add_criterion(dx_certainty.model1, "waic")
# dx_certainty.model1 = add_criterion(dx_certainty.model1, "loo")

# MCMC estimation
# Multinomial is undefined
# dx_certainty.model2 = brm(
#   diagnosis_certainty ~ ((has_dx + has_ax) * noise_level) + (age_group + robot_experience + gender) + state_idx + (1 | user_id),
#   data=plot_df,
#   family = 'multinomial',
#   cores = 8
#   # future = T
# )
# summary(dx_certainty.model2)
# dx_certainty.model2 = add_criterion(dx_certainty.model2, "waic")
# dx_certainty.model2 = add_criterion(dx_certainty.model2, "loo")

# MCMC estimation
dx_certainty.model3 = brm(
  diagnosis_certainty ~ ((has_dx + has_ax) * noise_level) + (age_group + robot_experience + gender) + state_idx + (1 | user_id),
  data=plot_df,
  family = 'cumulative',
  cores = 8
)
summary(dx_certainty.model3)
dx_certainty.model3 = add_criterion(dx_certainty.model3, "waic")
dx_certainty.model3 = add_criterion(dx_certainty.model3, "loo")

# MCMC estimation
dx_certainty.model4 = brm(
  diagnosis_certainty ~ ((has_dx + has_ax) * noise_level) + (age_group + robot_experience + gender) + state_idx + (1 | user_id),
  data=plot_df,
  family = 'cratio',
  cores = 8
)
summary(dx_certainty.model4)
dx_certainty.model4 = add_criterion(dx_certainty.model4, "waic")
dx_certainty.model4 = add_criterion(dx_certainty.model4, "loo")

# MCMC estimation
dx_certainty.model5 = brm(
  diagnosis_certainty ~ ((has_dx + has_ax) * noise_level) + (age_group + robot_experience + gender) + state_idx + (1 | user_id),
  data=plot_df,
  family = 'sratio',
  cores = 8
)
summary(dx_certainty.model5)
dx_certainty.model5 = add_criterion(dx_certainty.model5, "waic")
dx_certainty.model5 = add_criterion(dx_certainty.model5, "loo")

# MCMC estimation
dx_certainty.model6 = brm(
  diagnosis_certainty ~ ((has_dx + has_ax) * noise_level) + (age_group + robot_experience + gender) + state_idx + (1 | user_id),
  data=plot_df,
  family = 'acat',
  cores = 8
)
summary(dx_certainty.model6)
dx_certainty.model6 = add_criterion(dx_certainty.model6, "waic")
dx_certainty.model6 = add_criterion(dx_certainty.model6, "loo")

# MCMC estimation
dx_certainty.model7 = brm(
  diagnosis_certainty ~ ((has_dx + has_ax) * noise_level) + (age_group + robot_experience + gender) + state_idx + (1 | user_id),
  data=plot_df,
  family = 'poisson',
  cores = 8
)
summary(dx_certainty.model7)
dx_certainty.model7 = add_criterion(dx_certainty.model7, "waic")
dx_certainty.model7 = add_criterion(dx_certainty.model7, "loo")

# MCMC estimation
dx_certainty.model8 = brm(
  diagnosis_certainty ~ ((has_dx + has_ax) * noise_level) + (age_group + robot_experience + gender) + state_idx + (1 | user_id),
  data=plot_df,
  family = 'negbinomial',
  cores = 8
)
summary(dx_certainty.model8)
dx_certainty.model8 = add_criterion(dx_certainty.model8, "waic")
dx_certainty.model8 = add_criterion(dx_certainty.model8, "loo")

# MCMC estimation
dx_certainty.model9 = brm(
  diagnosis_certainty ~ ((has_dx + has_ax) * noise_level) + (age_group + robot_experience + gender) + state_idx + (1 | user_id),
  data=plot_df,
  family = 'geometric',
  cores = 8
)
summary(dx_certainty.model9)
dx_certainty.model9 = add_criterion(dx_certainty.model9, "waic")
dx_certainty.model9 = add_criterion(dx_certainty.model9, "loo")

# Model 3 is the chosen one
loo_compare(# dx_certainty.model1,
            # dx_certainty.model2,
            dx_certainty.model3,
            dx_certainty.model4,
            dx_certainty.model5,
            dx_certainty.model6,
            dx_certainty.model7,
            dx_certainty.model8,
            dx_certainty.model9)
```

```{r}
# We choose model 2. Get the model details again
summary(dx_certainty.model3)
print(dx_certainty.model3$criteria$loo)
bayes_R2(dx_certainty.model3)
```


## Likert Scales

Is there no significance here?

First the SUS

```{r message=FALSE, warning=FALSE}
plot_df = users
x = plot_df$sus
```

```{r, eval=F}
fnorm = fitdist(x, "norm")
flogis = fitdist(x, "logis")

descdist(x, boot=1000)
plotdist(x, histo=T, demp=T)
plot(sort(x))

par(mfrow = c(2, 2))
plot.legend = c("norm", "logit")
denscomp(list(fnorm, flogis), legendtext = plot.legend)
qqcomp(list(fnorm, flogis), legendtext = plot.legend)
cdfcomp(list(fnorm, flogis), legendtext = plot.legend)
ppcomp(list(fnorm, flogis), legendtext = plot.legend)
```

```{r}
# Plot the number of actions taken by users using GLM
plot_df = users

sus.model = lm(
  sus ~ ((has_dx + has_ax) * noise_level) + (gender + age_group + robot_experience) + num_optimal,
  data=plot_df,
)

summary(sus.model)
shapiro.test(resid(sus.model))

# Do boxcox	
plot_df$sus = plot_df$sus + 20
sus.model = lm(
  sus ~ ((has_dx + has_ax) * noise_level) + (gender + age_group + robot_experience) + num_optimal,
  data=plot_df,
)
shapiro.test(resid(sus.model))
bc = boxcox(sus.model)
(lambda = bc$x[which.max(bc$y)])	
sus.model = lm(powerTransform(sus, lambda) ~ ((has_dx + has_ax) * noise_level) + (gender + age_group + robot_experience) + num_optimal,	
        data=plot_df)	
summary(sus.model)
shapiro.test(resid(sus.model))
car::vif(sus.model)
```


Then the diagnoses

```{r message=FALSE, warning=FALSE}
plot_df = users
x = plot_df$combo1
```

```{r, eval=F}
fnorm = fitdist(x, "norm")
flogis = fitdist(x, "logis")

descdist(x, boot=1000)
plotdist(x, histo=T, demp=T)
plot(sort(x))

par(mfrow = c(2, 2))
plot.legend = c("norm", "logit")
denscomp(list(fnorm, flogis), legendtext = plot.legend)
qqcomp(list(fnorm, flogis), legendtext = plot.legend)
cdfcomp(list(fnorm, flogis), legendtext = plot.legend)
ppcomp(list(fnorm, flogis), legendtext = plot.legend)
```

```{r}
# Plot the number of actions taken by users using GLM
plot_df = users

combo1.model = lm(
  combo1 ~ ((has_dx + has_ax) * noise_level) + (gender + age_group + robot_experience) + num_optimal,
  data=plot_df,
)

summary(combo1.model)
shapiro.test(resid(combo1.model))

car::vif(combo1.model)
```

Then the actions

```{r message=FALSE, warning=FALSE}
plot_df = users
x = plot_df$combo2
```

```{r, eval=F}
fnorm = fitdist(x, "norm")
flogis = fitdist(x, "logis")

descdist(x, boot=1000)
plotdist(x, histo=T, demp=T)
plot(sort(x))

par(mfrow = c(2, 2))
plot.legend = c("norm", "logit")
denscomp(list(fnorm, flogis), legendtext = plot.legend)
qqcomp(list(fnorm, flogis), legendtext = plot.legend)
cdfcomp(list(fnorm, flogis), legendtext = plot.legend)
ppcomp(list(fnorm, flogis), legendtext = plot.legend)
```

```{r}
# Plot the number of actions taken by users using GLM
plot_df = users

combo2.model = lm(
  combo2 ~ ((has_dx + has_ax) * noise_level) + (gender + age_group + robot_experience) + num_optimal,
  data=plot_df,
)

summary(combo2.model)
shapiro.test(resid(combo2.model))

car::vif(combo2.model)
```
